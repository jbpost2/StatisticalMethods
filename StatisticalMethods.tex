\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistical Methods},
  pdfauthor={Post, Avery, Osborne},
  pdfborder={0 0 0},
  breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Statistical Methods}
\author{Post, Avery, Osborne}
\date{2020-04-15}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{about-the-book}{%
\section{About the book}\label{about-the-book}}

The goal in creating this book is to provide a thorough treatment of applied statistical methodologies geared toward analyzing designed experiments. Our approach emphasizes the problems researchers encounter rather than providing a litany of solutions with only modest context. We discuss a real scientific problem, thoughtfully consider the data that could be used to answer that problem, investigate experimental designs that would be useful, and pursue statistical models to make informed decisions in the context of how the data was collected. The focus of the book is on the statistical modeling portion but problems are viewed holistically. We purposefully introduce the linear model framework in matrix form early on and place most of the methodologies under that umbrella. This helps the reader to see the methods as a part of a general framework as opposed to a set of tools within a toolbox. We believe that the book should be appropriate for a graduate level student that has some comfort in mathematics, particularly linear algebra. Both SAS and R are used throughout to make sure the book works for a wide audience of practitioners.

\hypertarget{software}{%
\section{Software}\label{software}}

At this point, software is a requirement for statistics in practice. There are many available software solutions ranging from point and click to full on programming. We've decided to focus on R and SAS for this book.

\begin{itemize}
\item
  R is an open source, platform agnostic, software that is widely used by statisticians. We'll use the RStudio interactive development environment to write and execute our R code.
\item
  SAS is an extremely powerful and widely used software for modelling and analysis. It requires a license, but for those without one, SAS University Edition can be installed for free and is also platform agnostic. We'll use the SAS Studio environment that comes with University Edition.
\end{itemize}

As we progress through the book we'll include graphs, descriptive statistics, and analyses from R and/or SAS. At the end of each chapter a section explaining how to create these in both R and SAS is included. The following sections give a brief introduction to each software that should prepare you for what's ahead!

You'll also notice a certain style to the way our code is written. Good programming practices (GPPs) are essential for improving productivity and collaborating with others - including future you! There are a lot of guidelines and resources about GPPs available. We'll cover just a few of the essentials here.

\begin{itemize}
\item
  Include a header at the top of the program that gives the author, date, and purpose of the program.
\item
  Place comments throughout the program explaining the purpose of different chunks of code as well as your thought process.
\item
  Spacing and indentation should be used throughout for readability of the program.
\item
  Group sections of your code that serve a certain purpose together.
\item
  Use a consistent naming scheme such as camelCase or underscores\_between\_words.
\end{itemize}

Many of these and other GPPs can be taken care of by programming in a notebook environment such as JUPYTER (which can include SAS) or R Markdown. Also using a version control software such as Git and Github is really useful!

\hypertarget{r}{%
\section{R}\label{r}}

The general workflow for programming in R involves taking raw data and importing it into R. Once that data is imported we often create numerical and graphical summaries of the data. An appropriate model and statistical methods are then applied.

\begin{center}\includegraphics[width=0.8\linewidth]{img/RWorkFlow} \end{center}

At the end of this section the reader should be able to do the following:

\begin{itemize}
\tightlist
\item
  install R and RStudio\\
\item
  read and write basic R programs\\
\item
  import well-formatted data into R
\item
  do basic data manipulation in R
\end{itemize}

As the book progresses the steps of summarizing and analyzing the data will be covered. Let's get started!

\hypertarget{installing-r-and-rstudio}{%
\subsection{Installing R and RStudio}\label{installing-r-and-rstudio}}

The R software itself can be downloaded and installed by visiting the \href{https://cran.r-project.org/}{Comprehensive R Archive Network (Cran) website}. Here there are links to install R for Linux, Mac, and Windows-based machines.

\begin{itemize}
\item
  For Windows users, follow the initial `Download R for Windows' link and then click `install R for the first time.' From here you should now see a Download R X.x.x for Windows link that will download a .exe file. Once downloaded run that file and follow the prompts.
\item
  For Mac users, follow the initial `Download R for (Mac) OS X' link and click on the link near the `Latest Release' section similar to R-x.x.x.pkg. Once downloaded, you should be able to install by double clicking on the file.
\item
  For Linux users, follow the initial `Download R for Linux' link. Choose your OS and instructions are given on how to download R.
\end{itemize}

Once you've installed R you'll want to install RStudio. RStudio is a well-developed environment that makes programming in R much easier! To download head to \href{https://rstudio.com/products/rstudio/download/}{RStudio's download page}. From here choose RStudio Desktop (Open Source License) and a page with appropriate links to install are provided.

\hypertarget{using-rstudio}{%
\subsection{Using RStudio}\label{using-rstudio}}

To program in R you'll want to open RStudio. RStudio will submit R code for you so you never actually need to open R itself.

There are four main `areas' of the RStudio IDE:

\begin{itemize}
\item
  Console (\& Terminal)
\item
  Scripting and Viewing Window
\item
  Plots/Help (\& Files/Packages)
\item
  Environment (\& Connections/Git)
\end{itemize}

You may wish to rearrange the panes. This can be done via the menus at the top. Choose ``Tools --\textgreater{} Global Options''.

\begin{center}\includegraphics[width=0.8\linewidth]{img/panes} \end{center}

Other useful global options to change are under the appearance tab (font size, theme) and under the code tab (editing --\textgreater{} soft-wrap, display --\textgreater{} show whitespace).

\hypertarget{console}{%
\subsubsection{Console}\label{console}}

To evaluate code you can type directly into the \textbf{console}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#simple math operations}
\CommentTok{# <-- is a comment - code not evaluated}
\DecValTok{3} \OperatorTok{+}\StringTok{ }\DecValTok{7}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#exp is exponential function}
\DecValTok{10} \OperatorTok{*}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 200.8554
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#log is the natural logarithm (base exp(1)) by default}
\CommentTok{#pi is the built-in constant}
\KeywordTok{log}\NormalTok{(pi}\OperatorTok{^}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.28946
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(cars}\OperatorTok{$}\NormalTok{speed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(cars}\OperatorTok{$}\NormalTok{speed)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-35-1} \end{center}

In the R sections of the book we spend much of our time learning the R syntax needed to create the appropriate summaries or analysis.

\hypertarget{scripting-and-viewing-window}{%
\subsubsection{Scripting and Viewing Window}\label{scripting-and-viewing-window}}

Usually you don't want to type code directly into the console because there isn't an easy way to get the code for later use. Instead code is usually written in an R `script' which is then saved.

From an R script you can send code to console via:

\begin{itemize}
\item
  ``Run'' button (runs current line)
\item
  CTRL+Enter (PC and Linux) or Command+Enter (MAC)
\item
  Highlight section and do above
\end{itemize}

To create a new R script you can use the menus at the top and go to File --\textgreater{} New File --\textgreater{} R Script. Take a moment and do this! Type the following into your script:

\begin{itemize}
\item
  \texttt{View(cars)} (note capital \texttt{V})
\item
  \texttt{plot(cars)}
\end{itemize}

Submit it to the console using a button or hot key!

\hypertarget{plotshelp}{%
\subsubsection{Plots/Help}\label{plotshelp}}

Created plots are stored in the \texttt{Plots} tab. This is a nice feature that allows you to cycle through past plots and easily save plots via menus.

In this pane there is also a \texttt{Help} tab that will enable you to learn about R functions. In the console type \texttt{help(hist)} for instance. Information about the \texttt{hist} function is presented. Being able to parse these types of help files is a really useful skill!

For every R function there are a few sections:

\begin{itemize}
\item
  Description - What the function is intended for.
\item
  Usage - How to call the function, inputs required, and which inputs have default arguments.

  \begin{itemize}
  \tightlist
  \item
    Here we see \texttt{hist(x,\ ...)}. This implies there is only one required input, \texttt{x}, and there is no default. The ellipsis \texttt{(...)} is an important tool that gives a function (say A) that calls another function (say B) the flexibility to supply arguments specific to function B when calling function A. We'll learn more about this in later chapters.\\
  \item
    Below you see a more detailed call to \texttt{hist} that includes other inputs. Each of these inputs has an equal sign with a value after it. This is the default value for that input (since there is a default value you don't have to specify it when you call). For instance, the \texttt{breaks\ =\ "Sturges"} input implies that the ``Sturges'' method is the default for determining how the bins of the histogram are created.
  \end{itemize}
\item
  Arguments - Describes the input requirements in more detail.
\item
  Details - Information about how the function works.
\item
  Values - Information about what is returned to the user.
\item
  References
\item
  See Also - Related functions.
\item
  Examples - Highly useful section giving code you can copy and paste to see an example of how the function can be used.
\end{itemize}

\hypertarget{environment}{%
\subsubsection{Environment}\label{environment}}

R stores \textbf{data/info/functions/etc.} in R objects. An object is a data structure having attributes and methods (more on this shortly). You can create an R object via \texttt{\textless{}-} (recommended) or \texttt{=}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#save for later}
\NormalTok{avg <-}\StringTok{ }\NormalTok{(}\DecValTok{5} \OperatorTok{+}\StringTok{ }\DecValTok{7} \OperatorTok{+}\StringTok{ }\DecValTok{6}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{3}
\CommentTok{#call avg object}
\NormalTok{avg}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#strings (text) can be saved as well}
\NormalTok{words <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Hello there!"}\NormalTok{, }\StringTok{"How are you?"}\NormalTok{)}
\NormalTok{words}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello there!" "How are you?"
\end{verbatim}

Notice that when you send the line \texttt{avg\ \textless{}-\ (5+\ 7\ +\ 6)\ /\ 3} to the console (i.e.~create the object \texttt{avg}) that nothing prints out. This is common behavior when storing the object. The output or information is saved for later use in the object. To see the output or information you then simply call the object (a default printing method is used to display it).

You can look at all current objects with \texttt{ls()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "avg"   "words"
\end{verbatim}

Use \texttt{rm()} to remove an object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(avg)}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "words"
\end{verbatim}

Built-in objects exist like \texttt{letters} and \texttt{cars}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
## [20] "t" "u" "v" "w" "x" "y" "z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(cars, }\DataTypeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   speed dist
## 1     4    2
## 2     4   10
## 3     7    4
\end{verbatim}

The function \texttt{data()} shows available built-in datasets.

You should now be roughly familiar with the four main `areas' of the RStudio IDE:

\begin{itemize}
\item
  Console (\& Terminal)
\item
  Scripting and Viewing Window
\item
  Plots/Help (\& Files/Packages)
\item
  Environment (\& Connections/Git)
\end{itemize}

\hypertarget{r-objects-and-classes}{%
\subsection{R Objects and Classes}\label{r-objects-and-classes}}

R has strong \textbf{O}bject \textbf{O}riented \textbf{P}rogramming (OOP) tools.

\begin{itemize}
\item
  Object: data structure with attributes (class)
\item
  Method: procedures (functions) that act on object based on attributes
\end{itemize}

R functions like \texttt{print()} or \texttt{plot()} act differently depending on an object's class.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-41-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "function"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(exp)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-43-1} \end{center}

Many R functions exist to help understand an R Object.

\begin{itemize}
\tightlist
\item
  \texttt{str()} (structure)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    50 obs. of  2 variables:
##  $ speed: num  4 4 7 7 8 9 10 10 10 11 ...
##  $ dist : num  2 10 4 22 16 10 18 26 34 17 ...
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{class()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{typeof()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

We'll use these functions later to help us know how to extract information from an R object.

Recall that we can create an R object via \texttt{\textless{}-} (recommended) or \texttt{=}. This allocates computer memory to the object. The object's attributes depend on how you created it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\KeywordTok{class}\NormalTok{(vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(dist }\OperatorTok{~}\StringTok{ }\NormalTok{speed, }\DataTypeTok{data =}\NormalTok{ cars)}
\KeywordTok{class}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "lm"
\end{verbatim}

\hypertarget{data-objects}{%
\subsection{Data Objects}\label{data-objects}}

To understand how to use R for data analysis we need to understand commonly used data structures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Atomic Vector (1D)\\
\item
  Matrix (2D)\\
\item
  Array (nd) (not covered)\\
\item
  Data Frame (2D)\\
\item
  List (1D)
\end{enumerate}

\hypertarget{atomic-vector}{%
\subsubsection{Atomic Vector}\label{atomic-vector}}

Let's start with the most basic object and work our way up. An atomic vector is a 1D group of elements with an ordering. Two examples of atomic vectors are given below:

\begin{center}\includegraphics[width=0.8\linewidth]{img/vectorVisualF} \end{center}

All of the elements must be same `type'. Types include numeric (integer or double), character, or logical. We create an atomic vector with the \texttt{c()} function (`combine').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#vectors (1 dimensional) objects}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{-3}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"cat"}\NormalTok{, }\StringTok{"dog"}\NormalTok{, }\StringTok{"bird"}\NormalTok{, }\StringTok{"frog"}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 17 22  1  3 -3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "cat"  "dog"  "bird" "frog"
\end{verbatim}

In addition, many `functions' output a numeric vector. Functions are at the heart of R so it is vital to understand them. The concept of a function is that the function takes an input or inputs and maps those inputs to some output(s).

\begin{center}\includegraphics[width=0.8\linewidth]{img/funVisual1F} \end{center}

As an example, one function that outputs a numeric vector is the \texttt{seq} or sequence function. To know about a function you need to know about the inputs and ouputs. For \texttt{seq} we have the following:

\begin{verbatim}
+ Inputs = from, to, by (among others)  

+ Output = a sequence of numbers
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{img/funVisual2F} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1}\NormalTok{, }\DataTypeTok{to =} \DecValTok{5}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{)}
\NormalTok{v}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(v)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:5] 1 2 3 4 5
\end{verbatim}

\texttt{str} tells about the object \texttt{v}:

\begin{itemize}
\item
  \texttt{num} says it is numeric
\item
  \texttt{{[}1:5{]}} implies one dimensional with elements 1, 2, 3, 4, 5
\end{itemize}

If the order of the function arguments is known, they do not need to be named when invoking the function as R uses positional matching to determine the inputs. Also, if you name some arguments but not others, R will use positional matching for the remaining inputs. For example, the calls below are all equivalent:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1}\NormalTok{, }\DataTypeTok{to =} \DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{seq}\NormalTok{(}\DataTypeTok{to =} \DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{seq}\NormalTok{(}\DataTypeTok{by =} \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{seq} function is used quite a bit. There is a shorthand way to create an integer sequence using \texttt{:}.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{20} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

It is also important to know how R does math on its objects. R does elementwise addition/subtraction and multiplication/division to vectors, matrices, and data frames. (The matrix multiplicaiton operator is \texttt{\%*\%}.).

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{20}\OperatorTok{/}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
## [16] 0.80 0.85 0.90 0.95 1.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{20} \OperatorTok{+}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21
\end{verbatim}

As we mentioned earlier, understanding help files is really critical to being able to program in R. As functions are ubiquitous in R we often need to learn about their inputs (or arguments) and we can do so using \texttt{help}.

To recap, our first commonly used R object for storing data is an atomic vector. This is a 1D group of elements with an ordering where all of the elements are of the same type. Generally vectors are useful to know about but not usually useful for a storing a dataset exactly. They can often be considered as the `building blocks' for other data types.

\hypertarget{matrix}{%
\subsubsection{Matrix}\label{matrix}}

A Matrix is a 2D data structure in R whose elements are all of the same type. The first dimension refers to the rows and the second dimension refers to the columns. A 2D data object is very common. The rows often represent the \emph{observations} and the columns represent the \emph{variables}. Although not technically right, it is useful to think of the columns of a matrix as vectors of the same \textbf{type and length}.

\begin{center}\includegraphics[width=0.75\linewidth]{img/matrixVisualF} \end{center}

For instance, consider the three vectors created here:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#populate vectors}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{11}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DataTypeTok{times =} \DecValTok{4}\NormalTok{)}
\NormalTok{z <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

These are all of the same type. This can be checked with an \texttt{is.} (read as `is dot') function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#check 'type'}
\KeywordTok{is.numeric}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.numeric}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.numeric}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Not only are these three objects the same type but they are also the same length. This can be checked using the \texttt{length} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#check 'length'}
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

Again, it is useful to visualize the columns of a potential matrix as these vectors. We can create the matrix using the \texttt{matrix} function. The \texttt{matrix} function requires us to give the data as one vector. We can combine the \texttt{x}, \texttt{y}, and \texttt{z} objects into one vector using the \texttt{c} funciton. This is the first argument to the \texttt{matrix} function. The only other argument required is to either specify the number of rows (\texttt{nrow\ =}) or the number of columns (\texttt{ncol\ =}) (R will attempt to figure out the one that is not given using the total length of the specified data vector).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#combine in a matrix}
\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(x, y, z), }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]   17   -3    1
## [2,]    3   -3    2
## [3,]   13   -3    3
## [4,]   11   -3    4
\end{verbatim}

A matrix can also store character data as well. An example of this is given below and the number of rows is specified rather than the number of columns. Note the use of \texttt{is.character} from the \texttt{is.} family of functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Hi"}\NormalTok{, }\StringTok{"There"}\NormalTok{, }\StringTok{"!"}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"One"}\NormalTok{, }\StringTok{"Two"}\NormalTok{, }\StringTok{"Three"}\NormalTok{)}
\KeywordTok{is.character}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(x, y, z), }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]    [,2] [,3]   
## [1,] "Hi"    "a"  "One"  
## [2,] "There" "b"  "Two"  
## [3,] "!"     "c"  "Three"
\end{verbatim}

To recap, a Matrix is a 2D data structure where we can think of the columns as vectors of the same \textbf{type and length}. These are useful for some datasets but most datasets have some numeric and some character variables.

\begin{center}\includegraphics[width=0.8\linewidth]{img/dataset} \end{center}

Another 2D object called a data frame is perfect for this type of data!

\hypertarget{data-frame}{%
\subsubsection{Data Frame}\label{data-frame}}

A Data Frame is a 2D data structure where elements within a column must be of the same type but the columns themselves can differ in type. When thinking of a data frame, consider them as a collection (list) of vectors of the same \textbf{length}.

\begin{center}\includegraphics[width=0.8\linewidth]{img/dfVisualF} \end{center}

A data frame can be created with the \texttt{data.frame} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{, }\StringTok{"e"}\NormalTok{, }\StringTok{"f"}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{-1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{z <-}\StringTok{ }\DecValTok{10}\OperatorTok{:}\DecValTok{15}
\KeywordTok{data.frame}\NormalTok{(x, y, z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x  y  z
## 1 a  1 10
## 2 b  3 11
## 3 c  4 12
## 4 d -1 13
## 5 e  5 14
## 6 f  6 15
\end{verbatim}

You can also easily name the columns during creation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{char =}\NormalTok{ x, }\DataTypeTok{data1 =}\NormalTok{ y, }\DataTypeTok{data2 =}\NormalTok{ z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   char data1 data2
## 1    a     1    10
## 2    b     3    11
## 3    c     4    12
## 4    d    -1    13
## 5    e     5    14
## 6    f     6    15
\end{verbatim}

Notice that char, data1, and data2 become the variable names for the data frame.

To recap, consider a data frame as a collection (list) of vectors of the same \textbf{length}. Tis type of data structure is perfect for most data sets! Most functions that read 2D data into R store it as a data frame.

\hypertarget{list}{%
\subsubsection{List}\label{list}}

A List is a 1D group of objects with ordering. Really it is a vector that can have differing elements. Think of this in a similar way to the atomic vector previously discussed except the elements are really flexible.

\begin{center}\includegraphics[width=0.8\linewidth]{img/listVisualF} \end{center}

A list can be created with the \texttt{list} function. You specify the elements you want to include, separated by commas.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{list}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"!"}\NormalTok{, }\StringTok{"?"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1 2 3
## 
## [[2]]
## [1] -0.5859762  1.5969464
## 
## [[3]]
## [1] "!" "?"
\end{verbatim}

Similar to a data frame, you can add names to the list elements during creation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{list}\NormalTok{(}\DataTypeTok{seq =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{normVals =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{), }\DataTypeTok{punctuation =} \KeywordTok{c}\NormalTok{(}\StringTok{"!"}\NormalTok{, }\StringTok{"?"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $seq
## [1] 1 2 3
## 
## $normVals
## [1] -0.12990285  0.04792657
## 
## $punctuation
## [1] "!" "?"
\end{verbatim}

To recap, a list is a very flexible 1D object. It is really useful for more complex types of data.

The table below gives a summary of the data objects we've covered. For most data analysis you'll use data frames.

\begin{longtable}[]{@{}lll@{}}
\toprule
Dimension & Homogeneous & Heterogeneous\tabularnewline
\midrule
\endhead
1d & Atomic Vector & List\tabularnewline
2d & Matrix & Data Frame\tabularnewline
\bottomrule
\end{longtable}

Next we look at how to access or change parts of these common data objects.

\hypertarget{accessing-common-data-objects}{%
\subsection{Accessing Common Data Objects}\label{accessing-common-data-objects}}

When we are dealing with a data object (1D or 2D) we may want to extract a single element, certain columns, or certain rows. In this section we'll look at how to subset or extract information from each of the common data objects covered in the previous section.

\hypertarget{atomic-vector-1d}{%
\subsubsection{Atomic Vector (1D)}\label{atomic-vector-1d}}

For atomic vectors (and lists, see later) you can return elements using square brackets \texttt{{[}{]}}. You may notice that when R prints a vector to the console you often see \texttt{{[}1{]}} next to the first element and perhaps a \texttt{{[}\#{]}} where R has to break and move to the next line of the console. The \texttt{{[}1{]}} implies the element printed next is the first element of the vector (R starts its counting at 1 not 0 like some other languages). The \texttt{{[}\#{]}} implies that the element printed to the right is the \texttt{\#} element of the vector. This is a good reminder of how to extract values from an atomic vector.

As an example, here we extract from a built-in R object called letters that is a vector of length 26 containing the letters of the alphabet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters }\CommentTok{#built-in vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
## [20] "t" "u" "v" "w" "x" "y" "z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters[}\DecValTok{1}\NormalTok{] }\CommentTok{#R starts counting at 1!}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters[}\DecValTok{26}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "z"
\end{verbatim}

To obtain more than one element you can `feed' in a vector of indices to that you'd like to return.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a" "b" "c" "d"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters[}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "e" "j" "o" "t" "y"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{letters[x]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a" "b" "e"
\end{verbatim}

If you'd like to return all values except a certain subset, you can use negative indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letters[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w"
## [20] "x" "y" "z"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{letters[}\OperatorTok{-}\NormalTok{x]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "c" "d" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v"
## [20] "w" "x" "y" "z"
\end{verbatim}

\hypertarget{matrices-2d}{%
\subsubsection{Matrices (2D)}\label{matrices-2d}}

For rectangular data like a matrix you can return rectangular subsets using square brackets with a comma \texttt{{[}\ ,\ {]}}. Notice default row and column names when R prints a matrix!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{20}\OperatorTok{:}\DecValTok{17}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1   20
## [2,]    2   19
## [3,]    3   18
## [4,]    4   17
\end{verbatim}

This is a nice reminder of how to index a matrix. The value prior to the columns represents which row(s) you want to return and the value after the comma which column(s). If an index is left blank then all of that corresponding dimension (row or column) is returned.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    2   19
## [2,]    4   17
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\DecValTok{2}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Notice that R simplifies the result where possible. That is, returns an atomic vector if you have only 1 dimension and a matrix if two. This can be changed by adding an additional argument to the \texttt{{[}} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[ , }\DecValTok{1}\NormalTok{, drop =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    1
## [2,]    2
## [3,]    3
## [4,]    4
\end{verbatim}

Also, if you only give a single value in the \texttt{{[}{]}} then R uses the count of the value in the matrix. Counts go down columns first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

If your matrix has column names associated with it, you can also use those to return columns of interest. To add column names we can look run \texttt{help(matrix)} to learn how! Notice the \texttt{dimnames} argument. You can specify names for the rows and columns by using a list with two vectors. The first vector indicating row names and the second column names. If we don't want to give row names we can give a NULL (a special value in R that is used for undefined values - here giving no specification of row names). We can do this and give a character vector for the column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{20}\OperatorTok{:}\DecValTok{17}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{,}
            \DataTypeTok{dimnames =} \KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"First"}\NormalTok{, }\StringTok{"Second"}\NormalTok{))}
\NormalTok{            )}
\NormalTok{mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      First Second
## [1,]     1     20
## [2,]     2     19
## [3,]     3     18
## [4,]     4     17
\end{verbatim}

Now we can request columns be using a single name or a character vector of names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[, }\StringTok{"First"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4
\end{verbatim}

To return all but certain parts of a matrix you can still use negative indices but note that this won't work with column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\OperatorTok{-}\StringTok{"First"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in -"First": invalid argument to unary operator
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\StringTok{"First"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 4
\end{verbatim}

\hypertarget{data-frames-2d}{%
\subsubsection{Data Frames (2D)}\label{data-frames-2d}}

Since a data frame is also a rectangular data object you can return rectangular subsets using square brackets with a comma \texttt{{[}\ ,\ {]}}!

As an example, we'll subset the built-in \texttt{iris} data frame. To get an idea about this object we can run \texttt{str(iris)}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

We can see this is a data frame with a few columns, four are numeric and one is a factor (a special type of character vector essentially - these will be covered when we discuss plotting).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Width Petal.Length Petal.Width
## 1         3.5          1.4         0.2
## 2         3.0          1.4         0.2
## 3         3.2          1.3         0.2
## 4         3.1          1.5         0.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[}\DecValTok{1}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1
##  [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0
##  [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5
##  [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1
##  [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5
##  [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

Notice the simplification done when a single column is selected. R will simplify to a vector unless \texttt{drop\ =\ FALSE} is included as done in the matrix section. (The simplification doesn't occur when a single row is selected because data frames are actually lists - we'll discuss this more in the list section!)

You can use columns names to subset as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{ , }\KeywordTok{c}\NormalTok{(}\StringTok{"Sepal.Length"}\NormalTok{, }\StringTok{"Species"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Sepal.Length Species
## 1           5.1  setosa
## 2           4.9  setosa
## 3           4.7  setosa
## 4           4.6  setosa
## 5           5.0  setosa
## 6           5.4  setosa
## 7           4.6  setosa
## 8           5.0  setosa
## 9           4.4  setosa
## 10          4.9  setosa
\end{verbatim}

The most common way to access a single column is to use the dollar sign operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{Sepal.Length}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1
##  [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0
##  [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5
##  [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1
##  [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5
##  [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

A nice benefit of using RStudio is that column names will be filled in automatically as you type. In your console do the following:

\begin{itemize}
\tightlist
\item
  Type \texttt{iris\$}\\
\item
  If no choices - hit tab
\item
  Scroll up and down or continue typing to highlight the column of interest\\
\item
  Hit tab again to choose
\end{itemize}

\hypertarget{lists-1d}{%
\subsubsection{Lists (1D)}\label{lists-1d}}

As a list is a 1D data object we can use single square brackets \texttt{{[}\ {]}} for multiple list elements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"HI"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "HI"
## 
## [[2]]
##  [1] 10 11 12 13 14 15 16 17 18 19 20
## 
## [[3]]
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
##  [1] 10 11 12 13 14 15 16 17 18 19 20
## 
## [[2]]
## [1] 1
\end{verbatim}

We can use double square brackets \texttt{{[}{[}\ {]}{]}} (or \texttt{{[}\ {]}}) to return a single list element. The major difference is in whether or not a list with the element chosen is returned or just the element itself. \texttt{{[}{[}} will return just the element requested.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"HI"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\NormalTok{x[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "HI"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "HI"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 10 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{2}\NormalTok{]][}\DecValTok{4}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13 14
\end{verbatim}

Recall we could name our list elements. If they are named we can use the \texttt{\$} similar to a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{"HI"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\KeywordTok{str}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ : chr "HI"
##  $ : int [1:11] 10 11 12 13 14 15 16 17 18 19 ...
##  $ : num 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{First =} \StringTok{"Hi"}\NormalTok{, }\DataTypeTok{Second =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{), }\DataTypeTok{Third =} \DecValTok{1}\NormalTok{)}
\NormalTok{x}\OperatorTok{$}\NormalTok{Second}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 10 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

Under the hood a data frame is just a \emph{list} of equal length vectors!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ First : chr "Hi"
##  $ Second: int [1:11] 10 11 12 13 14 15 16 17 18 19 ...
##  $ Third : num 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

This means we can index a data frame in a similar way to how we index a list if we want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 3.7 3.4 3.0 3.0 4.0 4.4 3.9 3.5
##  [19] 3.8 3.8 3.4 3.7 3.6 3.3 3.4 3.0 3.4 3.5 3.4 3.2 3.1 3.4 4.1 4.2 3.1 3.2
##  [37] 3.5 3.6 3.0 3.4 3.5 2.3 3.2 3.5 3.8 3.0 3.8 3.2 3.7 3.3 3.2 3.2 3.1 2.3
##  [55] 2.8 2.8 3.3 2.4 2.9 2.7 2.0 3.0 2.2 2.9 2.9 3.1 3.0 2.7 2.2 2.5 3.2 2.8
##  [73] 2.5 2.8 2.9 3.0 2.8 3.0 2.9 2.6 2.4 2.4 2.7 2.7 3.0 3.4 3.1 2.3 3.0 2.5
##  [91] 2.6 3.0 2.6 2.3 2.7 3.0 2.9 2.9 2.5 2.8
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

Lastly, one nice thing about lists (and data frames) is that you can use partial matching with \texttt{{[}{[}} and \texttt{\$}. That is, you don't need to specify the full column name as long as you've specified enough characters as to be clear which column you are referring to.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{Sp[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa
## Levels: setosa versicolor virginica
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[[}\StringTok{"Petal.Len"}\NormalTok{, exact =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4
##  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2
##  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0
##  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0
##  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0
##  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

This is less important now that RStudio can auto-complete long column names.

\hypertarget{basics-of-r-recap}{%
\subsection{Basics of R Recap}\label{basics-of-r-recap}}

\begin{itemize}
\item
  RStudio IDE (Integrated Development Environment)
\item
  R Objects and Classes
\item
  Data Objects \& Basic Manipulation
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule
Dimension & Homogeneous & Heterogeneous\tabularnewline
\midrule
\endhead
1d & Atomic Vector & List\tabularnewline
2d & Matrix & Data Frame\tabularnewline
\bottomrule
\end{longtable}

Basic access via

\begin{itemize}
\item
  Atomic vectors - \texttt{x{[}\ {]}}
\item
  Matrices - \texttt{x{[}\ ,\ {]}}
\item
  Data Frames - \texttt{x{[}\ ,\ {]}} or \texttt{x\$name}
\item
  Lists - \texttt{x{[}\ {]}}, \texttt{x{[}{[}\ {]}{]}}, or \texttt{x\$name}
\end{itemize}

\hypertarget{reading-data-basics}{%
\subsection{Reading Data Basics}\label{reading-data-basics}}

When it comes to reading in data, where do we start? Our plan for this section is as follows:

\begin{itemize}
\item
  Look at common raw data formats
\item
  Take a few quick asides: R projects, \texttt{factors}, and R packages
\item
  Read `clean' delimited data
\item
  Read Excel, SAS, \& SPSS data
\item
  Resources for JSON data, XML data, databases, and APIs
\end{itemize}

\textbf{How to read in data depends on raw/external data type!}

We'll start by focusing on delimited data.

\begin{itemize}
\tightlist
\item
  Delimiter - Character (such as a\texttt{,}) that separates data entries
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img/delimitedData} \end{center}

To read in data we'll need functions to do so. When you open R a few \texttt{packages} are loaded.

\begin{center}\includegraphics[width=0.2\linewidth]{img/loadR} \end{center}

R Packages:

\begin{itemize}
\item
  Collection of functions/datasets/etc. in one place
\item
  Packages exist to do almost anything
\item
  \href{https://cran.r-project.org/web/packages/available_packages_by_name.html}{List of CRAN} approved packages on R's website
\item
  Plenty of other packages on places like GitHub
\end{itemize}

The \texttt{utils} package that automatically loads has a \emph{family} of \texttt{read.} functions ready for use! Reading data with these functions is often referred to as reading with a standard R or base R method.

Function and purpose:

\begin{longtable}[]{@{}ll@{}}
\toprule
Type of Delimeter & Function\tabularnewline
\midrule
\endhead
Comma & \texttt{read.csv()}\tabularnewline
Semicolon (\texttt{,} for decimal) & \texttt{read.csv2()}\tabularnewline
Tab & \texttt{read.delim()}\tabularnewline
White Space/General & \texttt{read.table(sep\ =\ "")}\tabularnewline
\bottomrule
\end{longtable}

Each of these functions requires a \textbf{path} to the file in order to read it in. Let's read in the `\href{https://www4.stat.ncsu.edu/~online/datasets/neuralgia.csv}{neuralgia.csv}' file. This is a comma separated value file (.csv). This requires the \texttt{read.csv} function.

R locates the file by the path you give it. You can give \emph{full path name}. For example,

\begin{itemize}
\tightlist
\item
  ex: C:/Users/jbpost2/repos/StatisticalMethods/datasets/neuralgia.csv\\
\item
  ex: C:\textbackslash{}\textbackslash{}Users\textbackslash{}\textbackslash{}jbpost2\textbackslash{}\textbackslash{}repos\textbackslash{}\textbackslash{}StatisticalMethods\textbackslash{}\textbackslash{}datasets\textbackslash{}\textbackslash{}neuralgia.csv
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img/pathVis} \end{center}

Notice that a double \texttt{\textbackslash{}} is needed because \texttt{\textbackslash{}} is an escape character in R so \texttt{\textbackslash{}\textbackslash{}} is really read as \texttt{\textbackslash{}}.

Ok, let's read in the neuralgia csv file using \texttt{read.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}
           \StringTok{"C:/Users/jbpost2/repos/StatisticalMethods/datasets/neuralgia.csv"}
\NormalTok{           )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(neuralgiaData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Treatment Sex Age Duration Pain
## 1         P   F  68        1   No
## 2         B   M  74       16   No
## 3         P   F  67       30   No
## 4         P   M  66       26  Yes
## 5         B   F  67       28   No
## 6         B   F  77       16   No
\end{verbatim}

Pretty simple if the data is nicely formatted! Using a full local path is not recommended though! Doing so makes it difficult to share code without having to go in and change the paths. Instead, you can change the \emph{working directory} R is using. That is, the folder by default R is `looking' for files. Then we can supply a \textbf{relative} path. As long as other users have the same folder structure as you (say if you are using a github repo), no changes need to be made for them to run the code!

We can determine the working directory using \texttt{getwd}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "C:/Users/jbpost2/repos/StatisticalMethods/software/R"
\end{verbatim}

This can be changed using \texttt{setwd}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{"C:/Users/jbpost2/repos/StatisticalMethods/datasets"}\NormalTok{)}
\CommentTok{#or}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"C:}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{Users}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{jbpost2}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{repos}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{StatisticalMethods}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{datasets"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The working directory can also be changed via the menus in RStudio.

\begin{center}\includegraphics[width=0.8\linewidth]{img/setwd} \end{center}

Another way to supply a path is via a URL. This is really handy if you have a place to host your datasets!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/neuralgia.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To recap, to read a csv file you can

\begin{itemize}
\item
  Use full local path (not recommended)
\item
  Use relative path

  \begin{itemize}
  \tightlist
  \item
    set working directory with \texttt{setwd()}
  \end{itemize}
\item
  Pull from a URL
\end{itemize}

\hypertarget{quick-aside-rstudio-project}{%
\subsection{Quick Aside: RStudio Project}\label{quick-aside-rstudio-project}}

Often we have many files associated with an analysis. When working on multiple undertakings things get cluttered in R\ldots{} With each analysis we may want to associate different

\begin{verbatim}
+ environments  
+ histories  
+ working directories  
+ source documents.  
\end{verbatim}

The ``Project'' feature in R Studio allows us to easily do this! To create you can use the drop down menus.

\begin{center}\includegraphics[width=0.8\linewidth]{img/project} \end{center}

Now you can easily switch between analyses by using ``File --\textgreater{} Open Project'' or by using the little drop down menu in the top right of RStudio.

\hypertarget{quick-aside-factors}{%
\subsection{Quick Aside: Factors}\label{quick-aside-factors}}

As mentioned above there are \texttt{read.} functions for many different types of delimited data. These functions work really well but there are a few areas they could be improved.

\begin{itemize}
\tightlist
\item
  A poor default function behavior as strings are read as \texttt{factors}
\end{itemize}

Understanding factors is important enough to warrant a quick discussion. Let's look at the structure of our neuralgiaData object we read in with \texttt{read.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(neuralgiaData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    60 obs. of  5 variables:
##  $ Treatment: Factor w/ 3 levels "A","B","P": 3 2 3 3 2 2 1 2 2 1 ...
##  $ Sex      : Factor w/ 2 levels "F","M": 1 2 1 2 1 1 1 1 1 2 ...
##  $ Age      : int  68 74 67 66 67 77 71 72 76 71 ...
##  $ Duration : int  1 16 30 26 28 16 12 50 9 17 ...
##  $ Pain     : Factor w/ 2 levels "No","Yes": 1 1 1 2 1 1 1 1 2 2 ...
\end{verbatim}

We can see that all of the character variables are \texttt{Factor} vectors. A factor is a special class of vector with a \texttt{levels} attribute. The levels define all possible values for that variable. This is a great concept for a variable that can only take on certain values such as \texttt{Day} (Monday, Tuesday, \ldots{}, Sunday). However, if you have a variable like \texttt{Name} that you will eventually add new values (levels) to factors become a bit of a nuisance.

For example, in the neuralgia dataset we may have a fourth treatment we want to add to the \texttt{Treatment} variable. Let's try to assign the first observation value with a `new' treatment called `M'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData}\OperatorTok{$}\NormalTok{Treatment}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] P B P P B B A B B A A A B A P A P A P B B A A A B P B B P P A A B B B A P B
## [39] B P P P A B A P P A B P P P B A P A P A B A
## Levels: A B P
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData}\OperatorTok{$}\NormalTok{Treatment[}\DecValTok{1}\NormalTok{] <-}\StringTok{ "M"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in `[<-.factor`(`*tmp*`, 1, value = structure(c(NA, 2L, 3L, 3L, :
## invalid factor level, NA generated
\end{verbatim}

We can see this throws an error because `M' is not one of the levels defined for the variable. To add the new value we have to alter the \texttt{levels} attribute of the factor.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#overwrite with another possible level}
\KeywordTok{levels}\NormalTok{(neuralgiaData}\OperatorTok{$}\NormalTok{Treatment) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{levels}\NormalTok{(neuralgiaData}\OperatorTok{$}\NormalTok{Treatment), }\StringTok{"M"}\NormalTok{)}
\KeywordTok{levels}\NormalTok{(neuralgiaData}\OperatorTok{$}\NormalTok{Treatment)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A" "B" "P" "M"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData}\OperatorTok{$}\NormalTok{Treatment[}\DecValTok{1}\NormalTok{] <-}\StringTok{ "M"}
\end{Highlighting}
\end{Shaded}

Factors are very useful for plotting as we'll see later.

For the other issues with the \texttt{read.} family we can look at useful functions from other R packages. R packages deserve a brief discussion as well!

\hypertarget{quick-aside-r-packages}{%
\subsection{Quick Aside: R Packages}\label{quick-aside-r-packages}}

An R package is a collection of functions in one place. There are tons of packages to do most anything. In particular a group of packages called the ``\href{http://tidyverse.org/}{TidyVerse}'' has modernized the use of R for a larger audience. The tidyverse is a package that is a collection of eight R packages that share common philosophies and are designed to work together! One of these packages, \texttt{readr}, is extremely useful for reading in data and remedies the concerns mentioned above about the \texttt{read.} family of functions.

The first time using a package you must `install' the package (download the files). You can do this

\begin{itemize}
\tightlist
\item
  Using code:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\CommentTok{#can do multiple packages at once}
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"readr"}\NormalTok{, }\StringTok{"readxl"}\NormalTok{, }\StringTok{"haven"}\NormalTok{, }\StringTok{"DBI"}\NormalTok{, }\StringTok{"httr"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Using menus:
\end{itemize}

\begin{center}\includegraphics[width=0.7\linewidth]{img/packages} \end{center}

Note that you can also install packages from local sources (such as a downloaded .zip or .tar) but that isn't usually required unless you are behind a firewall or R updates and the packages haven't been updated for that version of R.

The good thing is that you only need to install the packages once! However, this doesn't mean you have direct access to your package functions or datasets in your R session. \textbf{Each R session} you open you need to read in the package using \texttt{library()} or \texttt{require()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"readr"}\NormalTok{)}
\KeywordTok{require}\NormalTok{(}\StringTok{"haven"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These functions are very similar; they both give you direct access to the functions or data in your R session. The difference is that if you try to load a package that doesn't exist \texttt{library} throws an error where \texttt{require()} returns \texttt{FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"notAPackage"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in library("notAPackage"): there is no package called 'notAPackage'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(}\StringTok{"notAPackage"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: notAPackage
\end{verbatim}

\begin{verbatim}
## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called 'notAPackage'
\end{verbatim}

Now is a good time to install the \texttt{tidyverse} package if you haven't already.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The functions in the \texttt{tidyverse} generally have

\begin{itemize}
\item
  Fast code
\item
  Easy syntax
\item
  Good default settings on functions
\item
  A nice set of examples and vignettes
\end{itemize}

Read the package into your R session.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

You'll likely see a message about functions being masked. This implies that one of the functions just loaded has a function under the same name as a function that already exists. If you type \texttt{help(filter)}, R will now give you an option of which \texttt{filter} to look at. R uses the most recently loaded function and ``masks'' the old ones. You can access specific package's functions using \texttt{::}. This allows you to call functions without loading a full library.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/neuralgia.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   Treatment = col_character(),
##   Sex = col_character(),
##   Age = col_double(),
##   Duration = col_double(),
##   Pain = col_character()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 60 x 5
##   Treatment Sex     Age Duration Pain 
##   <chr>     <chr> <dbl>    <dbl> <chr>
## 1 P         F        68        1 No   
## 2 B         M        74       16 No   
## 3 P         F        67       30 No   
## 4 P         M        66       26 Yes  
## 5 B         F        67       28 No   
## # ... with 55 more rows
\end{verbatim}

\hypertarget{reading-delimited-data}{%
\subsection{Reading Delimited Data}\label{reading-delimited-data}}

Again the \texttt{read.} functions exist to read in many different types of delimited data. These functions work really well but there are a few areas they could be improved.

\begin{itemize}
\item
  A poor default function behavior as strings are read as \texttt{factors}
\item
  Raw data row \& column names can be troublesome
\item
  Slow processing (relatively speaking)
\item
  (Slightly) different behavior on different computers
\end{itemize}

Functions from the \texttt{tidyverse} (and \texttt{readr} in particular) remedy all of these!

\begin{longtable}[]{@{}lll@{}}
\toprule
Type of Delimeter & \texttt{utils} Function & \texttt{readr}\tabularnewline
\midrule
\endhead
Comma & \texttt{read.csv()} & \texttt{read\_csv()}\tabularnewline
Semicolon (\texttt{,} for decimal) & \texttt{read.csv2()} & \texttt{read\_csv2()}\tabularnewline
Tab & \texttt{read.delim()} & \texttt{read\_tsv()}\tabularnewline
General & \texttt{read.table(sep\ =\ "")} & \texttt{read\_delim()}\tabularnewline
White Space & \texttt{read.table(sep\ =\ "")} & \texttt{read\_table()} \texttt{read\_table2()}\tabularnewline
\bottomrule
\end{longtable}

Let's reread the `\href{https://www4.stat.ncsu.edu/~online/datasets/neuralgia.csv}{neuralgia.csv}' file using \texttt{read\_csv} from the \texttt{readr} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/neuralgia.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   Treatment = col_character(),
##   Sex = col_character(),
##   Age = col_double(),
##   Duration = col_double(),
##   Pain = col_character()
## )
\end{verbatim}

You can see that the package displays a bit of information about how the data was parsed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 60 x 5
##   Treatment Sex     Age Duration Pain 
##   <chr>     <chr> <dbl>    <dbl> <chr>
## 1 P         F        68        1 No   
## 2 B         M        74       16 No   
## 3 P         F        67       30 No   
## 4 P         M        66       26 Yes  
## 5 B         F        67       28 No   
## # ... with 55 more rows
\end{verbatim}

You'll also notice the fancy printing. This gives a quick check for the column type you have, which is a basic data validation step. The \texttt{tidyverse} has a special class of data frames called \texttt{tibbles}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(neuralgiaData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "spec_tbl_df" "tbl_df"      "tbl"         "data.frame"
\end{verbatim}

The behavior of \texttt{tibbles} is slightly different than that of a standard \texttt{data\ frame}. One is the printing method. The other major difference is that \texttt{tibbles} don't simplify.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData[,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 60 x 1
##   Treatment
##   <chr>    
## 1 P        
## 2 B        
## 3 P        
## 4 P        
## 5 B        
## # ... with 55 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.data.frame}\NormalTok{(neuralgiaData)[,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "P" "B" "P" "P" "B" "B" "A" "B" "B" "A" "A" "A" "B" "A" "P" "A" "P" "A" "P"
## [20] "B" "B" "A" "A" "A" "B" "P" "B" "B" "P" "P" "A" "A" "B" "B" "B" "A" "P" "B"
## [39] "B" "P" "P" "P" "A" "B" "A" "P" "P" "A" "B" "P" "P" "P" "B" "A" "P" "A" "P"
## [58] "A" "B" "A"
\end{verbatim}

As this behavior can cause some issues with functions that are expecting a vector it is useful to force simplification sometimes. You can either use the \texttt{pull} function or the \texttt{\$} operator to return a column as a vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pull}\NormalTok{(neuralgiaData, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "P" "B" "P" "P" "B" "B" "A" "B" "B" "A" "A" "A" "B" "A" "P" "A" "P" "A" "P"
## [20] "B" "B" "A" "A" "A" "B" "P" "B" "B" "P" "P" "A" "A" "B" "B" "B" "A" "P" "B"
## [39] "B" "P" "P" "P" "A" "B" "A" "P" "P" "A" "B" "P" "P" "P" "B" "A" "P" "A" "P"
## [58] "A" "B" "A"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralgiaData}\OperatorTok{$}\NormalTok{Treatment }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "P" "B" "P" "P" "B" "B" "A" "B" "B" "A" "A" "A" "B" "A" "P" "A" "P" "A" "P"
## [20] "B" "B" "A" "A" "A" "B" "P" "B" "B" "P" "P" "A" "A" "B" "B" "B" "A" "P" "B"
## [39] "B" "P" "P" "P" "A" "B" "A" "P" "P" "A" "B" "P" "P" "P" "B" "A" "P" "A" "P"
## [58] "A" "B" "A"
\end{verbatim}

One question you may have about the column types is, how did R determine the column types? The help file for \texttt{read\_csv} tells us that it checks the first 1000 rows of data and uses those to figure out the type of data. You can of course override this default behavior.

Some useful inputs you may want to change when reading in data are

\begin{itemize}
\item
  \texttt{skip\ =\ 0}
\item
  \texttt{col\_names\ =\ TRUE}
\item
  \texttt{na\ =\ c("",\ "NA")}
\end{itemize}

These allow you to skip lines of data, specify column names, and define what represents a missing value in the raw data (\texttt{NA} is the missing data indicator in R).

Generally, reading \emph{clean} delimited data is pretty easy with the \texttt{read\_} family of functions! Let's go through a few examples.

First, let's read in the space delimited file `\href{https://www4.stat.ncsu.edu/~online/datasets/chemical.txt}{chemical.txt}'. Since this is space delimited we'll use \texttt{read\_table}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_table}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/chemical.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   `temp conc time percent` = col_character()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 19 x 1
##    `temp conc time percent`
##    <chr>                   
##  1 -1 -1 -1 45.9           
##  2 1 -1 -1 60.6            
##  3 -1 1 -1 57.5            
##  4 1 1 -1 58.6             
##  5 -1 -1 1 53.3            
##  6 1 -1 1 58               
##  7 -1 1 1 58.8             
##  8 1 1 1 52.4              
##  9 -2 0 0 46.9             
## 10 2 0 0 55.4              
## 11 0 -2 0 55               
## 12 0 2 0 57.5              
## 13 0 0 -2 56.3             
## 14 0 0 2 58.9              
## 15 0 0 0 56.9              
## 16 2 -3 0 61.1             
## 17 2 -3 0 62.9             
## 18 -1.4 2.6 0.7 60         
## 19 -1.4 2.6 0.7 60.6
\end{verbatim}

Next, let's read in the tab delimited file `\href{https://www4.stat.ncsu.edu/~online/datasets/crabs.txt}{crabs.txt}'. Since this is tab delimited we'll use \texttt{read\_tsv}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_tsv}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/crabs.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   color = col_double(),
##   spine = col_double(),
##   width = col_double(),
##   satell = col_double(),
##   weight = col_double(),
##   y = col_double()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 173 x 6
##   color spine width satell weight     y
##   <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>
## 1     3     3  28.3      8   3050     1
## 2     4     3  22.5      0   1550     0
## 3     2     1  26        9   2300     1
## 4     4     3  24.8      0   2100     0
## 5     4     3  26        4   2600     1
## # ... with 168 more rows
\end{verbatim}

Lastly, let's read in the \texttt{\textgreater{}} delimited file `\href{https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt}{umps2012.txt}'. As this isn't a standard delimiter we'll use \texttt{read\_delim} and specify the \texttt{delim\ =} input. However, this file doesn't contain column names in the raw data. The columns represent Year, Month, Day, Home, Away, and HPUmpire. The column names can be specified using the \texttt{col\_names} input and specifying them with a character vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_delim}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt"}\NormalTok{, }\DataTypeTok{delim =} \StringTok{">"}\NormalTok{,}
           \DataTypeTok{col_names =} \KeywordTok{c}\NormalTok{(}\StringTok{"Year"}\NormalTok{, }\StringTok{"Month"}\NormalTok{, }\StringTok{"Day"}\NormalTok{, }\StringTok{"Home"}\NormalTok{, }\StringTok{"Away"}\NormalTok{, }\StringTok{"HPUmpire"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   Year = col_double(),
##   Month = col_double(),
##   Day = col_double(),
##   Home = col_character(),
##   Away = col_character(),
##   HPUmpire = col_character()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 2,359 x 6
##    Year Month   Day Home  Away  HPUmpire        
##   <dbl> <dbl> <dbl> <chr> <chr> <chr>           
## 1  2012     4    12 MIN   LAA   D.J. Reyburn    
## 2  2012     4    12 SD    ARI   Marty Foster    
## 3  2012     4    12 WSH   CIN   Mike Everitt    
## 4  2012     4    12 PHI   MIA   Jeff Nelson     
## 5  2012     4    12 CHC   MIL   Fieldin Culbreth
## # ... with 2,354 more rows
\end{verbatim}

\hypertarget{non-standard-data}{%
\subsubsection{Non-Standard Data}\label{non-standard-data}}

To read in tricky, non-standard data there are a few functions that can help.

\begin{itemize}
\item
  \texttt{read\_file} - reads an entire file into a single string
\item
  \texttt{read\_lines} - reads a file into a character vector with one element per line
\end{itemize}

These are often parsed with \texttt{regular\ expressions}.

\hypertarget{excel-data}{%
\subsection{Excel data}\label{excel-data}}

Next we'll cover reading data from Excel files (\texttt{readxl} package), SAS datasets (\texttt{haven} package), and SPSS files (\texttt{haven} package).

\begin{longtable}[]{@{}lll@{}}
\toprule
Type of file & Package & Function\tabularnewline
\midrule
\endhead
Delimited & \texttt{readr} & \texttt{read\_csv()}, \texttt{read\_tsv()},\texttt{read\_table()}, \texttt{read\_delim()}\tabularnewline
Excel (.xls,.xlsx) & \texttt{readxl} & \texttt{read\_excel()}\tabularnewline
SAS (.sas7bdat) & \texttt{haven} & \texttt{read\_sas()}\tabularnewline
SPSS (.sav) & \texttt{haven} & \texttt{read\_spss()}\tabularnewline
\bottomrule
\end{longtable}

Let's read in the \href{https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx}{censusEd.xlsx} file.This can be done with the \texttt{read\_excel()} from \texttt{readxl} package! This funcion reads in both xls and xlsx files. It detects the format from the file extension given in the path name. One issue is that excel files cannot be read from the web so they do need to be downloaded locally.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install package if necessary}
\KeywordTok{library}\NormalTok{(readxl)}
\CommentTok{#reads first sheet by default}
\NormalTok{edData <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\StringTok{"../../datasets/censusEd.xlsx"}\NormalTok{)}
\NormalTok{edData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3,198 x 42
##   Area_name STCOU EDU010187F EDU010187D EDU010187N1 EDU010187N2 EDU010188F
##   <chr>     <chr>      <dbl>      <dbl> <chr>       <chr>            <dbl>
## 1 UNITED S~ 00000          0   40024299 0000        0000                 0
## 2 ALABAMA   01000          0     733735 0000        0000                 0
## 3 Autauga,~ 01001          0       6829 0000        0000                 0
## 4 Baldwin,~ 01003          0      16417 0000        0000                 0
## 5 Barbour,~ 01005          0       5071 0000        0000                 0
## # ... with 3,193 more rows, and 35 more variables: EDU010188D <dbl>,
## #   EDU010188N1 <chr>, EDU010188N2 <chr>, EDU010189F <dbl>, EDU010189D <dbl>,
## #   EDU010189N1 <chr>, EDU010189N2 <chr>, EDU010190F <dbl>, EDU010190D <dbl>,
## #   EDU010190N1 <chr>, EDU010190N2 <chr>, EDU010191F <dbl>, EDU010191D <dbl>,
## #   EDU010191N1 <chr>, EDU010191N2 <chr>, EDU010192F <dbl>, EDU010192D <dbl>,
## #   EDU010192N1 <chr>, EDU010192N2 <chr>, EDU010193F <dbl>, EDU010193D <dbl>,
## #   EDU010193N1 <chr>, EDU010193N2 <chr>, EDU010194F <dbl>, EDU010194D <dbl>,
## #   EDU010194N1 <chr>, EDU010194N2 <chr>, EDU010195F <dbl>, EDU010195D <dbl>,
## #   EDU010195N1 <chr>, EDU010195N2 <chr>, EDU010196F <dbl>, EDU010196D <dbl>,
## #   EDU010196N1 <chr>, EDU010196N2 <chr>
\end{verbatim}

If you want to read in a sheet other than the first sheet, you can do so with the \texttt{sheet\ =} argument. To look at the available sheets without opening in Excel you can use the \texttt{excel\_sheets} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{excel_sheets}\NormalTok{(}\StringTok{"../../datasets/censusEd.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "EDU01A" "EDU01B" "EDU01C" "EDU01D" "EDU01E" "EDU01F" "EDU01G" "EDU01H"
##  [9] "EDU01I" "EDU01J"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_excel}\NormalTok{(}\StringTok{"../../datasets/censusEd.xlsx"}\NormalTok{, }\DataTypeTok{sheet =} \StringTok{"EDU01D"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are also ways to specify which cells to read in with the \texttt{range\ =} argument. You can select cells that are contiguous only (next to each other).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edData <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\StringTok{"../../datasets/censusEd.xlsx"}\NormalTok{, }\DataTypeTok{sheet =} \StringTok{"EDU01A"}\NormalTok{, }
                   \DataTypeTok{range =} \KeywordTok{cell_cols}\NormalTok{(}\StringTok{"A:D"}\NormalTok{))}
\NormalTok{edData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3,198 x 4
##   Area_name     STCOU EDU010187F EDU010187D
##   <chr>         <chr>      <dbl>      <dbl>
## 1 UNITED STATES 00000          0   40024299
## 2 ALABAMA       01000          0     733735
## 3 Autauga, AL   01001          0       6829
## 4 Baldwin, AL   01003          0      16417
## 5 Barbour, AL   01005          0       5071
## # ... with 3,193 more rows
\end{verbatim}

\hypertarget{sas-data}{%
\subsection{SAS Data}\label{sas-data}}

SAS datasets have a file extension of `.sas7bdat'. Let's read in the \href{https://www4.stat.ncsu.edu/~online/datasets/smoke2003.sas7bdat}{smoke2003.sas7bdat} dataset. This can be done using the \texttt{read\_sas} function from the \texttt{haven} package. As .sas7bdat files are pretty structured there aren't many options to use with this function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install if necessary}
\KeywordTok{library}\NormalTok{(haven)}
\NormalTok{smokeData <-}\StringTok{ }\KeywordTok{read_sas}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/smoke2003.sas7bdat"}\NormalTok{)}
\NormalTok{smokeData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 443 x 54
##    SEQN SDDSRVYR RIDSTATR RIDEXMON RIAGENDR RIDAGEYR RIDAGEMN RIDAGEEX RIDRETH1
##   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
## 1 21010        3        2        2        2       52      633      634        3
## 2 21012        3        2        2        1       63      765      766        4
## 3 21048        3        2        1        2       42      504      504        1
## 4 21084        3        2        1        2       57      692      693        3
## 5 21093        3        2        1        2       64      778      778        2
## # ... with 438 more rows, and 45 more variables: RIDRETH2 <dbl>,
## #   DMQMILIT <dbl>, DMDBORN <dbl>, DMDCITZN <dbl>, DMDYRSUS <dbl>,
## #   DMDEDUC3 <dbl>, DMDEDUC2 <dbl>, DMDEDUC <dbl>, DMDSCHOL <dbl>,
## #   DMDMARTL <dbl>, DMDHHSIZ <dbl>, INDHHINC <dbl>, INDFMINC <dbl>,
## #   INDFMPIR <dbl>, RIDEXPRG <dbl>, DMDHRGND <dbl>, DMDHRAGE <dbl>,
## #   DMDHRBRN <dbl>, DMDHREDU <dbl>, DMDHRMAR <dbl>, DMDHSEDU <dbl>,
## #   SIALANG <dbl>, SIAPROXY <dbl>, SIAINTRP <dbl>, FIALANG <dbl>,
## #   FIAPROXY <dbl>, FIAINTRP <dbl>, MIALANG <dbl>, MIAPROXY <dbl>,
## #   MIAINTRP <dbl>, AIALANG <dbl>, WTINT2YR <dbl>, WTMEC2YR <dbl>,
## #   SDMVPSU <dbl>, SDMVSTRA <dbl>, Gender <dbl>, Age <dbl>, IncomeGroup <chr>,
## #   Ethnicity <chr>, Education <dbl>, SMD070 <dbl>, SMQ077 <dbl>, SMD650 <dbl>,
## #   PacksPerDay <dbl>, lbdvid <dbl>
\end{verbatim}

Often times SAS datasets have labels associated with the variable names. These are more descriptive titles that will print in SAS if requested. This is the case here. However, as you see above the labels did not print out. The labels will show if you look at the data set using the \texttt{View} function (or click on smokeData object from environment tab). How do we get to those labels?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(smokeData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    443 obs. of  54 variables:
##  $ SEQN       : num  21010 21012 21048 21084 21093 ...
##   ..- attr(*, "label")= chr "Patient ID"
##  $ SDDSRVYR   : num  3 3 3 3 3 3 3 3 3 3 ...
##   ..- attr(*, "label")= chr "Data Release Number"
##  $ RIDSTATR   : num  2 2 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Interview/Examination Status"
##  $ RIDEXMON   : num  2 2 1 1 1 2 1 2 1 1 ...
##   ..- attr(*, "label")= chr "Six month time period"
##  $ RIAGENDR   : num  2 1 2 2 2 2 1 2 1 2 ...
##   ..- attr(*, "label")= chr "Gender 1=M 2=F"
##  $ RIDAGEYR   : num  52 63 42 57 64 63 66 60 65 47 ...
##   ..- attr(*, "label")= chr "Age in Years at Exam"
##  $ RIDAGEMN   : num  633 765 504 692 778 763 801 731 786 573 ...
##   ..- attr(*, "label")= chr "Age in Months - Recode"
##  $ RIDAGEEX   : num  634 766 504 693 778 763 801 732 787 573 ...
##   ..- attr(*, "label")= chr "Exam Age in Months - Recode"
##  $ RIDRETH1   : num  3 4 1 3 2 3 1 3 3 3 ...
##   ..- attr(*, "label")= chr " Ethnicity 1=MexAm 2=OthHisp 3=OthCauc 4=OthBla 5=Oth"
##  $ RIDRETH2   : num  1 2 3 1 5 1 3 1 1 1 ...
##   ..- attr(*, "label")= chr "Linked NH3 Race/Ethnicity - Recode"
##  $ DMQMILIT   : num  2 2 2 2 2 2 2 2 1 2 ...
##   ..- attr(*, "label")= chr "Veteran/Military Status"
##  $ DMDBORN    : num  1 1 1 1 3 1 1 1 1 1 ...
##   ..- attr(*, "label")= chr "Country of Birth - Recode"
##  $ DMDCITZN   : num  1 1 1 1 1 1 1 1 1 1 ...
##   ..- attr(*, "label")= chr "Citizenship Status"
##  $ DMDYRSUS   : num  NA NA NA NA 9 NA NA NA NA NA ...
##   ..- attr(*, "label")= chr "Length of time in US"
##  $ DMDEDUC3   : num  NA NA NA NA NA NA NA NA NA NA ...
##   ..- attr(*, "label")= chr "Education Level - Children/Youth 6-19"
##  $ DMDEDUC2   : num  4 3 3 4 1 3 1 4 4 4 ...
##   ..- attr(*, "label")= chr "Education Level for Over 20"
##  $ DMDEDUC    : num  3 2 2 3 1 2 1 3 3 3 ...
##   ..- attr(*, "label")= chr "Education - Recode (old version)"
##  $ DMDSCHOL   : num  NA NA NA NA NA NA NA NA NA NA ...
##   ..- attr(*, "label")= chr "Now attending school?"
##  $ DMDMARTL   : num  6 6 3 1 2 1 6 3 1 1 ...
##   ..- attr(*, "label")= chr "Marital Status"
##  $ DMDHHSIZ   : num  3 2 5 2 2 2 2 3 2 6 ...
##   ..- attr(*, "label")= chr "Total number of people in the Household"
##  $ INDHHINC   : num  6 2 5 9 2 5 3 6 8 5 ...
##   ..- attr(*, "label")= chr "Annual Household Income"
##  $ INDFMINC   : num  4 2 2 9 2 5 3 6 8 4 ...
##   ..- attr(*, "label")= chr "Family Income"
##  $ INDFMPIR   : num  1.24 0.89 0.48 4.62 0.61 1.92 1.39 2.21 3.71 1.23 ...
##   ..- attr(*, "label")= chr "Family PIR"
##  $ RIDEXPRG   : num  2 NA 2 2 NA NA NA NA NA 2 ...
##   ..- attr(*, "label")= chr "Pregnancy Status at Exam - Recode"
##  $ DMDHRGND   : num  1 1 2 2 2 1 1 2 1 1 ...
##   ..- attr(*, "label")= chr "HH Ref Person Gender"
##  $ DMDHRAGE   : num  54 63 59 57 64 66 66 84 65 50 ...
##   ..- attr(*, "label")= chr "HH Ref Person Age"
##  $ DMDHRBRN   : num  1 1 1 1 3 1 1 1 1 NA ...
##   ..- attr(*, "label")= chr "HH Ref Person Country of Birth"
##  $ DMDHREDU   : num  1 3 4 4 1 3 1 5 4 NA ...
##   ..- attr(*, "label")= chr "HH Ref Person Education Level"
##  $ DMDHRMAR   : num  6 6 3 1 2 1 6 2 1 1 ...
##   ..- attr(*, "label")= chr "HH Ref Person Marital Status"
##  $ DMDHSEDU   : num  NA NA NA 3 NA 3 NA NA 2 4 ...
##   ..- attr(*, "label")= chr "HH Ref Person's Spouse Education Level"
##  $ SIALANG    : num  1 1 1 1 1 1 1 1 1 1 ...
##   ..- attr(*, "label")= chr "Language of SP Interview"
##  $ SIAPROXY   : num  2 2 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Proxy used in SP Interview?"
##  $ SIAINTRP   : num  2 2 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Interpreter used in SP Interview?"
##  $ FIALANG    : num  1 1 1 1 1 1 1 1 1 1 ...
##   ..- attr(*, "label")= chr "Language of Family Interview"
##  $ FIAPROXY   : num  2 2 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Proxy used in Family Interview?"
##  $ FIAINTRP   : num  2 2 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Interpreter used in Family Interview?"
##  $ MIALANG    : num  1 NA 1 1 1 1 1 1 1 1 ...
##   ..- attr(*, "label")= chr "Language of MEC Interview"
##  $ MIAPROXY   : num  2 NA 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Proxy used in MEC Interview?"
##  $ MIAINTRP   : num  2 NA 2 2 2 2 2 2 2 2 ...
##   ..- attr(*, "label")= chr "Interpreter used in MEC Interview?"
##  $ AIALANG    : num  1 NA 1 1 NA NA NA NA NA 1 ...
##   ..- attr(*, "label")= chr "Language of ACASI Interview"
##  $ WTINT2YR   : num  39599 12629 18792 91437 24475 ...
##   ..- attr(*, "label")= chr "Full Sample 2 Year Interview Weight"
##  $ WTMEC2YR   : num  43287 12947 19035 93163 27829 ...
##   ..- attr(*, "label")= chr "Full Sample 2 Year MEC Exam Weight"
##  $ SDMVPSU    : num  1 2 2 1 1 2 1 2 1 2 ...
##   ..- attr(*, "label")= chr "Masked Variance Pseudo-PSU"
##  $ SDMVSTRA   : num  29 33 39 34 35 30 34 30 38 34 ...
##   ..- attr(*, "label")= chr "Masked Variance Pseudo-Stratum"
##  $ Gender     : num  2 1 2 2 2 2 1 2 1 2 ...
##  $ Age        : num  52 63 42 57 64 63 66 60 65 47 ...
##  $ IncomeGroup: chr  "Less Than 20K" "Less Than 20K" "Less Than 20K" "More Than 20K" ...
##  $ Ethnicity  : chr  "Non-Hispanic Caucasian" "Non-Hispanic Black" "MexicanAmerican & Hispanic" "Non-Hispanic Caucasian" ...
##  $ Education  : num  4 3 3 4 1 3 1 4 4 4 ...
##  $ SMD070     : num  20 20 20 20 20 16 20 20 10 6 ...
##   ..- attr(*, "label")= chr "Number of Cagarettes Smoked/day now"
##  $ SMQ077     : num  2 2 1 3 2 2 2 1 3 3 ...
##   ..- attr(*, "label")= chr "How soon after waking do you smoke?"
##  $ SMD650     : num  20 20 20 20 20 10 20 20 10 1 ...
##   ..- attr(*, "label")= chr "Number of Cigarettes/day for last 30 days"
##  $ PacksPerDay: num  1 1 1 1 1 0.5 1 1 0.5 0.05 ...
##  $ lbdvid     : num  16 18 16 17 18 25 9 27 9 25 ...
##  - attr(*, "label")= chr "DATA2003"
\end{verbatim}

The labels are an \textbf{attribute} of the dataset. The attribute is called ``label''. These can be accessed using the \texttt{attr} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attr}\NormalTok{(smokeData}\OperatorTok{$}\NormalTok{SDDSRVYR, }\StringTok{"label"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Data Release Number"
\end{verbatim}

\hypertarget{spss-data}{%
\subsection{SPSS Data}\label{spss-data}}

SPSS datasets have a file extension of ``.sav''. Let's read in the \href{https://www4.stat.ncsu.edu/~online/datasets/bodyFat.sav}{bodyFat.sav} dataset. This can be done using the \texttt{read\_spss} function from the \texttt{haven} package. As with SAS datasets, these are well structured so there aren't many options to use with the function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bodyFatData <-}\StringTok{ }\KeywordTok{read_spss}\NormalTok{(}\StringTok{"https://www4.stat.ncsu.edu/~online/datasets/bodyFat.sav"}\NormalTok{)}
\NormalTok{bodyFatData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 20 x 4
##        y    x1    x2    x3
##    <dbl> <dbl> <dbl> <dbl>
##  1  19.5  43.1  29.1  11.9
##  2  24.7  49.8  28.2  22.8
##  3  30.7  51.9  37    18.7
##  4  29.8  54.3  31.1  20.1
##  5  19.1  42.2  30.9  12.9
##  6  25.6  53.9  23.7  21.7
##  7  31.4  58.5  27.6  27.1
##  8  27.9  52.1  30.6  25.4
##  9  22.1  49.9  23.2  21.3
## 10  25.5  53.5  24.8  19.3
## 11  31.1  56.6  30    25.4
## 12  30.4  56.7  28.3  27.2
## 13  18.7  46.5  23    11.7
## 14  19.7  44.2  28.6  17.8
## 15  14.6  42.7  21.3  12.8
## 16  29.5  54.4  30.1  23.9
## 17  27.7  55.3  25.7  22.6
## 18  30.2  58.6  24.6  25.4
## 19  22.7  48.2  27.1  14.8
## 20  25.2  51    27.5  21.1
\end{verbatim}

\hypertarget{json}{%
\subsection{JSON}\label{json}}

\textbf{JSON} stands for JavaScript Object Notation. This data format is used widely across the internet and in databases. JSON data can represent our usual 2D data or heirarchical data. JSON uses key-value pairs. An example of raw JSON data is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\{  }
\NormalTok{  \{  }
    \StringTok{"name"}\OperatorTok{:}\StringTok{ "Barry Sanders"}  
    \StringTok{"games"} \OperatorTok{:}\StringTok{ }\DecValTok{153}  
    \StringTok{"position"}\OperatorTok{:}\StringTok{ "RB"}  
\NormalTok{  \},  }
\NormalTok{  \{  }
    \StringTok{"name"}\OperatorTok{:}\StringTok{ "Joe Montana"}  
    \StringTok{"games"}\OperatorTok{:}\StringTok{ }\DecValTok{192}  
    \StringTok{"position"}\OperatorTok{:}\StringTok{ "QB"}  
\NormalTok{  \}  }
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

There are three major R packages for reading in JSON data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{rjson}
\item
  \texttt{RJSONIO}
\item
  \texttt{jsonlite}
\end{enumerate}

We prefer \texttt{jsonlite}. It has many nice features to simplify reading in data, but these features do make the package's functions a little slower. The most useful functions from \href{https://www.rdocumentation.org/packages/jsonlite/versions/1.6}{\texttt{jsonlite}} are summarized below:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright
Function\strut
\end{minipage} & \begin{minipage}[b]{0.77\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright
\texttt{fromJSON}\strut
\end{minipage} & \begin{minipage}[t]{0.77\columnwidth}\raggedright
Reads JSON data from file path or character string. Converts and simplfies to R object\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright
\texttt{toJSON}\strut
\end{minipage} & \begin{minipage}[t]{0.77\columnwidth}\raggedright
Writes R object to JSON object\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright
\texttt{stream\_in}\strut
\end{minipage} & \begin{minipage}[t]{0.77\columnwidth}\raggedright
Accepts a \emph{file connection} - can read streaming JSON data\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{xml}{%
\subsection{XML}\label{xml}}

XML stands for eXtensible Markup Language. This is another data format that is used widely across the internet and in databases. This type of data can again represent our usual 2D data or heirarchical data. XML uss tags \textless{} \textgreater{} similar to HTML. An example of raw XML data is given below.

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{<}\NormalTok{roster}\OperatorTok{>}
\StringTok{  }\ErrorTok{<}\NormalTok{player}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{name}\OperatorTok{>}\NormalTok{Barry Sanders}\OperatorTok{<}\ErrorTok{/}\NormalTok{name}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{games}\OperatorTok{>}\DecValTok{153}\OperatorTok{<}\ErrorTok{/}\NormalTok{games}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{position}\OperatorTok{>}\NormalTok{RB}\OperatorTok{<}\ErrorTok{/}\NormalTok{position}\OperatorTok{>}
\StringTok{  }\ErrorTok{</}\NormalTok{player}\OperatorTok{>}
\StringTok{  }\ErrorTok{<}\NormalTok{player}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{name}\OperatorTok{>}\NormalTok{Joe Montana}\OperatorTok{<}\ErrorTok{/}\NormalTok{name}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{games}\OperatorTok{>}\DecValTok{192}\OperatorTok{<}\ErrorTok{/}\NormalTok{games}\OperatorTok{>}
\StringTok{    }\ErrorTok{<}\NormalTok{position}\OperatorTok{>}\NormalTok{QB}\OperatorTok{<}\ErrorTok{/}\NormalTok{position}\OperatorTok{>}
\StringTok{  }\ErrorTok{</}\NormalTok{player}\OperatorTok{>}
\ErrorTok{</}\NormalTok{roster}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

The structure of the nodes has parent nodes, child nodes, etc. A basic diagram is given below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/xmlDiagram} 

}

\caption{Source: mysamplecode.com}\label{fig:unnamed-chunk-130}
\end{figure}

There are two major R packages for reading in XML data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{XML}
\item
  \texttt{xml2}
\end{enumerate}

\texttt{xml2} has all the basic functionality to get data into R. Reading XML data is generally tough since the structure of tags varies by data source! The \href{https://cran.r-project.org/web/packages/xml2/index.html}{\texttt{xml2}} core functions are:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright
Function\strut
\end{minipage} & \begin{minipage}[b]{0.72\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.22\columnwidth}\raggedright
\texttt{read\_xml}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Accepts string, file path, or url argument. Returns XML data object\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
\texttt{xml\_children}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Returns list of elements downstream from current node\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
\texttt{xml\_parents}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Returns list of all parent elements from current node\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
\texttt{xml\_contents}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Returns list of contents from current node\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
\texttt{as\_list}\strut
\end{minipage} & \begin{minipage}[t]{0.72\columnwidth}\raggedright
Converts XML document or node set to equivalent R list\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{databases}{%
\subsection{Databases}\label{databases}}

A database is a collection of data, usually a bunch of 2D tables that have keys that connect them. The Database Management System (DBMS) controls how users interact with the database. There is a common and very useful Structured Query Language (SQL - pronounced ess-que-el or sequel) used by relational database management systems (RDBMS) for retrieving and combining datasets from a database.

An example of a relational database structure is given below. Notice there are keys that link different tables.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/lahman} 

}

\caption{Source: oreilly.com}\label{fig:unnamed-chunk-131}
\end{figure}

There are many popular RDBMS. Some are free and some are proprietary. These are often simply referred to as databases.

\begin{itemize}
\item
  Oracle - most popular (cross platform)
\item
  SQL Server - Microsoft product
\item
  DB2 - IBM product
\item
  MySQL (open source) - Not as many features but popular
\item
  PostgreSQL (open source)
\end{itemize}

Again there is a \href{http://www.sqltutorial.org/sql-cheat-sheet/}{Basic SQL language} that is constant across all these database types.

The common flow to connect to a database using R is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Connect to the database with \texttt{DBI::dbConnect()}\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Need appropriate R package for database backend

  \begin{itemize}
  \tightlist
  \item
    \texttt{RSQLite::SQLite()} for RSQLite\\
  \item
    \texttt{RMySQL::MySQL()} for RMySQL\\
  \item
    \texttt{RPostgreSQL::PostgreSQL()} for RPostgreSQL\\
  \item
    \texttt{odbc::odbc()} for Open Database Connectivity\\
  \item
    \texttt{bigrquery::bigquery()} for google's bigQuery
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{con <-}\StringTok{ }\NormalTok{DBI}\OperatorTok{::}\KeywordTok{dbConnect}\NormalTok{(RMySQL}\OperatorTok{::}\KeywordTok{MySQL}\NormalTok{(), }
  \DataTypeTok{host =} \StringTok{"hostname.website"}\NormalTok{,}
  \DataTypeTok{user =} \StringTok{"username"}\NormalTok{,}
  \DataTypeTok{password =}\NormalTok{ rstudioapi}\OperatorTok{::}\KeywordTok{askForPassword}\NormalTok{(}\StringTok{"DB password"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Use \texttt{tbl()} to reference a table in the database
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tbl}\NormalTok{(con, }\StringTok{"name_of_table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Query the database with \texttt{SQL} or \texttt{dplyr/dbplyr}
\end{enumerate}

There is much more about \href{https://db.rstudio.com/}{R Studio and Databases here}.

\hypertarget{apis}{%
\subsection{APIs}\label{apis}}

API stands for Application Programming Interfaces. This is essentially a defined method for asking for information from a computer. They are useful for getting data or allowing others to run a model you've built.

There are many open APIs. They usually just require you to register and obtain a key. Once you have a key you simply need to construct the proper URL to return the information you want from the API.

As a quick example we will query the Harry Potter database at \url{https://www.potterapi.com/}. There is a button on the top right where you can register and obtain a key.

\begin{center}\includegraphics[width=0.8\linewidth]{img/HPAPI} \end{center}

The documentation for returning Harry Potter spells states:

\begin{verbatim}
+ All routes need to be prefixed with https://www.potterapi.com/v1/  

+ GET request: /spells returns all spells 

+ Key goes on the end  
\end{verbatim}

This tells us how to construct the appropriate URL. The \texttt{paste} and \texttt{paste0} functions are useful for combining strings (check their help).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baseURL <-}\StringTok{ "https://www.potterapi.com/v1/"}
\NormalTok{value <-}\StringTok{ "spells?"}
\NormalTok{key <-}\StringTok{ "key=$2a$10$UMvDCH.93fa2KOjKbJYkOOPMNzdzQpJ0gMnVEtcHzW5Ic04HUmcsa"}
\NormalTok{URL <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(baseURL, value, key)}
\NormalTok{URL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "https://www.potterapi.com/v1/spells?key=$2a$10$UMvDCH.93fa2KOjKbJYkOOPMNzdzQpJ0gMnVEtcHzW5Ic04HUmcsa"
\end{verbatim}

Now we use the \texttt{RCurl} package and the \texttt{getURL} function to ping the URL we just created. This will return the spell data set in JSON form as that is the default response format for this API.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spellData <-}\StringTok{ }\NormalTok{RCurl}\OperatorTok{::}\KeywordTok{getURL}\NormalTok{(URL)}
\end{Highlighting}
\end{Shaded}

This is a reasonably large string of information so we can just look at the first 100 characters using the \texttt{substr} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{substr}\NormalTok{(spellData, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "[{\"_id\":\"5b74ebd5fb6fc0739646754c\",\"spell\":\"Aberto\",\"type\":\"Charm\",\"effect\":\"opens objects\"},{\"_id\":"
\end{verbatim}

To convert this to a data frame we can use the \texttt{fromJSON} function in the \texttt{jsonlite} package. \texttt{tbl\_df} converts the dataframe to a \texttt{tibble} (for printing purposes).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spellDataDF <-}\StringTok{ }\NormalTok{jsonlite}\OperatorTok{::}\KeywordTok{fromJSON}\NormalTok{(spellData)}
\KeywordTok{tbl_df}\NormalTok{(spellDataDF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 151 x 5
##   `_id`                spell          type      effect                     `__v`
##   <chr>                <chr>          <chr>     <chr>                      <int>
## 1 5b74ebd5fb6fc073964~ Aberto         Charm     opens objects                 NA
## 2 5b74ecfa3228320021a~ Accio          Charm     Summons an object              0
## 3 5b74ed2f3228320021a~ Age Line       Enchantm~ Hides things from younger~     0
## 4 5b74ed453228320021a~ Aguamenti      Charm     shoots water from wand         0
## 5 5b74ed583228320021a~ Alarte Ascend~ Spell     shoots things high in the~     0
## # ... with 146 more rows
\end{verbatim}

Of course constructing URLs like this yourself isn't ideal. Languages like python have many packages to help you contact APIs without reading as much documentation. Unfortunately, R does not have a very mature collection of API packages. The article \href{https://www.programmableweb.com/news/how-to-access-any-restful-api-using-r-language/how-to/2017/07/21}{here} discusses accessing APIs generically with R. The same website gives a \href{https://www.programmableweb.com/category/all/apis}{list of APIs} that you might consider.

\hypertarget{data-manipulation-ideas}{%
\subsection{Data Manipulation Ideas}\label{data-manipulation-ideas}}

As you can see it isn't too difficult to bring well structured raw data into R. You should now have the basics of reading in delimited, Excel, SAS, SPSS, JSON, and XML data as well as how to connect to a database and contact an API. Once you have your data you may want to manipulate it in some way.

Often we want to grab only certain types of observations (filter rows).

\begin{center}\includegraphics[width=0.8\linewidth]{img/filterVisualF} \end{center}

We also want to only look at only certain variables (select columns).

\begin{center}\includegraphics[width=0.7\linewidth]{img/selectVisualF} \end{center}

Other times we want to create new variables that may be functions of the data in the data set.

\begin{center}\includegraphics[width=0.8\linewidth]{img/createVarVisualF} \end{center}

When doing data manipulation it is vital to make your work reproducible! Traditionally documentation has been done through comments (\texttt{\#} in R) in your R script. This is being replaced by using a `Notebook' environment like R Markdown.

\hypertarget{documenting-with-markdown}{%
\subsection{Documenting with Markdown}\label{documenting-with-markdown}}

You may have heard of \href{http://jupyter.org/}{JUPYTER} notebooks. This is a program that allows you to weave plain text with formatting characters along side code. JUPYTER allows you to call Julia, Python, R, or SAS code (among others).

R Markdown is a built in notebook for R studio! A nice intro video is \href{https://vimeo.com/178485416}{available here}.

R Markdown is designed to be used in three ways (R for Data Science):

\begin{itemize}
\item
  Communicating to decision makers (focus on conclusions not code)
\item
  Collaborating with other data scientists (including future you!)
\item
  As environment to do data science (documents what you did and what you were thinking)
\end{itemize}

Most have heard of HTML or HyperText Mark-up Language. This is really just plain text that a web browser like firefox interprets and renders. Markdown is a specific markup language that has easier syntax but is not as powerful. Any plain text file can be used although the .Rmd extension associates the file with R Studio.

RStudio makes it easy to create a Markdown document.

\begin{center}\includegraphics[width=0.75\linewidth]{img/startMD} \end{center}

You can create many commonly used types of output including HTML, PDF, Word, and HTML slides.

\begin{center}\includegraphics[width=0.65\linewidth]{img/startMDDoc} \end{center}

An R Markdown file contains three important types of content:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  (Optional) YAML header surrounded by \texttt{-\/-\/-}s
\item
  Chunks of R code
\item
  Text mixed with simple text formatting instructions
\end{enumerate}

The YAML header defines settings for document:

\begin{verbatim}
---
title: "Untitled"
author: "First Last"
date: "xxxx"
output: html_document
---
\end{verbatim}

The hot key combination of CTRL/CMD + Shift + k `knits' (or creates the output type) via this information.

Code Chunks can contain any R code. These can be started by typing ```\{r\} out or with CTRL/CMD + Alt + I. This code will be executed when document is created and the chunks will be evaulated sequentially. Options can be specified on individual code chunks to hide their code or output (among other things).

Below you'll see plain text with markdown sytnax included:

\begin{verbatim}
##R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. 
\end{verbatim}

When the file is created \texttt{\#\#} becomes a header, ``\textless{}\ldots{}\textgreater{}'' a link, and \texttt{**Knit**} bold font.

You can learn much more about how to use R Markdown with this handy \href{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{cheat sheet}.

The key idea here is that you can easily write down your thought process and document all of the changes you make to your data. This creates a reproducible final product!

\hypertarget{logical-statements}{%
\subsection{Logical Statements}\label{logical-statements}}

Our current goal is to subset rows or columns of a dataset.

\begin{center}\includegraphics[width=0.8\linewidth]{img/filterVisualF} \end{center}

To do this efficiently we need to learn about logical statements. A logical statement is a comparison that resolves as \texttt{TRUE} or \texttt{FALSE}. R has all of the standard comparison operators:

\begin{itemize}
\tightlist
\item
  \texttt{==} equal to
\item
  \texttt{!=} not equal to
\item
  \texttt{\textless{}}, \texttt{\textless{}=}, \texttt{\textgreater{}}, \texttt{\textgreater{}=} less than (or equal) and greater than (or equal)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"hi"} \OperatorTok{==}\StringTok{ " hi"} \CommentTok{#== is comparison}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"hi"} \OperatorTok{==}\StringTok{ "hi"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4} \OperatorTok{>=}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4} \OperatorTok{!=}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Sometimes we see issues due to a loss of precision when doing mathematical operations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\DecValTok{3}\NormalTok{)}\OperatorTok{^}\DecValTok{2}  \OperatorTok{==}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

The \texttt{near} function from the \texttt{dplyr} package can help with this type of situation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{near}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{3}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Another common way to do a logical statement in R is to use an \texttt{is.} family function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.numeric}\NormalTok{(}\StringTok{"Word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.numeric}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.character}\NormalTok{(}\StringTok{"10"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.na}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE FALSE  TRUE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"hello"}\NormalTok{, }\StringTok{"world"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

How do we use logical statements to subset our data? Logical vectors can be used for indexing an R object. The concept is:

\begin{itemize}
\tightlist
\item
  Feed index a vector of \texttt{TRUE}/\texttt{FALSE} or \texttt{0}/\texttt{1} values\\
\item
  R will return elements where \texttt{TRUE} or \texttt{1} occurred
\end{itemize}

Let's subset the built-in \texttt{iris} data set. First we'll convert it to a \texttt{tibble} so it prints nicely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris <-}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{(iris)}
\NormalTok{iris}
\end{Highlighting}
\end{Shaded}

Now, we can create an indexing vector corresponding to some condition of interest. For instance, we may want to only look at the Species `setosa' flowers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "setosa"} \CommentTok{#vector indicating setosa values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [49]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [97] FALSE FALSE FALSE FALSE
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

Now we can feed this in as our row index to the \texttt{{[}} function. Remember for rectangular data the first index you give refers to the rows and the second to columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## 11          5.4         3.7          1.5         0.2  setosa
## 12          4.8         3.4          1.6         0.2  setosa
## 13          4.8         3.0          1.4         0.1  setosa
## 14          4.3         3.0          1.1         0.1  setosa
## 15          5.8         4.0          1.2         0.2  setosa
## 16          5.7         4.4          1.5         0.4  setosa
## 17          5.4         3.9          1.3         0.4  setosa
## 18          5.1         3.5          1.4         0.3  setosa
## 19          5.7         3.8          1.7         0.3  setosa
## 20          5.1         3.8          1.5         0.3  setosa
##  [ reached 'max' / getOption("max.print") -- omitted 30 rows ]
\end{verbatim}

Rather than use \texttt{{[}}, a base R function called \texttt{subset} can be used.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{subset}\NormalTok{(iris, Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## 11          5.4         3.7          1.5         0.2  setosa
## 12          4.8         3.4          1.6         0.2  setosa
## 13          4.8         3.0          1.4         0.1  setosa
## 14          4.3         3.0          1.1         0.1  setosa
## 15          5.8         4.0          1.2         0.2  setosa
## 16          5.7         4.4          1.5         0.4  setosa
## 17          5.4         3.9          1.3         0.4  setosa
## 18          5.1         3.5          1.4         0.3  setosa
## 19          5.7         3.8          1.7         0.3  setosa
## 20          5.1         3.8          1.5         0.3  setosa
##  [ reached 'max' / getOption("max.print") -- omitted 30 rows ]
\end{verbatim}

This function works quite well but we want to work in the tidyverse. The \texttt{filter} function from the \texttt{dplyr} package (installed with \texttt{tidyverse}) will be our function of choice. For \texttt{filter} the first argument is the data frame (or tibble) and the second is the logical statement used for indexing the rows.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(iris, Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## 11          5.4         3.7          1.5         0.2  setosa
## 12          4.8         3.4          1.6         0.2  setosa
## 13          4.8         3.0          1.4         0.1  setosa
## 14          4.3         3.0          1.1         0.1  setosa
## 15          5.8         4.0          1.2         0.2  setosa
## 16          5.7         4.4          1.5         0.4  setosa
## 17          5.4         3.9          1.3         0.4  setosa
## 18          5.1         3.5          1.4         0.3  setosa
## 19          5.7         3.8          1.7         0.3  setosa
## 20          5.1         3.8          1.5         0.3  setosa
##  [ reached 'max' / getOption("max.print") -- omitted 30 rows ]
\end{verbatim}

Often we'll want to subset based on more than one condition. These can be created using standard logical operators. In R these are:

\begin{itemize}
\tightlist
\item
  \texttt{\&} `and'
\item
  \texttt{\textbar{}} `or'
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\toprule
Operator & A,B true & A true, B false & A,B false\tabularnewline
\midrule
\endhead
\texttt{\&} & \texttt{A\ \&\ B\ =\ TRUE} & \texttt{A\ \&\ B\ =\ FALSE} & \texttt{A\ \&\ B\ =\ FALSE}\tabularnewline
\texttt{\textbar{}} & \texttt{A\ \textbar{}\ B\ =\ TRUE} & \texttt{A\ \textbar{}\ B\ =\ TRUE} & \texttt{A\ \textbar{}\ B\ =\ FALSE}\tabularnewline
\bottomrule
\end{longtable}

For the most part we'll want to use the single \texttt{\&} or \texttt{\textbar{}}. \texttt{\&\&} and \texttt{\textbar{}\textbar{}} are alternatives that only look at only first comparison done (if given a vector of comparisons).

A quick example of the compound logical syntax is given below. Parenthesis are not necessary but are quite useful to keep things straight! Here we generate 10 random values between 0 and 1 (\texttt{set.seed} just starts the random number generator at a specific spot so we can get the same 10 values each time we create this document!). We use \texttt{\textbar{}} to return TRUE if the randomly generated value is either below 0.25 or above 0.75.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10}\NormalTok{, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{1}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007 0.6043941 0.1246334
##  [8] 0.2946009 0.5776099 0.6309793
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x }\OperatorTok{<}\StringTok{ }\FloatTok{0.25}\NormalTok{) }\OperatorTok{|}\StringTok{ }\NormalTok{(x }\OperatorTok{>}\StringTok{ }\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
\end{verbatim}

With this kind of syntax we can now create an indexing vector to only pull out large petal setosa flowers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Length }\OperatorTok{>}\StringTok{ }\FloatTok{1.5}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{>}\StringTok{ }\FloatTok{0.3}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
##  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
##  [25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE
##  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [97] FALSE FALSE FALSE FALSE
##  [ reached getOption("max.print") -- omitted 50 entries ]
\end{verbatim}

Using this in the filter function we return only a few observations corresponding to our condition.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(iris, (Petal.Length }\OperatorTok{>}\StringTok{ }\FloatTok{1.5}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(Petal.Width }\OperatorTok{>}\StringTok{ }\FloatTok{0.3}\NormalTok{) }\OperatorTok{&}\StringTok{ }
\StringTok{         }\NormalTok{(Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.4         3.9          1.7         0.4  setosa
## 2          5.1         3.3          1.7         0.5  setosa
## 3          5.0         3.4          1.6         0.4  setosa
## 4          5.0         3.5          1.6         0.6  setosa
## 5          5.1         3.8          1.9         0.4  setosa
\end{verbatim}

\hypertarget{dplyr}{%
\subsection{\texorpdfstring{\texttt{dplyr}}{dplyr}}\label{dplyr}}

The tidyverse has many useful packages for common data manipulation tasks. Make sure \texttt{library(tidyverse)} has been run when working through this section!\\
Two major packages for data manipulation are:

-\texttt{dplry} package made for most standard data manipulation tasks

\begin{itemize}
\tightlist
\item
  \texttt{tidyr} package reshapes data (wide and long format, split columns, etc)
\end{itemize}

This section focuses on the most useful functions from the \texttt{dplyr} package:

\begin{itemize}
\tightlist
\item
  \texttt{tbl\_df()} - convert data frame to one with better printing\\
\item
  \texttt{filter()} - subset rows\\
\item
  \texttt{arrange()} - reorder rows\\
\item
  \texttt{select()} - subset columns\\
\item
  \texttt{rename()} - rename columns
\end{itemize}

Later we'll look at

\begin{itemize}
\tightlist
\item
  \texttt{mutate()} - add newly created column\\
\item
  \texttt{transmute()} - create new variable\\
\item
  \texttt{group\_by()} - group rows by a variable\\
\item
  \texttt{summarise()} - apply basic function to data
\end{itemize}

One really nice thing about the functions in the tidyverse is that the syntax is mostly consistent (save \texttt{ggplot2}). The basic syntax is

\texttt{function(tibble,\ actions,\ ...)}

Let's get started! We've seen \texttt{tbl\_df} a few times. This function converts a data frame to one with better printing and no simplification. To use it we can simply `wrap' data frame with it. In this section we'll do examples on datasets from the \texttt{Lahman} pacakge. This package has data about baseball players dating back from the start of professional baseball.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install.packages("Lahman")}
\KeywordTok{library}\NormalTok{(Lahman)}
\CommentTok{#old method for previewing a dataset}
\KeywordTok{head}\NormalTok{(Batting, }\DataTypeTok{n =} \DecValTok{4}\NormalTok{) }\CommentTok{#look at just first 4 observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    playerID yearID stint teamID lgID  G  AB  R  H X2B X3B HR RBI SB CS BB SO
## 1 abercda01   1871     1    TRO   NA  1   4  0  0   0   0  0   0  0  0  0  0
## 2  addybo01   1871     1    RC1   NA 25 118 30 32   6   0  0  13  8  1  4  0
## 3 allisar01   1871     1    CL1   NA 29 137 28 40   4   5  0  19  3  1  2  5
## 4 allisdo01   1871     1    WS3   NA 27 133 28 44  10   2  2  27  1  1  0  2
##   IBB HBP SH SF GIDP
## 1  NA  NA NA NA    0
## 2  NA  NA NA NA    0
## 3  NA  NA NA NA    1
## 4  NA  NA NA NA    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Batting <-}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{(Batting)}
\NormalTok{Batting}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 abercda~   1871     1 TRO    NA        1     4     0     0     0     0     0
## 2 addybo01   1871     1 RC1    NA       25   118    30    32     6     0     0
## 3 allisar~   1871     1 CL1    NA       29   137    28    40     4     5     0
## 4 allisdo~   1871     1 WS3    NA       27   133    28    44    10     2     2
## 5 ansonca~   1871     1 RC1    NA       25   120    29    39    11     3     0
## # ... with 1.059e+05 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

If the data has been read in with \texttt{haven}, \texttt{readxl}, or \texttt{readr}, it is probably in this format already!

\hypertarget{row-manipulations}{%
\subsubsection{Row Manipulations}\label{row-manipulations}}

Again, we may to do a subset based on the rows of our dataset.

\begin{center}\includegraphics[width=0.8\linewidth]{img/filterVisualF} \end{center}

We just looked at using the \texttt{filter} function to subset rows or observations of a dataset. Let's look at a few more examples. We may only want to return observations from the Batting dataset corresponding to the Pittsburgh Pirates (PIT).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(Batting, teamID }\OperatorTok{==}\StringTok{ "PIT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4,817 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 barklsa~   1887     1 PIT    NL       89   340    44    76    10     4     1
## 2 beeched~   1887     1 PIT    NL       41   169    15    41     8     0     2
## 3 bishobi~   1887     1 PIT    NL        3     9     0     0     0     0     0
## 4 brownto~   1887     1 PIT    NL       47   192    30    47     3     4     0
## 5 carrofr~   1887     1 PIT    NL      102   421    71   138    24    15     6
## # ... with 4,812 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

We could use a compound logical to only return Pirate data from the year 2000.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(Batting, teamID }\OperatorTok{==}\StringTok{ "PIT"} \OperatorTok{&}\StringTok{ }\NormalTok{yearID }\OperatorTok{==}\StringTok{ }\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 46 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 anderji~   2000     1 PIT    NL       27    50     5     7     1     0     0
## 2 arroybr~   2000     1 PIT    NL       21    21     2     3     2     0     0
## 3 avenbr01   2000     1 PIT    NL       72   148    18    37    11     0     5
## 4 benjami~   2000     1 PIT    NL       93   233    28    63    18     2     2
## 5 bensokr~   2000     1 PIT    NL       32    65     3     6     2     0     0
## # ... with 41 more rows, and 10 more variables: RBI <int>, SB <int>, CS <int>,
## #   BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>, GIDP <int>
\end{verbatim}

Another useful row operation is to rearrange the data based on some criteria. The \texttt{arrange} function allows us to sort a data set by numeric or character variables. For instance we could reorder alphabetically by the teamID variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(Batting, teamID)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 berrych~   1884     1 ALT    UA        7    25     2     6     0     0     0
## 2 brownji~   1884     1 ALT    UA       21    88    12    22     2     2     1
## 3 carropa~   1884     1 ALT    UA       11    49     4    13     1     0     0
## 4 connojo~   1884     1 ALT    UA        3    11     0     1     0     0     0
## 5 crosscl~   1884     1 ALT    UA        2     7     1     4     1     0     0
## # ... with 1.059e+05 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

A secondary arrangement can be done as well (and third, etc.)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(Batting, teamID, G)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 daisege~   1884     1 ALT    UA        1     4     0     0     0     0     0
## 2 crosscl~   1884     1 ALT    UA        2     7     1     4     1     0     0
## 3 manloch~   1884     1 ALT    UA        2     7     1     3     0     0     0
## 4 connojo~   1884     1 ALT    UA        3    11     0     1     0     0     0
## 5 shafff01   1884     1 ALT    UA        6    19     1     3     0     0     0
## # ... with 1.059e+05 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

The arrangement can be done descending as well by giving the column (variable) with \texttt{desc}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{arrange}\NormalTok{(Batting, teamID, }\KeywordTok{desc}\NormalTok{(G))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 22
##   playerID yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR
##   <chr>     <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int> <int>
## 1 smithge~   1884     1 ALT    UA       25   108     9    34     8     1     0
## 2 harrifr~   1884     1 ALT    UA       24    95    10    25     2     1     0
## 3 doughch~   1884     1 ALT    UA       23    85     6    22     5     0     0
## 4 murphjo~   1884     1 ALT    UA       23    94    10    14     1     0     0
## 5 brownji~   1884     1 ALT    UA       21    88    12    22     2     2     1
## # ... with 1.059e+05 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

\hypertarget{column-manipulations}{%
\subsubsection{Column Manipulations}\label{column-manipulations}}

We may want to look at only certain variables (select columns).

\begin{center}\includegraphics[width=0.65\linewidth]{img/selectVisualF} \end{center}

The \texttt{select} function from the \texttt{dplyr} package allows us to choose columns of interest. We've seen the use of \texttt{\$} and \texttt{{[}\ ,\ {]}} to do this already but \texttt{select} offers many advantages.

\begin{itemize}
\item
  Same syntax as tidyverse functions
\item
  Useful ways to use character matching to select columns
\end{itemize}

Let's see it in action! To choose a single column of interest just supply the column name (or position) after the \texttt{tibble}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(Batting, X2B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 1
##     X2B
##   <int>
## 1     0
## 2     6
## 3     4
## 4    10
## 5    11
## # ... with 1.059e+05 more rows
\end{verbatim}

Multiple columns can be selected by giving multiple column names.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(Batting, playerID, X2B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 2
##   playerID    X2B
##   <chr>     <int>
## 1 abercda01     0
## 2 addybo01      6
## 3 allisar01     4
## 4 allisdo01    10
## 5 ansonca01    11
## # ... with 1.059e+05 more rows
\end{verbatim}

There are many ways to select multiple columsn (variables). For instance, contiguous columns can be selected using the \texttt{:}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#all columns between}
\KeywordTok{select}\NormalTok{(Batting, X2B}\OperatorTok{:}\NormalTok{HR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 3
##     X2B   X3B    HR
##   <int> <int> <int>
## 1     0     0     0
## 2     6     0     0
## 3     4     5     0
## 4    10     2     2
## 5    11     3     0
## # ... with 1.059e+05 more rows
\end{verbatim}

Character matching can be done to select all columns that contain a certain character.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#all columns containing}
\KeywordTok{select}\NormalTok{(Batting, }\KeywordTok{contains}\NormalTok{(}\StringTok{"X"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 2
##     X2B   X3B
##   <int> <int>
## 1     0     0
## 2     6     0
## 3     4     5
## 4    10     2
## 5    11     3
## # ... with 1.059e+05 more rows
\end{verbatim}

Similary, there is a \texttt{starts\_with} and \texttt{ends\_with} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#all columns starting with}
\KeywordTok{select}\NormalTok{(Batting, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"X"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 2
##     X2B   X3B
##   <int> <int>
## 1     0     0
## 2     6     0
## 3     4     5
## 4    10     2
## 5    11     3
## # ... with 1.059e+05 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#multiple selections}
\KeywordTok{select}\NormalTok{(Batting, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"X"}\NormalTok{), }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"ID"}\NormalTok{), G)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 7
##     X2B   X3B playerID  yearID teamID lgID      G
##   <int> <int> <chr>      <int> <fct>  <fct> <int>
## 1     0     0 abercda01   1871 TRO    NA        1
## 2     6     0 addybo01    1871 RC1    NA       25
## 3     4     5 allisar01   1871 CL1    NA       29
## 4    10     2 allisdo01   1871 WS3    NA       27
## 5    11     3 ansonca01   1871 RC1    NA       25
## # ... with 1.059e+05 more rows
\end{verbatim}

Sometimes we want to rename variables. This can be done with the \texttt{rename} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#rename our previous selection}
\KeywordTok{rename}\NormalTok{(}\KeywordTok{select}\NormalTok{(Batting, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"X"}\NormalTok{), }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"ID"}\NormalTok{), G), }\StringTok{"Doubles"}\NormalTok{ =}\StringTok{ }\NormalTok{X2B, }\StringTok{"Triples"}\NormalTok{ =}\StringTok{ }\NormalTok{X3B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 7
##   Doubles Triples playerID  yearID teamID lgID      G
##     <int>   <int> <chr>      <int> <fct>  <fct> <int>
## 1       0       0 abercda01   1871 TRO    NA        1
## 2       6       0 addybo01    1871 RC1    NA       25
## 3       4       5 allisar01   1871 CL1    NA       29
## 4      10       2 allisdo01   1871 WS3    NA       27
## 5      11       3 ansonca01   1871 RC1    NA       25
## # ... with 1.059e+05 more rows
\end{verbatim}

You may notice this is kind function nesting makes this code difficult for humans to parse. Piping or Chaining can be used to make the use of multiple functions easier!

\texttt{\%\textgreater{}\%} is the piping operator. Generically, piping does the following

\texttt{x\ \%\textgreater{}\%\ f(y)} turns into \texttt{f(x,y)}

\texttt{x\ \%\textgreater{}\%\ f(y)\ \%\textgreater{}\%\ g(z)} turns into \texttt{g(f(x,\ y),\ z)}

Since the tidyverse functions all have the same syntax, piping works wonders for readability! Piping can be used with functions outside the tidyverse if this structure works. Let's rewrite our previous nested function with piping. When reading code with piping, read \texttt{\%\textgreater{}\%} as the word `then.'

Batting data set (then) select these columns (then) rename the variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Batting }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"X"}\NormalTok{), }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"ID"}\NormalTok{), G) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{rename}\NormalTok{(}\StringTok{"Doubles"}\NormalTok{ =}\StringTok{ }\NormalTok{X2B, }\StringTok{"Triples"}\NormalTok{ =}\StringTok{ }\NormalTok{X3B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 7
##   Doubles Triples playerID  yearID teamID lgID      G
##     <int>   <int> <chr>      <int> <fct>  <fct> <int>
## 1       0       0 abercda01   1871 TRO    NA        1
## 2       6       0 addybo01    1871 RC1    NA       25
## 3       4       5 allisar01   1871 CL1    NA       29
## 4      10       2 allisdo01   1871 WS3    NA       27
## 5      11       3 ansonca01   1871 RC1    NA       25
## # ... with 1.059e+05 more rows
\end{verbatim}

We may also wish to reorder our columns (variables). This can be done using \texttt{select}. The \texttt{everything} function is handy so you don't have to list all the variables out if you only want to reorder a few.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Batting }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(playerID, HR, }\KeywordTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 105,861 x 22
##   playerID    HR yearID stint teamID lgID      G    AB     R     H   X2B   X3B
##   <chr>    <int>  <int> <int> <fct>  <fct> <int> <int> <int> <int> <int> <int>
## 1 abercda~     0   1871     1 TRO    NA        1     4     0     0     0     0
## 2 addybo01     0   1871     1 RC1    NA       25   118    30    32     6     0
## 3 allisar~     0   1871     1 CL1    NA       29   137    28    40     4     5
## 4 allisdo~     2   1871     1 WS3    NA       27   133    28    44    10     2
## 5 ansonca~     0   1871     1 RC1    NA       25   120    29    39    11     3
## # ... with 1.059e+05 more rows, and 10 more variables: RBI <int>, SB <int>,
## #   CS <int>, BB <int>, SO <int>, IBB <int>, HBP <int>, SH <int>, SF <int>,
## #   GIDP <int>
\end{verbatim}

Another commonly done column manipulation is the creating of new variables.

\begin{center}\includegraphics[width=0.8\linewidth]{img/createVarVisualF} \end{center}

Given a data frame and an appropriate length vector (new variable) we can use \texttt{cbind} (column bind) to add the variable to the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(iris, }\DataTypeTok{extra =} \KeywordTok{rep}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\DecValTok{150}\NormalTok{))}
\KeywordTok{str}\NormalTok{(temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  6 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ extra       : Factor w/ 1 level "a": 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

More simply we can just add the new variable as a named (list) element!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{extra <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\DecValTok{150}\NormalTok{)}
\KeywordTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  6 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ extra       : chr  "a" "a" "a" "a" ...
\end{verbatim}

To stay in the tidyverse and add more functionality we can use two functions from \texttt{dplyr}:

\begin{itemize}
\item
  \texttt{mutate()} - add newly created column(s) to current data frame
\item
  \texttt{transmute()} - create new data frame with created variable(s)
\end{itemize}

The syntax for these functions is similar to previous. We simply name the new variables after specifying our data set.

\texttt{mutate(data,\ newVarName\ =\ functionOfData,\ newVarName2\ =\ functionOfData,\ ...)}\\
For this section let's consider a data set on movie ratings from the \texttt{fivethirtyeight} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fivethirtyeight)}
\NormalTok{fandango}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 23
##   film   year rottentomatoes rottentomatoes_~ metacritic metacritic_user  imdb
##   <chr> <dbl>          <int>            <int>      <int>           <dbl> <dbl>
## 1 Aven~  2015             74               86         66             7.1   7.8
## 2 Cind~  2015             85               80         67             7.5   7.1
## 3 Ant-~  2015             80               90         64             8.1   7.8
## 4 Do Y~  2015             18               84         22             4.7   5.4
## 5 Hot ~  2015             14               28         29             3.4   5.1
## # ... with 141 more rows, and 16 more variables: fandango_stars <dbl>,
## #   fandango_ratingvalue <dbl>, rt_norm <dbl>, rt_user_norm <dbl>,
## #   metacritic_norm <dbl>, metacritic_user_nom <dbl>, imdb_norm <dbl>,
## #   rt_norm_round <dbl>, rt_user_norm_round <dbl>, metacritic_norm_round <dbl>,
## #   metacritic_user_norm_round <dbl>, imdb_norm_round <dbl>,
## #   metacritic_user_vote_count <int>, imdb_user_vote_count <int>,
## #   fandango_votes <int>, fandango_difference <dbl>
\end{verbatim}

We can add a new variable that is the average of two columns using \texttt{mutate}. Remember to read \texttt{\%\textgreater{}\%} as `then.'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{avgRotten =}\NormalTok{ (rottentomatoes }\OperatorTok{+}\StringTok{ }\NormalTok{rottentomatoes_user)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(film, year, avgRotten, }\KeywordTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 24
##   film   year avgRotten rottentomatoes rottentomatoes_~ metacritic
##   <chr> <dbl>     <dbl>          <int>            <int>      <int>
## 1 Aven~  2015      80               74               86         66
## 2 Cind~  2015      82.5             85               80         67
## 3 Ant-~  2015      85               80               90         64
## 4 Do Y~  2015      51               18               84         22
## 5 Hot ~  2015      21               14               28         29
## # ... with 141 more rows, and 18 more variables: metacritic_user <dbl>,
## #   imdb <dbl>, fandango_stars <dbl>, fandango_ratingvalue <dbl>,
## #   rt_norm <dbl>, rt_user_norm <dbl>, metacritic_norm <dbl>,
## #   metacritic_user_nom <dbl>, imdb_norm <dbl>, rt_norm_round <dbl>,
## #   rt_user_norm_round <dbl>, metacritic_norm_round <dbl>,
## #   metacritic_user_norm_round <dbl>, imdb_norm_round <dbl>,
## #   metacritic_user_vote_count <int>, imdb_user_vote_count <int>,
## #   fandango_votes <int>, fandango_difference <dbl>
\end{verbatim}

More than one variable can be created. Here an average of the normed metacritic scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{avgRotten =}\NormalTok{ (rottentomatoes }\OperatorTok{+}\StringTok{ }\NormalTok{rottentomatoes_user)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }
         \DataTypeTok{avgMeta =}\NormalTok{ (metacritic_norm }\OperatorTok{+}\StringTok{ }\NormalTok{metacritic_user_nom)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(film, year, avgRotten, avgMeta, }\KeywordTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 25
##   film   year avgRotten avgMeta rottentomatoes rottentomatoes_~ metacritic
##   <chr> <dbl>     <dbl>   <dbl>          <int>            <int>      <int>
## 1 Aven~  2015      80      3.42             74               86         66
## 2 Cind~  2015      82.5    3.55             85               80         67
## 3 Ant-~  2015      85      3.62             80               90         64
## 4 Do Y~  2015      51      1.72             18               84         22
## 5 Hot ~  2015      21      1.58             14               28         29
## # ... with 141 more rows, and 18 more variables: metacritic_user <dbl>,
## #   imdb <dbl>, fandango_stars <dbl>, fandango_ratingvalue <dbl>,
## #   rt_norm <dbl>, rt_user_norm <dbl>, metacritic_norm <dbl>,
## #   metacritic_user_nom <dbl>, imdb_norm <dbl>, rt_norm_round <dbl>,
## #   rt_user_norm_round <dbl>, metacritic_norm_round <dbl>,
## #   metacritic_user_norm_round <dbl>, imdb_norm_round <dbl>,
## #   metacritic_user_vote_count <int>, imdb_user_vote_count <int>,
## #   fandango_votes <int>, fandango_difference <dbl>
\end{verbatim}

\texttt{transmute} is very similar to mutate except it doesn't return the original tibble, just the newly created variable(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{transmute}\NormalTok{(}\DataTypeTok{avgRotten =}\NormalTok{ (rottentomatoes }\OperatorTok{+}\StringTok{ }\NormalTok{rottentomatoes_user)}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 1
##   avgRotten
##       <dbl>
## 1      80  
## 2      82.5
## 3      85  
## 4      51  
## 5      21  
## # ... with 141 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{transmute}\NormalTok{(}\DataTypeTok{avgRotten =}\NormalTok{ (rottentomatoes }\OperatorTok{+}\StringTok{ }\NormalTok{rottentomatoes_user)}\OperatorTok{/}\DecValTok{2}\NormalTok{, }
                       \DataTypeTok{avgMeta =}\NormalTok{ (metacritic_norm }\OperatorTok{+}\StringTok{ }\NormalTok{metacritic_user_nom)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 2
##   avgRotten avgMeta
##       <dbl>   <dbl>
## 1      80      3.42
## 2      82.5    3.55
## 3      85      3.62
## 4      51      1.72
## 5      21      1.58
## # ... with 141 more rows
\end{verbatim}

\texttt{mutate} and \texttt{transmute} can also use `window' functions. These are functions that take a vector of values and return another vector of values (see \href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{Cheat sheet}). For instance we can find the cumulative sum of a column using \texttt{cumsum}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(rottentomatoes) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{cumulativeSum =} \KeywordTok{cumsum}\NormalTok{(rottentomatoes))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 2
##   rottentomatoes cumulativeSum
##            <int>         <int>
## 1             74            74
## 2             85           159
## 3             80           239
## 4             18           257
## 5             14           271
## # ... with 141 more rows
\end{verbatim}

\texttt{mutate} and \texttt{transmute} can also use some statistical functions to create new variables. Here we add a column representing the mean and standard deviation of the rottentomatoes score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(rottentomatoes) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(rottentomatoes), }\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(rottentomatoes))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 3
##   rottentomatoes   avg    sd
##            <int> <dbl> <dbl>
## 1             74  60.8  30.2
## 2             85  60.8  30.2
## 3             80  60.8  30.2
## 4             18  60.8  30.2
## 5             14  60.8  30.2
## # ... with 141 more rows
\end{verbatim}

These statistical quantities are easily found for subgroups of the data using the \texttt{group\_by} function. We can group the data set by year and run the same \texttt{mutate} function. Now the mean and standard deviation are found for each year and appended appropriately.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fandango }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(year, rottentomatoes) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(year) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(rottentomatoes), }\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(rottentomatoes))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 146 x 4
## # Groups:   year [2]
##    year rottentomatoes   avg    sd
##   <dbl>          <int> <dbl> <dbl>
## 1  2015             74  58.4  30.3
## 2  2015             85  58.4  30.3
## 3  2015             80  58.4  30.3
## 4  2015             18  58.4  30.3
## 5  2015             14  58.4  30.3
## # ... with 141 more rows
\end{verbatim}

Another important way to create variables is through the use of conditional logic. This allows code to be executed only under certain conditions. The main way this is done is through \texttt{if} \texttt{then} \texttt{else} syntax.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (condition) \{}
\NormalTok{  then execute code}
\NormalTok{\} }

\CommentTok{#if then else}
\ControlFlowTok{if}\NormalTok{ (condition) \{}
\NormalTok{  execute this code  }
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{  execute this code}
\NormalTok{\}}

\CommentTok{#Or more if statements}
\ControlFlowTok{if}\NormalTok{ (condition) \{}
\NormalTok{  execute this code  }
\NormalTok{\} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (condition2) \{}
\NormalTok{  execute this code}
\NormalTok{\} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (condition3) \{}
\NormalTok{  execute this code}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{#if no conditions met}
\NormalTok{  execute this code}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Consider the built-in data set \texttt{airquality}. This hasdaily air quality measurements in New York from May (Day 1) to September (Day 153) in 1973.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airquality <-}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{(airquality)}
\NormalTok{airquality}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 153 x 6
##   Ozone Solar.R  Wind  Temp Month   Day
##   <int>   <int> <dbl> <int> <int> <int>
## 1    41     190   7.4    67     5     1
## 2    36     118   8      72     5     2
## 3    12     149  12.6    74     5     3
## 4    18     313  11.5    62     5     4
## 5    NA      NA  14.3    56     5     5
## # ... with 148 more rows
\end{verbatim}

We may want to code a wind category variable:

\begin{itemize}
\tightlist
\item
  high wind days (15mph \(\leq\) wind)\\
\item
  windy days (10mph \(\leq\) wind \textless{} 15mph)\\
\item
  lightwind days (6mph \(\leq\) wind \textless{} 10mph)\\
\item
  calm days (wind \(\leq\) 6mph)
\end{itemize}

We may think using of using the standard \texttt{if} statements above. The issue is that \texttt{if(condition)} can only take in a single comparison.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{15}\NormalTok{) \{ }
  \StringTok{"High Wind"}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in if (airquality$Wind >= 15) {: the condition has length > 1 and only
## the first element will be used
\end{verbatim}

If you've programmed before you may think about this as an initial plan:

\begin{itemize}
\item
  loop through each observation
\item
  use if then else to determine wind status
\end{itemize}

There are a number of ways to do looping in R

\begin{itemize}
\item
  \texttt{for}
\item
  \texttt{while}
\item
  \texttt{repeat}
\end{itemize}

The idea of a loop is to run code repeatedly changing something each time. The syntax for the \texttt{for} loop is

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(index }\ControlFlowTok{in}\NormalTok{ values)\{}
\NormalTok{  code to be run}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The index defines the `counter' or variable that varies as the loop iterates and `values' define which values the index takes on.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)\{}
  \KeywordTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5
## [1] 6
## [1] 7
## [1] 8
## [1] 9
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (index }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\StringTok{"cat"}\NormalTok{,}\StringTok{"hat"}\NormalTok{,}\StringTok{"worm"}\NormalTok{))\{}
  \KeywordTok{print}\NormalTok{(index)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "cat"
## [1] "hat"
## [1] "worm"
\end{verbatim}

If we want to code our wind variable we could run a \texttt{for} loop with \texttt{if} logic inside:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{status<-}\KeywordTok{vector}\NormalTok{() }\CommentTok{#initialize vector to save results}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(airquality))\{}
  \ControlFlowTok{if}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind[i] }\OperatorTok{>=}\StringTok{ }\DecValTok{15}\NormalTok{)\{}
\NormalTok{    status[i] <-}\StringTok{ "HighWind"}
\NormalTok{  \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (airquality}\OperatorTok{$}\NormalTok{Wind[i] }\OperatorTok{>=}\StringTok{ }\DecValTok{10}\NormalTok{)\{}
\NormalTok{    status[i] <-}\StringTok{ "Windy"}
\NormalTok{  \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (airquality}\OperatorTok{$}\NormalTok{Wind[i] }\OperatorTok{>=}\StringTok{ }\DecValTok{6}\NormalTok{)\{}
\NormalTok{    status[i] <-}\StringTok{ "LightWind"}
\NormalTok{  \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (airquality}\OperatorTok{$}\NormalTok{Wind[i] }\OperatorTok{>=}\StringTok{ }\DecValTok{0}\NormalTok{)\{}
\NormalTok{    status[i] <-}\StringTok{ "Calm"}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    status[i] <-}\StringTok{ "Error"}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then we can append the new variable to our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airquality}\OperatorTok{$}\NormalTok{status <-}\StringTok{ }\NormalTok{status}
\NormalTok{airquality}\OperatorTok{$}\NormalTok{status}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] "LightWind" "LightWind" "Windy"     "Windy"     "Windy"     "Windy"    
##   [7] "LightWind" "Windy"     "HighWind"  "LightWind" "LightWind" "LightWind"
##  [13] "LightWind" "Windy"     "Windy"     "Windy"     "Windy"     "HighWind" 
##  [19] "Windy"     "LightWind" "LightWind" "HighWind"  "LightWind" "Windy"    
##  [25] "HighWind"  "Windy"     "LightWind" "Windy"     "Windy"     "Calm"     
##  [31] "LightWind" "LightWind" "LightWind" "HighWind"  "LightWind" "LightWind"
##  [37] "Windy"     "LightWind" "LightWind" "Windy"     "Windy"     "Windy"    
##  [43] "LightWind" "LightWind" "Windy"     "Windy"     "Windy"     "HighWind" 
##  [49] "LightWind" "Windy"     "Windy"     "LightWind" "Calm"      "Calm"     
##  [55] "LightWind" "LightWind" "LightWind" "Windy"     "Windy"     "Windy"    
##  [61] "LightWind" "Calm"      "LightWind" "LightWind" "Windy"     "Calm"     
##  [67] "Windy"     "Calm"      "LightWind" "Calm"      "LightWind" "LightWind"
##  [73] "Windy"     "Windy"     "Windy"     "Windy"     "LightWind" "Windy"    
##  [79] "LightWind" "Calm"      "Windy"     "LightWind" "LightWind" "Windy"    
##  [85] "LightWind" "LightWind" "LightWind" "Windy"     "LightWind" "LightWind"
##  [91] "LightWind" "LightWind" "LightWind" "Windy"     "LightWind" "LightWind"
##  [97] "LightWind" "Calm"      "Calm"      "Windy"    
##  [ reached getOption("max.print") -- omitted 53 entries ]
\end{verbatim}

This works just fine! Some other things to be aware of with loops:

\begin{itemize}
\tightlist
\item
  \texttt{break} kicks you out of the loop
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)\{}
    \ControlFlowTok{if}\NormalTok{ (i }\OperatorTok{==}\StringTok{ }\DecValTok{3}\NormalTok{)\{ }
      \ControlFlowTok{break} 
\NormalTok{      \}}
  \KeywordTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{next} jumps to the next iteration of the loop
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)\{}
    \ControlFlowTok{if}\NormalTok{ (i }\OperatorTok{==}\StringTok{ }\DecValTok{3}\NormalTok{)\{}
      \ControlFlowTok{next}
\NormalTok{    \} }
  \KeywordTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 2
## [1] 4
## [1] 5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{while} loop are similar
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{while}\NormalTok{(condition) \{}
\NormalTok{    expression to evaluate}
\NormalTok{  modify condition?}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The main issue with loops in R is that they are inefficient. R is an interpreted language so it must figure out how to evaluate code at each iteration of loop, slowing it down.

Vectorized functions are much faster! These functions work on an entire vector at once so R doesn't have to figure things out as often. \texttt{ifelse()} is a vectorized version of \texttt{if\ then\ else}. The syntax is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ifelse}\NormalTok{(vector_condition, if_true_do_this, if_false_do_this)}
\end{Highlighting}
\end{Shaded}

Now to create our Wind status variable we can nest \texttt{ifelse} statements.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{15}\NormalTok{, }\StringTok{"HighWind"}\NormalTok{,}
          \KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{10}\NormalTok{, }\StringTok{"Windy"}\NormalTok{,}
                 \KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{6}\NormalTok{, }\StringTok{"LightWind"}\NormalTok{, }\StringTok{"Calm"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] "LightWind" "LightWind" "Windy"     "Windy"     "Windy"     "Windy"    
##   [7] "LightWind" "Windy"     "HighWind"  "LightWind" "LightWind" "LightWind"
##  [13] "LightWind" "Windy"     "Windy"     "Windy"     "Windy"     "HighWind" 
##  [19] "Windy"     "LightWind" "LightWind" "HighWind"  "LightWind" "Windy"    
##  [25] "HighWind"  "Windy"     "LightWind" "Windy"     "Windy"     "Calm"     
##  [31] "LightWind" "LightWind" "LightWind" "HighWind"  "LightWind" "LightWind"
##  [37] "Windy"     "LightWind" "LightWind" "Windy"     "Windy"     "Windy"    
##  [43] "LightWind" "LightWind" "Windy"     "Windy"     "Windy"     "HighWind" 
##  [49] "LightWind" "Windy"     "Windy"     "LightWind" "Calm"      "Calm"     
##  [55] "LightWind" "LightWind" "LightWind" "Windy"     "Windy"     "Windy"    
##  [61] "LightWind" "Calm"      "LightWind" "LightWind" "Windy"     "Calm"     
##  [67] "Windy"     "Calm"      "LightWind" "Calm"      "LightWind" "LightWind"
##  [73] "Windy"     "Windy"     "Windy"     "Windy"     "LightWind" "Windy"    
##  [79] "LightWind" "Calm"      "Windy"     "LightWind" "LightWind" "Windy"    
##  [85] "LightWind" "LightWind" "LightWind" "Windy"     "LightWind" "LightWind"
##  [91] "LightWind" "LightWind" "LightWind" "Windy"     "LightWind" "LightWind"
##  [97] "LightWind" "Calm"      "Calm"      "Windy"    
##  [ reached getOption("max.print") -- omitted 53 entries ]
\end{verbatim}

\texttt{ifelse} can also easily be used with \texttt{transmute()} or \texttt{mutate()}!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mutate}\NormalTok{(airquality, }\DataTypeTok{status =} \KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{15}\NormalTok{, }\StringTok{"HighWind"}\NormalTok{,}
                                \KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{10}\NormalTok{, }\StringTok{"Windy"}\NormalTok{,}
                                       \KeywordTok{ifelse}\NormalTok{(airquality}\OperatorTok{$}\NormalTok{Wind }\OperatorTok{>=}\StringTok{ }\DecValTok{6}\NormalTok{, }\StringTok{"LightWind"}\NormalTok{, }\StringTok{"Calm"}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 153 x 7
##   Ozone Solar.R  Wind  Temp Month   Day status   
##   <int>   <int> <dbl> <int> <int> <int> <chr>    
## 1    41     190   7.4    67     5     1 LightWind
## 2    36     118   8      72     5     2 LightWind
## 3    12     149  12.6    74     5     3 Windy    
## 4    18     313  11.5    62     5     4 Windy    
## 5    NA      NA  14.3    56     5     5 Windy    
## # ... with 148 more rows
\end{verbatim}

Note: the \texttt{cut} function can also be used to categorize a numeric variable pretty easily.

This covers the major uses of \texttt{dplyr} for manipulating rows and columns. \texttt{dplyr} also has great functionality for doing \texttt{joins} similar to SQL. We'll also see how it can be used to create basic numeric summaries using \texttt{group\_by} and \texttt{summarize}. The \href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{cheat sheet} is a great reference!

Recap of basic commands:

\begin{itemize}
\tightlist
\item
  \texttt{tbl\_df} - convert data frame to one with better printing\\
\item
  \texttt{filter} - subset rows\\
\item
  \texttt{arrange} - reorder rows\\
\item
  \texttt{select} - subset columns\\
\item
  \texttt{rename} - reorder columns\\
\item
  \texttt{mutate/transmute} - create new variable
\end{itemize}

\hypertarget{reshaping-data}{%
\subsection{Reshaping Data}\label{reshaping-data}}

We've talked about rows being observations and columns being variables. This is generally how most statistical analysis software likes their data to be formatted. This is called `long' format data - each row is an observation. Sometimes data doesn't come that way!

\begin{center}\includegraphics[width=0.7\linewidth]{img/longWideF} \end{center}

Data may have observations across some columns since viewing data is often more natural that way. For example, consider the weather data set below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempsData <-}\StringTok{ }\KeywordTok{read_table2}\NormalTok{(}\DataTypeTok{file =} \StringTok{"../../datasets/cityTemps.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   city = col_character(),
##   sun = col_double(),
##   mon = col_double(),
##   tue = col_double(),
##   wed = col_double(),
##   thr = col_double(),
##   fri = col_double(),
##   sat = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempsData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 8
##   city        sun   mon   tue   wed   thr   fri   sat
##   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1 atlanta      81    87    83    79    88    91    94
## 2 baltimore    73    75    70    78    73    75    79
## 3 charlotte    82    80    75    82    83    88    93
## 4 denver       72    71    67    68    72    71    58
## 5 ellington    51    42    47    52    55    56    59
## 6 frankfort    70    70    72    70    74    74    79
\end{verbatim}

This data set is said to be in `wide' format because columns represent observations. For most analyses this type of data will need to be reshaped into long format. The \texttt{tidyr} package can be used for this purpose!

The \texttt{gather} function takes multiple columns and gathers them into key-value pairs. This tkes wide data and makes it long.

Similarly there is a \texttt{spread} function takes two columns (key \& value) and spreads in to multiple columns. This takes long data and makes it wide.

Let's switch the tempsData dataset to `long' form with \texttt{gather()}. We need to identify the

\begin{itemize}
\item
  key = new name for values in columns
\item
  value = new name for data values
\item
  columns describe which columns to take
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempsData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ day, }\DataTypeTok{value =}\NormalTok{ temp, }\DecValTok{2}\OperatorTok{:}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 42 x 3
##   city      day    temp
##   <chr>     <chr> <dbl>
## 1 atlanta   sun      81
## 2 baltimore sun      73
## 3 charlotte sun      82
## 4 denver    sun      72
## 5 ellington sun      51
## # ... with 37 more rows
\end{verbatim}

The columns can be provided to gather in similar ways to how we chose them in the \texttt{select} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newTempsData <-}\StringTok{ }\NormalTok{tempsData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ day, }\DataTypeTok{value =}\NormalTok{ temp, sun}\OperatorTok{:}\NormalTok{sat)}
\NormalTok{newTempsData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 42 x 3
##   city      day    temp
##   <chr>     <chr> <dbl>
## 1 atlanta   sun      81
## 2 baltimore sun      73
## 3 charlotte sun      82
## 4 denver    sun      72
## 5 ellington sun      51
## # ... with 37 more rows
\end{verbatim}

To give an example of using \texttt{spread} we can take our long format data and turn it back into wide format. WE just need to identify the:

\begin{itemize}
\item
  key = new column names
\item
  value = value to spread out
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newTempsData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ day, }\DataTypeTok{value =}\NormalTok{ temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 8
##   city        fri   mon   sat   sun   thr   tue   wed
##   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1 atlanta      91    87    94    81    88    83    79
## 2 baltimore    75    75    79    73    73    70    78
## 3 charlotte    88    80    93    82    83    75    82
## 4 denver       71    71    58    72    72    67    68
## 5 ellington    56    42    59    51    55    47    52
## 6 frankfort    74    70    79    70    74    72    70
\end{verbatim}

The \texttt{tidyr} package also has useful functions for separating a column (or combining two columns) using \texttt{separate} (and \texttt{unite})

\begin{center}\includegraphics[width=0.8\linewidth]{img/uniteSeparateF} \end{center}

Consider a data set on air pollution in Chicago.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chicagoData <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"../../datasets/Chicago.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   X = col_double(),
##   city = col_character(),
##   date = col_character(),
##   death = col_double(),
##   temp = col_double(),
##   dewpoint = col_double(),
##   pm10 = col_double(),
##   o3 = col_double(),
##   time = col_double(),
##   season = col_character(),
##   year = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chicagoData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,461 x 11
##       X city  date     death  temp dewpoint  pm10    o3  time season  year
##   <dbl> <chr> <chr>    <dbl> <dbl>    <dbl> <dbl> <dbl> <dbl> <chr>  <dbl>
## 1  3654 chic  1/1/1997   137  36       37.5  13.1  5.66  3654 winter  1997
## 2  3655 chic  1/2/1997   123  45       47.2  41.9  5.53  3655 winter  1997
## 3  3656 chic  1/3/1997   127  40       38    27.0  6.29  3656 winter  1997
## 4  3657 chic  1/4/1997   146  51.5     45.5  25.1  7.54  3657 winter  1997
## 5  3658 chic  1/5/1997   102  27       11.2  15.3 20.8   3658 winter  1997
## # ... with 1,456 more rows
\end{verbatim}

The \texttt{lubridate} package great for date data but let's just do some basic parsing of the \texttt{date} variable using \texttt{separate}. We can split the \texttt{date} variable by the \texttt{/} that separates the day, month, and year to create three new columns. Using \texttt{remove\ =\ FALSE} keeps the original variable (\texttt{date}) and \texttt{convert\ =\ TRUE} attempts to convert the newly created variables to numeric if possible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chicagoData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{separate}\NormalTok{(date, }\KeywordTok{c}\NormalTok{(}\StringTok{"Day"}\NormalTok{, }\StringTok{"Month"}\NormalTok{, }\StringTok{"Year"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"/"}\NormalTok{, }
                                                 \DataTypeTok{convert =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,461 x 14
##       X city  date    Day Month  Year death  temp dewpoint  pm10    o3  time
##   <dbl> <chr> <chr> <int> <int> <int> <dbl> <dbl>    <dbl> <dbl> <dbl> <dbl>
## 1  3654 chic  1/1/~     1     1  1997   137  36       37.5  13.1  5.66  3654
## 2  3655 chic  1/2/~     1     2  1997   123  45       47.2  41.9  5.53  3655
## 3  3656 chic  1/3/~     1     3  1997   127  40       38    27.0  6.29  3656
## 4  3657 chic  1/4/~     1     4  1997   146  51.5     45.5  25.1  7.54  3657
## 5  3658 chic  1/5/~     1     5  1997   102  27       11.2  15.3 20.8   3658
## # ... with 1,456 more rows, and 2 more variables: season <chr>, year <dbl>
\end{verbatim}

Similarly we can combine columns with \texttt{unite}. Let's create a new column that is just the day and month separated by a \texttt{-}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chicagoData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{separate}\NormalTok{(date, }\KeywordTok{c}\NormalTok{(}\StringTok{"Day"}\NormalTok{, }\StringTok{"Month"}\NormalTok{, }\StringTok{"Year"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"/"}\NormalTok{, }
                                                 \DataTypeTok{convert =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unite}\NormalTok{(DayMonth, Day, Month, }\DataTypeTok{sep =} \StringTok{"-"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,461 x 13
##       X city  date  DayMonth  Year death  temp dewpoint  pm10    o3  time season
##   <dbl> <chr> <chr> <chr>    <int> <dbl> <dbl>    <dbl> <dbl> <dbl> <dbl> <chr> 
## 1  3654 chic  1/1/~ 1-1       1997   137  36       37.5  13.1  5.66  3654 winter
## 2  3655 chic  1/2/~ 1-2       1997   123  45       47.2  41.9  5.53  3655 winter
## 3  3656 chic  1/3/~ 1-3       1997   127  40       38    27.0  6.29  3656 winter
## 4  3657 chic  1/4/~ 1-4       1997   146  51.5     45.5  25.1  7.54  3657 winter
## 5  3658 chic  1/5/~ 1-5       1997   102  27       11.2  15.3 20.8   3658 winter
## # ... with 1,456 more rows, and 1 more variable: year <dbl>
\end{verbatim}

You should now be ready to use R to get data in and do some basic manipulation!

\hypertarget{sas}{%
\section{SAS}\label{sas}}

The general workflow for programming in SAS is similar to that of R. First raw data must be imported to SAS. Once that data is imported you will find an appropriate PROC (or procedure) that will summarize or analyze your data appropriate. Often times relevant graphs and summaries are created with a single PROC.

At the end of this section the reader should be able to do the following:

\begin{itemize}
\tightlist
\item
  install SAS University Edition\\
\item
  read and write basic SAS programs\\
\item
  import well-formatted data into SAS
\item
  do basic data manipulation in SAS
\end{itemize}

As the book progresses the steps of summarizing and analyzing the data will be covered. Let's get started!

\hypertarget{basics-of-sas}{%
\subsection{Basics of SAS}\label{basics-of-sas}}

\hypertarget{reading-data-with-sas}{%
\subsection{Reading Data with SAS}\label{reading-data-with-sas}}

\hypertarget{manipulating-data-with-sas}{%
\subsection{Manipulating Data with SAS}\label{manipulating-data-with-sas}}

\hypertarget{sampling-schemes-and-experimental-design}{%
\chapter{Sampling Schemes and Experimental Design}\label{sampling-schemes-and-experimental-design}}

\hypertarget{data-in-the-wild}{%
\section{Data in the Wild}\label{data-in-the-wild}}

Data is a collection of information about a group of individuals or units. Most often we have a number of variables, or measures of interest, that we observe on each individual or unit. The collection of information is called a dataset. Data is ubiquitous in today's society. Healthcare, marketing, history, biology, \ldots{} almost every field has data for which a sound statistical analysis can glean useful insights. However, the quality of data varies greatly from study to study and this implies the conclusions which you can draw from a study vary as well. Let's jump in!

\hypertarget{data-from-experiments}{%
\subsection{Data from Experiments}\label{data-from-experiments}}

Some data comes from a well-designed experiment where a researcher uses sound principles to select units for the study and conduct interventions.

For example, a mechanical engineer wanted to determine which variables influence gas mileage of a certain year and model of a car. Gas mileage would be referred to as the \textbf{response} variable for this study since it characterized the performance of interest.

After careful consideration, the engineer chose to investigate a few \textbf{explanatory variables} they believed were associated with the response. They wanted to learn about the relationship between gas mileage and the factors below. A \textbf{factor} is an explanatory variable that takes on a finite number of values, called levels, set by the researcher.

Study factors (\textbf{levels} of each factor are given in parentheses):

\begin{itemize}
\tightlist
\item
  Tire pressure (low, standard)\\
\item
  Octane rating of fuel (regular, midgrade, premium)\\
\item
  Type of driving (defensive, aggressive)
\end{itemize}

They also chose to \textbf{control} or hold constant the following variables during the execution of the study:

\begin{itemize}
\tightlist
\item
  Weather conditions\\
\item
  Route\\
\item
  Tire type\\
\item
  Past car usage
\end{itemize}

The engineer randomly selected a \textbf{sample} of 24 cars from the assembly line for that year and model of car (we'll learn more about the importance of selecting a representative sample of cars shortly). Software was used to randomly assign a \textbf{treatment} to each of the 24 cars. A treatment is a particular combination of the factor levels. For instance, low tire pressure, regular octane fuel, and defensive driving was a treatment. The cars would be called the \textbf{experimental units} (EUs) as they are the unit the treatments are assigned to.

The experiment was run and the gas mileage found for each car. As the car was measured, we'd refer to the car as the \textbf{observational unit} (OU).

The key thing that makes this study an \textbf{experimental study} is the active role the research plays in manipulating the environment. Here, the researcher uses random assignment of treatments to the EUs.

\BeginKnitrBlock{definition}
Experimental Study - researchers manipulate the conditions in which the study is done.
\EndKnitrBlock{definition}

\textbf{Visual of experiment - maybe a researcher with arrows going out to cars where the tires, gas tank, and driver are emphasized in some way}

This short description exhibits three important concepts in experimental design that we'll come back to many times.

Pillars of experimental design: (Put an outer block around this)

\BeginKnitrBlock{definition}
\begin{itemize}
\tightlist
\item
  Randomization - treatments are randomly assigned to experimental units\\
\end{itemize}
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}
\begin{itemize}
\tightlist
\item
  Replication - multiple (independent) experimental units are assigned the same treatment\\
\end{itemize}
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}
\begin{itemize}
\tightlist
\item
  Control - some study conditions are held constant to reduce variability in the response\\
\end{itemize}
\EndKnitrBlock{definition}

\hypertarget{data-from-observational-studies}{%
\subsection{Data from Observational Studies}\label{data-from-observational-studies}}

Some data comes from an observational study where the researcher collects data without imposing any changes.

For example, an economist wanted to investigate the effects of recently added tariffs on agricultural products to the amount and value of such products that are traded between the United States and Asia. This study had two \textbf{response} variables, the amount and value of each product traded between the two parties.

In order to take into account seasonal variation and time of year, the economist decided to compare the two response variables from the current year - 6 months worth of data - against the values of the two response variables during the same 6 month periods for each of the past 5 years. The year variable associated with a measurement was an \textbf{explanatory variable}. Alternatively, the year variable could have also been labeled to take on one of two values: no-tariff (past years' data) or tariff (current year's data).

The researcher obtained the data from the census bureau and conducted their analysis.

Notice that the researcher, while certainly being actively involved in the careful consideration of the data to be collected and how to format the data, did not actively intervene or impose a change. This is the key component of an \textbf{observational study}.

\BeginKnitrBlock{definition}
Observational Study - researchers collect data without imposing any changes on the study environment.
\EndKnitrBlock{definition}

\textbf{Visual of observational study here - something like a researcher with a clipboard looked at a globe with arrows to represent trading or something like that}

\hypertarget{observational-vs-experimental-studies}{%
\subsection{Observational vs Experimental Studies}\label{observational-vs-experimental-studies}}

You may have noticed that both example studies had some things in common. For instance, both studies had \textbf{response} variables that characterize the performance of the study in some sense. Importantly, these response variables had variation. That is, observing the variable is non-deterministic even under seemingly identical situations. Accounting for, and dealing with, this variation is at the heart of the reason statistical methods are needed! There were also \textbf{explanatory variables} that the researcher was interested in with regard to their relationship with the response variable. Determing ad quantifying these relationships is often the major goal of a study.

Both studies also hoped to make statements or conclusions about a larger group using data from a subset of that larger group. This idea is referred to as \textbf{statistical inference}. More formally the group of values, items, measurements, or individuals of interest defines the \textbf{population} of interest and the data collected on that group represents the \textbf{sample}. The number of observations in the sample is referred to as the \textbf{sample size}. For the gas mileage example, the population was all cars of the year and make in question, the sample was the data collected on the 24 cars, and the sample size was 24. For the tariff example, the population was all future agricultural products traded between the United States and Asia, the sample was the information from the six years of trade data, and the sample size is six. The two populations mentioned here differ in that the car population is a \textbf{real, finite population} and the trade population is a \textbf{conceptual, infinite population}. As long as a finite population is large relative to the sample size, the differences tend not to be important. We'll discuss these ideas in more detail as they arise.

\BeginKnitrBlock{definition}
Population - (Possibly conceptual) group of values, items, measurements, or individuals of interest
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}
Sample - Subset of the population on which we observe data

Sample Size - Number of observations in the sample
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}
Statistical Inference - Process of using sample data to make statements or claims about a population. Two major goals of inference:\\
- Determining and quantifying relationships between explanatory variables and the response\\
- Predicting the response for some setting of explanatory variables.
\EndKnitrBlock{definition}

Both of these studies had to determine how to obtain their observations. For the experiment, 24 cars were used. For the observational study, six years of data were collected. How this data is collected can be extremely important in terms of the types of conclusions that can be made. Data needs to be \textbf{representative} of the population in which the researcher hopes to make inference. Otherwise, the conclusions made are likely invalid or in need of qualifications. We'll discuss the idea of what makes a good or bad \textbf{sampling scheme} later in the chapter.

The major difference between the two studies was the active (experimental) and passive (observational) roles played by the researcher. This difference is also of vital importance to the types of conclusions that can be made from the study. A well-designed experiment can often allow the researcher to infer \textbf{causation} to the treatments, whereas an observational study cannot.

The conclusions a researcher can make based on how the data were collected and the type of study are outlined in the table below.

\textbf{Redo this table with our own wording}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/ScopeOfInferenceTable} 

}

\caption{Scope of Inference, cite: Khan Academy}\label{fig:scope}
\end{figure}

Doing an observational study doesn't mean that your study is bad! An observational study is sometimes done out of necessity when an experiment wouldn't be ethical or feasible. For the tariff example, there really isn't a way to conduct an experiment. If we wanted to design an experiment to see if smoking causes lung cancer, that would be unethical because we can't force people to smoke. The key point is that the implications we can draw will differ greatly between experimental and observational studies and will depend heavily on the quality (in relation to the population) of the data you have. To apply causation to an observational study, \textbf{causal inference} methods can sometimes be used. We won't cover this extensive topic in this text. See the references and readings section for a few useful texts.

\hypertarget{the-role-of-statistics}{%
\subsection{The Role of Statistics}\label{the-role-of-statistics}}

A statistic itself is generally a summary of data. When most people think of statistics they think of things like a batting average or a proportion of people that will vote for a proposal. \textbf{Statistics} as a discipline is the science of learning from data. It encompasses the collection of data, the possible design of an experiment, the summarization of collected data, and the modeling or analysis used in order to make a decision or further scientific knowledge.

\BeginKnitrBlock{definition}
Statistics in everyday use usually refers simply to summaries about data (means/averages, proportions, or counts).

Statistics as a field encompasses a much larger range of ideas including how to collect data, model data, and make decisions or come to conclusions when faced with uncertainty.
\EndKnitrBlock{definition}

\textbf{Statistical methods are needed in situations where data is variable.} There is no need to apply statistical methods to study the relationship between temperature in degrees Celsius and degrees Fahrenheit. Given the degrees in Celsius, we know teh exact value in degrees Fahrenheit. However, if we again collected data about the gas mileage of vehicles under the exact same study conditions we'll get slightly different gas mileage readings. If we observed another six month period of trade data we'll see different amounts and values traded. Accounting for this variability in data is the reason to apply statistical methods and is a key component of any statistical analysis.

Ideally, one should try to take a holistic view of a study. Before any data is collected it is vital to understand the goals and background of the study. These will inform the data you ideally want to collect as well as the data that you are able to collect - which may need to act as a proxy. A plan should be determined for the actual collection and storing of the data. The entire study design will then inform the statistical analysis and conclusions that can be drawn.

Taking this bigger picture view of the problem, we can usually follow these steps:

\textbf{Add icons to these as well as the overall logo here }

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/defineObjective.png}
  Define the objective of the experiment and understand the background (Define Objective \& Background)\\
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/selectResponse.png} Select appropriate response variables (Select Response)\\
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/determineVariation.png} Identify sources of variation (Determine Sources of Variation)\\
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/selectDesign.png} Choose sampling scheme and/or experimental design (Select Design)\\
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/conductStudy.png} Carry out the study (Do Study)
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/statsAnalysis.png} Statistically analyze the data (Do Statistical Analysis)\\
\item
  \includegraphics[width=\textwidth,height=0.4in]{img/concludeCommunicate.png} Draw conclusions from the analysis while considering limitations and the steps above as well as communicate results (Draw Conclusions \& Communicate)
\end{enumerate}

We'll focus on this entire process in our chapter motivating examples. Mostly, we'll investigate designed experiments. We attempt to tackle each major topic in this text with a problem-based approach. That is, we identify a real-world motivating example and discuss the relevant statistical ideas in the context of that problem. We then provide a discussion of the main statistical ideas and concepts and provide related references and readings. Each chapter includes with a section that outlines the use of R and SAS for implemention. Finally, where applicable, we include a section that outlines some of the mathematical concepts - this section is always optional!

\hypertarget{motivating-example-sampling---farmers-market}{%
\section{Motivating Example: Sampling - Farmer's Market}\label{motivating-example-sampling---farmers-market}}

\hypertarget{define-objective-background}{%
\subsection{Define Objective \& Background}\label{define-objective-background}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/defineObjective}
A nutrition scientist wanted to understand the cleanliness and food hygiene of the vendors at the North Carolina State Farmer's Market (henceforth the farmer's market). Secondarily, she wanted to learn about vendor sales to see if there was a relationship with their cleanliness and food hygiene. The researcher had access to the names of each vendor's business, their general purpose, and the products they sold.

The researcher needed to decide the scope of their study. Formally, they needed to define the \textbf{population} of interest. The population is the group of people or units of interest to the researcher. As her interest centered around food-related businesses, she restricted to looking at the vendors which sold horticultural crops. She hoped that conclusions made by her study could apply to all horticulture vendors at the farmer's market - thus, this is her population.

Note: One could try to do a study at just the North Carolina State Farmer's Market and extend the results to all farmer's markets in the state or in the south, but that would require many assumptions to be valid.

A \href{http://www.ncagr.gov/markets/chart.htm}{list of the horticultural products sold and their availability} is reproduced below.

\begin{center}\includegraphics[width=0.65\linewidth]{img/availabilitychart-1} \end{center}

\begin{center}\includegraphics[width=0.65\linewidth]{img/availabilitychart-2} \end{center}

\hypertarget{select-response}{%
\subsection{Select Response}\label{select-response}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectResponse}
The researcher needed to determine the variables to collect that would best help to answer their questions of interest. These variables that characterize the experiment are called \textbf{response} or target variables.

To investigate the knowledge of hygiene and safety, a short questionnaire was developed to allow the vendor's head manager (or similar employee) to describe their safety protocol and knowledge:

\begin{itemize}
\tightlist
\item
  For your produce with signs that say ``clean'' or ``washed'', what does this mean?
\item
  How are the foods transported to the market? eg: refrigerated/closed storage\\
\item
  What food safety risks do you as a vendor worry about?\\
\item
  Do you require one-use gloves to be used? (Yes or No)\\
\item
  Do you designate a person in charge of money transactions? (Yes or No)
\end{itemize}

The researcher also planned to do an assessment of the cleanliness of each vendor's station at different times. Her team would pick 30 days during the summer in which they'd walk through the vendor stations and collect the following information:

\begin{itemize}
\tightlist
\item
  Overall is the station clean (Yes or No)\\
\item
  Is anyone smoking around the food products? (Yes or No)\\
\item
  Are tables covered? (Yes or No) If so, what is the material?\\
\item
  Do employees appear to be clean? (Yes or No)\\
\item
  Are one-use gloves used? (Yes or No)\\
\item
  Is there a designated person in charge of money transactions (Yes or No)
\end{itemize}

She noted that there is a yearly cycle to the products sold and decided to collect vendors sales information by looking at the (AMT) amount sold in the last year (in dollars), the (PURCHASE) total number of purchases made in the last year, and the (NUM\_ITEMS) total number of items sold in the last year. For the last variable, they had to decide how to measure the number of items sold for the different types of crops. For most of the crops looking at the total weight (in lbs) sold made sense. But, for some, other measures were needed. For example, for sweet corn the number of ears sold would be recorded.

You can see that there are many decisions that the researcher must make in simply deciding the response variables to collect! A poor choice here can make or break a study.

\hypertarget{determine-sources-of-variation}{%
\subsection{Determine Sources of Variation}\label{determine-sources-of-variation}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/determineVariation}
The response variables clearly have some relationship to other variables that could be collected. For instance, the NUM\_ITEMS variable is clearly going to be different based upon what crops the vendor sells. The AMT variable would differ depending on the size of the vendor's inventory. These are examples of \textbf{explanatory variables} or variables that define the study conditions. Explanatory variables go by many names such as predictors, features, or independent variables.

\textbf{A main consideration about whether or not to record a variable is whether or not the variable would be related to a variation in a response variable.} Since the response variables are truly what is of interest, there is really not much of a point in recording variables that likely have no relationship with it.

Choosing the explanatory variables can also indicate further questions of interest. For instance, the researcher may want to compare the percent of ``Yes'' for the overall cleanliness score for vendors that mainly sell vegetables to those that mainly sell fruit leading to a comparison across groups being of interest. She may want to try to model the AMT of cantaloupe sold as a function of the cleanliness score.

The average amount for the population or a subpopulation would be referred to as a parameter of interest. Formally, a \textbf{parameter} is a summary measure about a population. Common parameters investigated include a mean, proportion, median, or variance of different subgroups of the population.

The explanatory variables she collected about the vendors included the types of crops sold, the services they provide (grow, pack, and/or ship), and whether or not they are a ``Got to be NC member''.

For the questionnaire, she added the additional questions below:

\begin{itemize}
\tightlist
\item
  Are there any organic or synthetic chemicals/fertilizers/pesticides/manures used on the products?\\
\item
  Are all foods grown/processed by the vendors?\\
\item
  What kind of soil were the products grown in? eg: organic/compost/plant material
\end{itemize}

For the assessment of cleanliness, she added the following question:

\begin{itemize}
\tightlist
\item
  How many people are working?
\end{itemize}

\hypertarget{select-design}{%
\subsection{Select Design}\label{select-design}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectDesign}
For this study the researchers aren't interested in doing an intervention so an observational study was being done. The major task to consider for the observational study is how to select participants from the population. The subset of the population we (attempt to) observe our data on is called the \textbf{sample}. The \textbf{sample size} is the number of measurements in the sample.

Ideally, we would measure every member of our population. This is called a \textbf{census}. If a census can be done then the value of a population's parameter can be found exactly by simply summarizing the population data. However, conducting a census can be extremely costly or time-intensive so most of the time a census cannot be done. This means that the information we collect would likely be different if we collected it again. Accounting for this variability is the main reason statistical analysis is needed.

How the researcher selects their sample is extremely important. This method is often referred to as the \textbf{sampling scheme}. Using a statistically valid sampling scheme is vital to the assumptions made when doing statistical inference. A valid sampling scheme implies that every member of the population has a known and non-zero chance of inclusion in the sample.

There are many good ways to select the sample and many bad ways. \textbf{Need to get more info about the farmer's market to finish this part}
(Talk about bad first and why bad - visuals too) Talk about good and why good - visuals too.

This idea is further fleshed out at the end of the chapter. (reference/link this)

Here they chose to do a stratified sample to make sure that they didn't leave out any important subgroups.

\hypertarget{do-study}{%
\subsection{Do Study}\label{do-study}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/conductStudy}
Go and talk to chosen vendors. May have some non-response issues. Ideally a contingency for this should be developed when considering the sampling scheme.

\hypertarget{do-statistical-analysis}{%
\subsection{Do Statistical Analysis}\label{do-statistical-analysis}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/statsAnalysis}
The major goals of this study were simply to describe the vendors at the farmer's market. In this case we can produce numerical and graphical summaries.

Careful discussion of not selecting a modeling technique based on this unless it is a pilot study or an exploratory study else we have increased our nominal type I error rate\ldots{}

Spend a lot of time here talking about graphs of different types. Sample means, sample variances, etc.

Discuss population curves vs sample histograms and the relationship.

Not a formal test here but comparisons of interest etc.

\hypertarget{draw-conclusions-communicate}{%
\subsection{Draw Conclusions \& Communicate}\label{draw-conclusions-communicate}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/concludeCommunicate}
What actionable things have we found? Likely some trends to investigate further. Perhaps run an experiment to formally see if some alteration can be effective.

What can we conclude realistically from this data? To what population are we talking?

\hypertarget{motivating-example-design---student-assessment-volunteers}{%
\section{Motivating Example: Design - Student Assessment Volunteers}\label{motivating-example-design---student-assessment-volunteers}}

\hypertarget{define-objective-background-1}{%
\subsection{Define Objective \& Background}\label{define-objective-background-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/defineObjective}
The Division of Academic and Student Affairs (DASA) is charged with assessing NC State's general education competency (or Pack Proficiencies) program. This is a set of competencies that every NC State graduate should master by the time they graduate. The five Pack Proficiencies are Critical Thinking, Creative Thinking, Oral Communication, Quantitative Literacy, and Written Communication.

There competencies go hand-in-hand with the general education program, but should be reinforced throughout each major curriculum.

\begin{center}\includegraphics[width=0.7\linewidth]{img/GEP-and-Gen-Ed-Diagram} \end{center}

DASA assesses the quantitative literacy proficiency through a number of instruments including a standardized test that is customized for NC State. Students are assessed when they are freshman and again when they are seniors. However, it is much easier to convince freshman to take this extra (mostly voluntary) exam than it is to convince a graduating senior. As such, DASA wanted to design an experiment around an email recruitment campaign to determine if there was a type of email that was most effective in recruiting students.

Discuss population as the conceptual population of all seniors that they would email about this.

Discuss email types?

\hypertarget{select-response-1}{%
\subsection{Select Response}\label{select-response-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectResponse}
Response is the proportion of students that respond positively to the email types.

\hypertarget{determine-sources-of-variation-1}{%
\subsection{Determine Sources of Variation}\label{determine-sources-of-variation-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/determineVariation}
When considering groups to send email to, DASA decided to break students up by race, \ldots{} something else I need to look up.

\hypertarget{select-design-1}{%
\subsection{Select Design}\label{select-design-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectDesign}
The design used was a randomized block design. Within each student group, the three email types were randomized.

\hypertarget{do-study-1}{%
\subsection{Do Study}\label{do-study-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/conductStudy}
Emails and reminder emails sent out over a period of two months. Data collected.

\hypertarget{do-statistical-analysis-1}{%
\subsection{Do Statistical Analysis}\label{do-statistical-analysis-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/statsAnalysis}
Data were analyzed to see if one email type performed better than another.

\hypertarget{draw-conclusions-communicate-1}{%
\subsection{Draw Conclusions \& Communicate}\label{draw-conclusions-communicate-1}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/concludeCommunicate}
The results of the study indicated\ldots{} This study was used to indicate how future email campaigns might run\ldots{}

\hypertarget{statistical-ideas-and-concepts}{%
\section{Statistical Ideas and Concepts}\label{statistical-ideas-and-concepts}}

When conducting a study, it is vital to identify the population and questions of interest. As mentioned previously, the \textbf{population} is the entire group of units of interest. Once defined, the questions of interest must be determined. Researchers often want to ask things like:

\begin{itemize}
\tightlist
\item
  Is there a difference in the response between group A and group B?\\
\item
  What should we expect to see if we change variable C?
\end{itemize}

However, these questions are not clearly defined!

What does it mean to have a difference in the response? Are you interested in a difference in the \emph{average} response for members of the poulation in group A vs those in group B? The \emph{median} response, the smallest or largest response value response, or the shape of the \emph{distribution} of values for members of the poulation in group A vs those in group B? The same detail is required in order to know what to expect with a change. Do you care about the average effect over the entire poulation by changing variable C? Or are you interestd in the change in the spread in expected outcomes in the population when the change is made?

Usually, these questions of interested need to be stated in terms of \textbf{parameters} of interest. A \textbf{parameter} is a measure about the population. These are values that, if we could do a \textbf{census} and measure the entire population, we could calculate. However, a census is not usually feasible due to cost or time that it would take to implement. This implies that the questions of interest that are centered around these parameter values cannot be known! Instead, we take a \textbf{sample} or subset of the population that we collect our data on. With the sample we calculate (sample) \textbf{statistics} or measures about the sample that correspond or \textbf{estimate} the parameter.

As you may expect, a good statistical measure will `converge' in some sense to the parameter as the \textbf{sample size}, or number of members in the sample, increases. We'll discuss how to create an \textbf{estimator} (statistics) and properties of good estimators and in later chapters. Often the key item that we need to know about an estimator is its (sampling) \textbf{distribution} or pattern in which it is observed. This allows us to know the values we should see for that statistic (at least under certain assumptions). With an understanding of the predictabilty of an estimator we can then make statements or claims about the population's parameters with some idea of variability attached! This idea and process of using sample data to discuss population values is the idea of \textbf{statistical inference}.

Hopefully, it is somewhat clear that the quality of our sample is vital to the inferences we can make!

Suppose we want to make inference about how well a drug works for treating acne in all people. If our sample consists mainly of people in their 20's, assumptions and domain-specific knowledge would need to be leveraged in order to extend the results from the study to the entire population of interest. For example, it would need to be argued that people in their 20's and teenagers, who are likely still going through puberty (among other differences), would have teh same reaction to the drug. This issue can be avoided if a \textbf{representative} sample is taken. The next section takes up the issue of obtaining a good sample from the population of interest.

\hypertarget{obtaining-a-representative-sample}{%
\subsection{Obtaining a Representative Sample}\label{obtaining-a-representative-sample}}

The method used to obtain the units for a sample is called the \textbf{sampling scheme}. There are good and bad sampling schemes. The main characteristic of a good sampling scheme is the use of a \textbf{probability sample}. A probability sample is one where every member of the population has a known and non-zero chance of inclusion in the sample. We'll study probability in more detail later, but a population member having a non-zero probability of inclusion simply implies, if the sampling scheme was repeated again and again, eventually every unit would be included in at least one of the samples collected. Having inclusion probabilities known for all members of the population allows for the quantification of error associated with sample estimates.

A probability sample requires some \textbf{random} mechanism in order to execute the selection of population members into the sample. However, there is a big difference between \emph{statistical} randomness and the everyday use of randomness. \textbf{Randomness} in everyday use usually refers to the idea that things are not known before hand. For instance, if a sample was collected by measuring the next 15 people that passed by us on a busy street, this is random but not random in a sense that helps statistically. Even though the members aren't known prior to collecting the sample, the method used to collect systematically excludes people that are otherwise not able to be walking down the street at the time the sample was taken. In statistics, having randomness usually involves a \textbf{random number generator} or a software that uses one (methods also include using a random digit table or flipping a coin, but these are mostly obsolete at this point!).

(\textbf{Mention that this is likely more important when dealing with humans?? But still important in many contexts})

Let's discuss sampling techniques that utilize a probability sample (and are often executed using a random number generator).

\hypertarget{simple-random-sample}{%
\subsubsection{Simple Random Sample}\label{simple-random-sample}}

The most simplistic sampling scheme is the \textbf{simple random sample} or \textbf{SRS}, which is outlined below.

If the population size is \(N\) and the sample size is \(n\),

\begin{itemize}
\tightlist
\item
  Assign members of the population numbers from 1 to \(N\)\\
\item
  Use a random number mechanism to select \(n\) of the \(N\) numbers\\
\item
  The sample is then the population members that correspond to the selected numbers
\end{itemize}

In a SRS, every subset of size \(n\) from the population has the same chance or probability of being included in the sample. (An SRS also implies that every member of the population has probabilty \(n/N\) of being included in the sample. Again, we'll cover probabiltiy in more depth later.) Since every population member has a known and non-zero chance of inclusion in the sample, this implies an SRS is a probability sample!

An everyday example of a process similar to a SRS is the lottery. Many states have a pick three lottery where the population of balls consist of balls numbering zero, one, two, three, \ldots{}, nine. The population size is \(N=9\). Three balls are then randomly selected (\(n=3\)) as the winning combination. Prior to the draw, players select three numbers and can win if they have the same three numbers as those selected. Here any combination of three numbers has the same probability of being selected (\(1/\binom{10,3} = 1/120\)).

Note that an SRS does not guarantee a representative sample every time!

For instance, suppose a university wants to determine the proportion of students that would favor a change to the bookstore. Every student is assigned a campus ID number. Due to time and cost constraints, the university decides to collect data on 400 students. An SRS can be used here to select the 400 students. The sample collected may, by random chance, contain only students from a particular college. The larger the sample, the less likely this is. An SRS implies that on average we should get a representative sample. Other techniques can increase the chance of a particular sample being representative of the population (at the expense of being more complicated and sometimes needing to be analyzed in a more complicated manner).

\hypertarget{stratified-random-sample}{%
\subsubsection{Stratified Random Sample}\label{stratified-random-sample}}

A \textbf{stratified random sample} is used when there are important subgroups that the researcher wants to ensure are included in a sample. These subgroups are called \textbf{strata}. Once the strata are determined, an SRS is often done within each strata.

For instance, again suppose a university wants to determine the proportion of students that would favor a change to the bookstore. The officials might want to make sure that members of each college are selected into the sample since the use of the bookstore can vary substantially from major-to-major and college-to-college. This time they create 5 strata, each corresponding to a college at the university. Within each strata they conduct an SRS, selecting 80 people from each college. This gives a total sample size of 400.

Note that the proportion of members selected doesn't need to be constant across the strata. The size of the sample done in each strata might be done proportionally to the total number of students that college constitutes.

For simplicity, assume the total population size was 10000. If there the five colleges had student population sizes 3000, 2500, 2000, 1500, and 1000, the proportion of students in each college would be \(3000/10000 = 0.3\), \(2500/10000 = 0.25\), \(2000/10000=0.2\), \(1500/10000=0.15\), and \(1000/10000=0.1\), respectively.

Still looking to obtain a total sample of size 400, we could select \(0.3*400 = 120\), \(0.25*400 = 100\), \(0.2*400 = 80\), \(0.15*400 = 60\), and \(0.1*400 = 40\) students from the each college, respectively.

Sometimes a larger proportion is taken from a strata because that strata is of greater interest to the researcher.

\textbf{(What other advantages? I think there can be some efficiency in doing this type of study but I don't know off the top of my head and I lack my books)}

\hypertarget{some-bad-sampling-schemes}{%
\subsubsection{Some Bad Sampling Schemes}\label{some-bad-sampling-schemes}}

There are many ``bad'' types of sampling schemes used in practice. Two examples are a \textbf{convenience (or haphazard) sample} and a \textbf{volunteer response sample}.\\
A convenience sample usually implies that the sample consists of the most convenient group available or the sample members are decided on the spot. For example, again suppose a university wants to determine the proportion of students that would favor a change to the bookstore. If the researcher simply went outside in between class periods and found 400 students that were walking by, this would be a convenience sample. Again, the members of the sample are random in the everyday sense, but not in the statistical sense. Perhaps the researcher was outside on a part of campus that mostly has classes for students in biology and chemistry. The students here are not necessarily representative of the entire college and, certainly, there is no known probability of inclusion associated with every student.

A volunteer response sample is one where particpants self-select. The members of a volunteer response sample tend to be those with a strong opinion, both positive and negative. This type of sample is often what is done for end of semester or end of course evaluations. The evaluations are not mandatory so, while many in the class will complete the evaluation, those with very strong opinions (both positive and neative) tend to be more likely to respond.

Neither of these is a probability sample, nor do they implement a random number mechanism! They should be avoided whenever possible. If they are implemented, most statistical analysis methods won't really be applicable and the best thing that can be done is a simple summary of the data collected (see chapter 3).

\hypertarget{why-sampling-scheme-matters---a-simulation}{%
\subsubsection{Why Sampling Scheme Matters - A Simulation}\label{why-sampling-scheme-matters---a-simulation}}

The importance of having a probability sample is paramount. To underscore the point, let's consider what differences and issues we might see using a \textbf{simulation study}. A simulation study usually implies that data are generated under certain assumptions using a computer and results are found on the simulated data. Since this process is done on a computer, the process can be repeated many times and the variabilty that is inherent can be accounted for in any comparisons of resutls. Of course, the data generating process must be valid or at least reasonable in order for a simulation study to yield any useful results.

Let's conduct a simple simulation study to compare the results found when doing an SRS vs those done with a contrived convenience sample.

Suppose our population consists of 100 marbles. We have interest in investigating the proportion of marbles that are blue. Since we are creating the population we will set this \textbf{parameter} value, making it a known quantity! Let's make the proportion of blue marbles 0.4 (40 blue marbles).

A sample of size 10 will be collected using the two methods outlined below:

\begin{itemize}
\tightlist
\item
  Method 1: An SRS - marbles are assigned numbers 1, 2, 3, \ldots{}, 100 at random. This implies the 40 blue marbles are randomly allocated across the numbers. Ten marbles are selected using a random number generator.\\
\item
  Method 2: A `convenience' sample is done. We'll use the same labeling of the population as above. However, units 1, 2, \ldots{}, 50 will have a higher probability of inclusion than units 51, 52, \ldots{}, 75, and units 75, 76, \ldots{}, 100 will have zero probability of being included in the sample.
\end{itemize}

Let's produce a sample using each method:

\begin{verbatim}
##    SRS.Pop. Color Conv.Pop. Color.1
## 1        30  Blue        20     Red
## 2        28   Red        15    Blue
## 3        31  Blue        14     Red
## 4        53   Red         4     Red
## 5        33   Red        69    Blue
## 6        79   Red        43     Red
## 7        94   Red         7    Blue
## 8        55   Red         6     Red
## 9        24   Red        34    Blue
## 10       54  Blue         2     Red
\end{verbatim}

The respective sample proportions of blue marbles found for these two samples are 0.3 for the SRS and 0.4 for the convenience sample.

Now let's repeat this process! Each time we'll record the sample proportion of blue marbles for each method. In the end we can compare the two and see the differences. Ten more of these (sample) proportions are reported below.

\begin{verbatim}
##    SRS Convenience
## 1  0.3         0.2
## 2  0.5         0.4
## 3  0.5         0.3
## 4  0.2         0.5
## 5  0.2         0.2
## 6  0.4         0.3
## 7  0.5         0.7
## 8  0.4         0.5
## 9  0.3         0.4
## 10 0.3         0.4
\end{verbatim}

Remember, the actual value of the population proportion or paramter is 0.4. It can be hard to see differences between these two methods in only a few samples. With computing power at our fingertips, let's repeat this process 10,000 times and investigate how the two sampling methods performed with regard to the true proportion of blue marbles, 0.4. The easiest way to visualize the sample proportions is by creating histograms. In this case, the histogram will show us how many times we observed each sample proportion possible (0, 0.1, 0.2, \ldots{}, 1).

\begin{center}\includegraphics[width=0.9\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-223-1} \end{center}

We can see there are differences in the two histograms. The histogram using the convenience sample tended to have slightly larger values for the sample proportion of blue marbles found. If we look at the overall mean of the sample proportions found using the SRS we get 0.4015, which is very close to the true value of 0.4 This means that on average the SRS method and the sample proportion are giving the correct parameter. Copare this to the convenience sample method where we get an overall mean of 0.454, which overestimates the population parameter of 0.4.

This is just a quick example of how using a poor sampling method might affect the analysis done and why we need to use a good sampling method. We `created' a convenience sample here. In real life, the mechanism that underlies the convenience sample wouldn't be known!

\hypertarget{practical-sampling-notes}{%
\subsubsection{Practical Sampling Notes}\label{practical-sampling-notes}}

There are many other good sampling schemes such as cluster sampling, sytematic sampling, and heirarchical sampling that can use combinations of techniques. Some of these will be covered in the book as the come up but references are given for those interested. There are entire books written on proper sampling methods and the myriad of issues that can come up, especially when dealing with humans. In particular, when doing survey sampling where a list of questions are given, some issues that come up often (but occur in other places too) are:

\begin{itemize}
\tightlist
\item
  bias in question writing - leading questions and double barreled questions to name two\\
\item
  nonresponse bias - sample member doesn't answer or respond\\
\item
  response bias - sample member answers a question wrong on purpose (often due to how they'll be percieved
\end{itemize}

Other common issues that arise are when the list of population units called the \textbf{sampling frame} doesn't match the actual population of interest. This is an issue called \textbf{undercoverage}. One famous example that relates to this idea comes from the 1936 US presidential election. \emph{Literary Digest} incorrectly predicted that Landon would overwhelmingly defeat Roosevelt. The prediction was based on survey results done by \emph{Literary Digest}, however they had an issue akin to undercoverage. They mailed questionnaires out only to people who had both telephones and cars. This left out a large portion of the population and so the results were based on a sample that was not representative of the popoulation.

One thing to note is that the sampling method can play a role in the analysis or modeling technique used but can sometimes be ignored (assuming a probability sample was done). When the sample size is small compared to the population size (for a SRS, one rule of thumb is \(n/N < 0.05\)), we often are able to make an assumption of a ``random sample.'' This assumption then allows for the standard modeling techniques and inference methods to be applied. We will point out cases in book where the exact type of sampling method plays an important role.

Sometimes it isn't feasible to implement a good sampling scheme. For instance, for medical studies volunteers are often used and for a crop experiment the fields in which crops are planted aren't usually something you can choose. In this case, collecting more explanatory variables is vital. Additional out of data assumptions are usually needed when attemtpting to make inference to entire population.

\hypertarget{observational-vs-experimental-studies-1}{%
\subsubsection{Observational vs Experimental Studies}\label{observational-vs-experimental-studies-1}}

Sampling schemes are all about obtaining units from your population for your study. Once units are selected there are two basic types of studies:

\begin{itemize}
\item
  \textbf{Observational Study} - observe individuals and measure outcomes without influencing the responses
\item
  \textbf{Experimental Study} - deliberately impose a treatment on individuals and observe their response
\end{itemize}

Recall the big difference in conclusions drawn!

\begin{itemize}
\item
  Cannot usually infer causation from observational studies, but you can from a well-designed experiment
\item
  Experiments are not always feasible or ethical. i.e.~cannot assign people to smoke a pack a day or have expectant mothers drink a certain amount of alcohol
\end{itemize}

For experimental studies, a randomization technique is needed to determine which EUs obtain which units. Sometimes this can go hand in hand with the sampling scheme (stratified random sampling and a randomized block design). How to do this important and nuanced. The experimental design is the topic of the next section.

\hypertarget{fundamentals-of-designed-experiments}{%
\subsection{Fundamentals of Designed Experiments}\label{fundamentals-of-designed-experiments}}

To describe the methods for creating a well-designed experiment, we first need to recap some definitions from earlier:

\begin{itemize}
\tightlist
\item
  \textbf{Response Variable} - variable(s) of interest that characterizes performance or behavior\\
\item
  \textbf{Explanatory Variable} - variable(s) of interest with regard to their relationship with the response variable\\
\item
  \textbf{Covariate} - Quantitative (numerical) explanatory variable (usually observed as the experiment is run and not set by the researcher)\\
\item
  \textbf{Factor} - explanatory variable that takes on a finite number of values (a Categorical or Qualitative variable)
\item
  \textbf{Level} - setting a factor can take on
\item
  \textbf{Treatment} - specific experimental condition, either the level of a factor (if only 1 factor) or the combinations of the levels from a number of factors

  \begin{itemize}
  \tightlist
  \item
    \textbf{Control Treatment} - benchmark treatment sometimes necessary for comparison (to avoid the placebo effect)\\
  \end{itemize}
\item
  \textbf{Experimental units (EUs)} - units on which the treatments are assigned\\
\item
  \textbf{Observational units (OUs)} - units on which measurements are taken
\end{itemize}

There is clearly a lot of jargon to remember when learning about experimental design. We'll do our best to remind the reader of these terms as we go along.

Before we dive into experimental designs, we should again discuss the reason we need statistical methods with a little more detail.

\hypertarget{sources-of-variation}{%
\subsubsection{Sources of Variation}\label{sources-of-variation}}

We've discussed that data is variable. If we repeat an experimental study, even with identical conditions, we are likely to obtain different data for the response. This has to do with the sample members being selected differently each time and possibly different units assigned a given treatment. Let's identify the sources variation in our response.

\begin{itemize}
\item
  \textbf{Treatment effect} - an effect due to the variables assigned by the researcher (treatments) in an experiment.

  \begin{itemize}
  \tightlist
  \item
    This is an effect we are usually hoping to see and quantify!
  \end{itemize}
\item
  (Other) \textbf{Recorded Variables} - some variables that are not of interest are recorded because we may know or think they are associated with variability in the response.

  \begin{itemize}
  \tightlist
  \item
    These may be covariates measured along the way (like temperature) or a variable with only a few values like the soil type a crop is grown in.
  \end{itemize}
\item
  \textbf{Unaccounted for Variables} - everything else causing variation.

  \begin{itemize}
  \tightlist
  \item
    This variation is estimated and used as a reference with which to compare the treatment variation.
  \end{itemize}
\end{itemize}

Consider a simplified example where a gardener wants to know what water (low or high) and fertilizer (A or B) conditions are better in terms of producing greater crop yield (dried weight).

\begin{itemize}
\tightlist
\item
  Response variable - the dried weight of the crop after growth
\item
  The explanatory variables are the two factors:

  \begin{itemize}
  \tightlist
  \item
    Water (with levels low and high)\\
  \item
    Fertilizer (with levels A or B)\\
  \end{itemize}
\item
  Treatments:

  \begin{itemize}
  \tightlist
  \item
    low water and fertilizer A\\
  \item
    low water and fertilizer B\\
  \item
    high water and fertilizer A\\
  \item
    high water and fertilizer B
  \end{itemize}
\end{itemize}

The gardener has two greenhouses each with 16 rows for growing crops. Within each greenhouse, they randomly assign the four treatments to the 16 rows. After 45 days, the crops are harvested, dried, and weighed.

\begin{itemize}
\tightlist
\item
  The experimental units are the rows within the greenhouses\\
\item
  The observational units are all the plants grown in a given row (so we may say the row is also the OU)
\end{itemize}

The sources of variation in the dried weight of the crop are:

\begin{itemize}
\tightlist
\item
  Treatment variation - variation we believe we'll see in dried weight from the differences in water and fertilizer applications.\\
\item
  Other recorded variable - greenhouse in which the crops are grown is recorded. Greenhouse is not of interest but may play a role in the variation in dried weight.
\item
  Unaccounted for variables - Amount of sunlight received, temperature/humidity differentials within a greenhouse, possible differences in the method of application of the treatment (fertilizer and water), and many others may also have an association with dried crop weight. These sources make up a sort of reference variability we can compare our treatment variation to.
\end{itemize}

The variation from unaccounted variables can generally be broken down into four cateogries:

\begin{itemize}
\tightlist
\item
  Inherent variability in EUs (units assigned a treatment).

  \begin{itemize}
  \tightlist
  \item
    No two people, paper towels, concrete blocks, or lab rats are exactly the same so they may respond differently to the same treatment.
  \end{itemize}
\item
  Measurement error - Multiple measurements of a same experimental unit may differ.

  \begin{itemize}
  \tightlist
  \item
    Two blood pressure readings a few minutes apart may give different readigns or if you break a water sample in two and measure each for bacteria, you may see different measurements
  \end{itemize}
\item
  Variations in applying or creating treatments.

  \begin{itemize}
  \tightlist
  \item
    Occasionaly a treatment protocol is not clearly defined, leaving room for interpretation. Perhaps applying a fertilizer before or after applying irrigation can cause a difference.
  \end{itemize}
\item
  Other unknown variables sometimes called lurking variables.
\end{itemize}

No matter how hard we try, some of these unaccounted for variables (and hence variation in the response) will remain. What we can do is use good experimental design techniques to try and minimize the effects of from these variables.

Good experimental designs generally have the following key attributes:

\begin{itemize}
\tightlist
\item
  \textbf{Randomization} - treatments are randomly assigned to experimental units.
\end{itemize}

This process must use a random number mechanism or software to allocate the treatments. Randomization makes sure that every EU has a chance to get a different treatment. This helps to protect the results of the analysis against a systematic influence of lurking variables. For example, if a doctor is assigning drug A or B to a patient without use of a random number mechanism, they may unwittingly assign drug A to patients they deem more likely to recover due to implicit biases.

\begin{itemize}
\tightlist
\item
  \textbf{Replication} - multiple (independent) experimental units are assigned the same treatment.
\end{itemize}

EUs that receive the same treatment are called \textbf{replicates}. By having replication we are able to create an estimate of variability due to our unaccounted for variables. Comparing our treatment variation to this variation is what allows us to have faith in the reliability of our conclusions.

Note that replication does not mean that we measure the same EUs multiple times! That is called a repeated measures design. Observations from repeated measures experiments cannot usually be considered independent.

\begin{itemize}
\tightlist
\item
  \textbf{Controlling Variables} - holding certain variables constant across the EUs.\\
  Generally, we're not interested in the effects of these variables on the response. These variables affect the response in exactly the same manner, so that we don't see the effects on the conclusions. Unfortunately, we don`t get information on what happens at settings of the variables other than the fixed ones. This decreases generalizability, but reduces overall variation. Experimental designs such as a randomized complete block design attempt to control variables while also maintaining generalizability.
\end{itemize}

Let's discuss some of the most used experimental designs. We'll cover analysis for these types of experiments in future chapters. When considering conducting an experiment, consider the advice below!

\begin{quote}
A poorly designed study can never be saved, but a poorly analyzed one has the possibility of being reanalyzed.
\end{quote}

\hypertarget{completely-randomized-design-crd}{%
\subsubsection{Completely Randomized Design (CRD)}\label{completely-randomized-design-crd}}

A Completely Randomized Design (CRD) is the most straightforward experimental design. Suppose the number of units in the sample is \(n\) and the number of treatments is \(t\). The design uses a random number mechanism to randomly assign the \(t\) treatments across the \(n\) experimental units. In a \textbf{balanced} design, the same number of units are assigned to each treatment. The number of replicates is usually denoted by \(n_t\).

This design assumes all of the EUs are \textbf{exchangeable}. (\textbf{Should I include this here?})

For example, suppose we are doing an experiment to determine the effect of nutrition (3 different diets or treatments) on weight gain in humans. Suppose we have 30 experimental units labeled 1, 2, \ldots{}, 30. A random number generator can be used to reorder the numbers 1 through 30. The first 10 numbers can then be assigned the first diet, the next 20 numbers the second diet, and the last 10 numbers the third diet.

Ideally we want to have as many replicates for each treatment as we can afford. If we had 3 diets and 3 EUs and found that the person assigned the first diet lost more weight than the person assigned to the second diet. This is not a very reliable conclusion! Perhaps the person assigned to the first diet naturally loses weight more easily. However, if we had 100 people assigned to each diet and on average the first diet yieled a greater weight loss, this would be a much more reliable conclusion.

Often to determine the appropriate sample size (or sample sizes for each treatment group) a power analysis or sample size calculation is done. These require assumptions about the population variation in some respect as well as an idea of a ``difference of interest.'' These topics will be covered later in the book.

\hypertarget{randomized-block-designs}{%
\subsubsection{Randomized Block Designs}\label{randomized-block-designs}}

A randomized block design divides EUs with similar characteristics into
`blocks' or subgroups. Within each block the treatments are then randomly assigned.

Within a block we are essentially controlling the blocking variable. However, by conducting the design across many settings of the blocking variable, we maintain generalizability of the study.

Let's consider an example. Two new finishes are developed (type A and type B) for use on a dash board in a car. The material and finish must withstand high temperatures due to the sun and the greenhouse effect. To simulate the temperatures and wear on the dashboard, the dashboards are placed into ovens for 24 hours and the amount of degradation is measured.

The company has 4 large ovens (oven 1, 2, 3, and 4) for testing. The manufacturer finds a random sample of 40 dashboards from the assembly line. Ten dashboards will be placed in each oven. Within the groups of ten assigned to the ovens, five of each treatment were randomly assigned.

The ovens act as the blocks in this experiment. There may be some variation from oven to oven but we don't really have an interest in this variation.

There are many other commonly used design including:

\begin{itemize}
\tightlist
\item
  Incomplete block designs
\item
  Latin squares designs\\
\item
  Split plot designs\\
\item
  Randomized complete block split plot designs\\
\item
  Strip plot designs\\
\item
  2\^{}k designs\\
\item
  3\^{}k designs\\
\item
  Response surface designs
\end{itemize}

We'll cover many of these later in the book!

\hypertarget{sampling-schemes-vs-randomization-methods}{%
\subsubsection{Sampling Schemes vs Randomization Methods}\label{sampling-schemes-vs-randomization-methods}}

Randomization applied to experimental design is different than randomization in sampling!

\begin{itemize}
\item
  In sampling, randomization is used to determine which members of the population are included in the sample.
\item
  If an experiment is being done, randomization is then used to determine which EUs get which treatments.
\end{itemize}

\textbf{(Comparison of CRD, SRS and Strat, Block)}

\hypertarget{references-and-readings}{%
\section{References and Readings}\label{references-and-readings}}

Something about causal inference\\
Sampling stuff
DOE stuff
Corr vs caus

\hypertarget{software-1}{%
\section{Software}\label{software-1}}

\hypertarget{r-1}{%
\subsection{R}\label{r-1}}

\hypertarget{sas-1}{%
\subsection{SAS}\label{sas-1}}

\hypertarget{summarizing-data}{%
\chapter{Summarizing Data}\label{summarizing-data}}

Recall a major goal of conducting a study is often to conduct statistical inference. Inference can involve determining which variables are important in relation to a response variable and/or predicting a response variable under different scenarios. To formally do inference we need to define the population of interest as well as \textbf{parameters} we want to study.

\begin{itemize}
\item
  Population - all the values, items, or individuals of interest
\item
  Parameter - a summary measure about a population
\end{itemize}

Ideally, we'd be able to measure every member of the population and exactly calculate the value of any population parameter. This would involved conducing a census. A census is usually not feasible. Instead we take a subset of the population and try to use these observations to make statements or claims about the population.

\begin{itemize}
\item
  Sample - a subset of the population we observe data on
\item
  Statistic - a summary value calculated from the sample observations
\end{itemize}

Your questions of interest will often lead you to which parameters you have interest in. Careful consideration of study goals and the time and money you have will also most likely lead you to which type of data you will collect to best make inference on those parameters.

\hypertarget{motivating-example-summarizing-data---california-health-interview-survey}{%
\section{Motivating Example: Summarizing Data - California Health Interview Survey}\label{motivating-example-summarizing-data---california-health-interview-survey}}

\hypertarget{define-objective-background-2}{%
\subsection{Define Objective \& Background}\label{define-objective-background-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/defineObjective}
Look at the California Health Interview Survey (CHIS). Describe the idea of the survey. For our purpose we'll imagine the dataset itself is the population of interest. We will take a sample from that population to investigate and summarize as well as compare to the truth.

\hypertarget{select-response-2}{%
\subsection{Select Response}\label{select-response-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectResponse}
Response is really certain variables in the dataset, maybe BMI or some other health related statistic we want to study.

\hypertarget{determine-sources-of-variation-2}{%
\subsection{Determine Sources of Variation}\label{determine-sources-of-variation-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/determineVariation}
Sources of variation include height, weight, ethnicity, etc. Going to take those into account.

\hypertarget{select-design-2}{%
\subsection{Select Design}\label{select-design-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/selectDesign}
Design will be to do a SRS and do an observational study - basically an exploration study. (Maybe also do an Stratified one and compare the results? Maybe that should be left as an exercise.)

\hypertarget{do-study-2}{%
\subsection{Do Study}\label{do-study-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/conductStudy}
Pretend that we got 100\% response rate and all that good stuff. Maybe discuss the idea of missingness here but probably not the right time.

\hypertarget{do-statistical-analysis-2}{%
\subsection{Do Statistical Analysis}\label{do-statistical-analysis-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/statsAnalysis}
Main analysis is just to summarize the data set. As an exercise we will compare the sample to the population since we actually know it here.

\hypertarget{draw-conclusions-communicate-2}{%
\subsection{Draw Conclusions \& Communicate}\label{draw-conclusions-communicate-2}}

\includegraphics[width=0.13\linewidth,style="float:left; padding:10px"]{img/concludeCommunicate}
Describe main results or takeaways to study in the future. How to present some graphs as well (i.e.~don't just put a graph, it must be described and meaningful or don't report it). Appropriate use of titles and labels too.

\hypertarget{statistical-ideas-and-concepts-1}{%
\section{Statistical Ideas and Concepts}\label{statistical-ideas-and-concepts-1}}

\hypertarget{study-purpose}{%
\subsection{Study Purpose}\label{study-purpose}}

A major goal of a study is usually to conduct statistical inference. Inference can involve determine which variables are important in relation to a response variable and/or predicting a response variable. To formally do inference we need to define the population of interest as well as \textbf{parameters} we want to study.

\begin{itemize}
\item
  Population - all the values, items, or individuals of interest
\item
  Parameter - a (usually) unknown summary value about the population
\end{itemize}

Ideally, we'd be able to measure every member of the population and exactly calculate the value of any population parameter. This would involved conducing a census. A census is usually not feasible. Instead we take a subset of the population and try to use these observations to make statements or claims about the population.

\begin{itemize}
\item
  Sample - a subset of the population we observe data on
\item
  Statistic - a summary value calculated from the sample observations
\end{itemize}

(\textbf{A better visual than this})

\begin{center}\includegraphics[width=0.8\linewidth]{img/paradigm} \end{center}

Example - A political scientist surveys 400 people randomly from a list of all registered voters in a particular county. He asks the people if they plan to vote in the upcoming election and 312 say they do.

\textbf{Relate above to definitions\ldots{}}

To discuss paramters and statistics more easily, symbols are used to denote them. Luckily, there is a common notation that, for the most, is consistent across statistical literature.

Note: \(\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i\) and \(S^2=\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2\) where \(n\) is the sample size (or number of observed values in the sample). Sometimes you'll see other notation for proportions \textbf{(\ldots{}\ldots{}. fill in and change table)}

Question of interest will lead you to which parameter you have interest in. Careful consideration of what your study goals are will also most likely lead you to which type of data you will collect.

\hypertarget{scales-types-of-data}{%
\subsection{Scales (Types) of Data:}\label{scales-types-of-data}}

\textbf{Quantitative or Numerical variable} - A variable that is described by numerical measurements where arithmetic can be performed

Subscales:
- Discrete - finite or countable finite number of values (\# of flowers on a plant, 0, 1, 2, \ldots{})
- Continuous - any value in an interval is possible (Temperature, \((-459.67\deg F, \infty)\)

\textbf{Qualitative or Categorical variable} - A variable that is described by attributes or labels

Subscales:
- Nominal - categories have no ordering (Male, Female) (zip codes)
- Ordinal - can order categories (Lickert scale data) (college football rankings)

How we summarize and analyze the data will depend on which type of data we have!

Example: SAT Performance

50 total students (16 males and 34 females) where matched on socio-economic background (all had similar income).

\begin{itemize}
\tightlist
\item
  A study was done to examine the effect of preparation atmosphere on SAT scores.\\
\item
  Two types of atmospheres were investigated (strict vs easy going).
\item
  Students were divided into two groups of 25 (12 males and 13 females in strict class and 4 males 21 females in the easy going class).
\item
  After a 9 week tutoring session the SAT was taken (although 1 in the strict group did not take the exam and 5 in the easy going group did not take the exam).
\end{itemize}

\textbf{Questions to fill in:}
- Determine the research question.
- Define the population and sample.
- Define possible parameter(s) of interest.
- Describe the scale of measurement for gender, atmosphere, SAT score.
- Define possible statistics that might be calculated.
- Why might the students have been matched on socio-economic background?
- What issues might you see with the design of this study?
- What other variables might you collect (to explain more variability in the response)?

\textbf{Descriptive Statistics}

Goal of descriptive statistics is to describe the \textbf{distribution} of the data.

We often want to summarize the center and spread of the data.

Common numerical summaries

\begin{itemize}
\item
  Measures of Location

  \begin{itemize}
  \tightlist
  \item
    Sample Mean = \(\bar{Y}=\frac{\sum_{i=1}^{n}Y_i}{n}\)
  \item
    Sample Median = Middle value of the data set (50\% of values to left, 50\% of values to right)
  \item
    Sample Proportion = \(\hat{p}=\frac{\#\mbox{ of successes}}{\mbox{sample size}}\)
  \end{itemize}
\item
  Measures of Spread

  \begin{itemize}
  \tightlist
  \item
    Sample Variance = \(S^2 = \frac{\sum_{i=1}^{n}(Y_i-\bar{Y})^2}{n-1}\)
  \item
    Sample Standard Deviation = \(S\)
  \item
    Sample Range = \(max(Y_1,...,Y_n)-min(Y_1,...,Y_n)\)
  \item
    Inter-quartile Range = Q3-Q1
  \end{itemize}
\end{itemize}

Common graphical summaries

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}}

In this chapter we'll discuss summarizing data only, not populations. We'll calculate numerical summaries (statistics) and graphical summaries for the sample. In the next chapter we'll relate these summaries to the population parameters and distributions.

When summarizing data we want to describe the distribution of the data. This may be a \emph{marginal} or \emph{univariate} summary of a single variable by itself.

\begin{center}\includegraphics[width=0.8\linewidth]{img/summarizeAllF} \end{center}

However, quite often we want to look at the distribution of a variable conditional on another variable - perhaps levels of a certain factor or the treatments of an experiment - or the relationship of more than one variable together. These would be referred to as \textbf{multivariate} summaries.

\begin{center}\includegraphics[width=0.8\linewidth]{img/summarizeGroupsF} \end{center}

How we summarize data depends on the \textbf{variable types} (sometimes called \textbf{variable scales}) and the attribute or quantity we are trying to describe about that variable. The two major types of variables are:

\begin{itemize}
\tightlist
\item
  Categorical (Qualitative) variable - entries are a label or attribute\\
\item
  Numerical (Quantitative) variable - entries are a numerical value where math can be performed
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img/variableTypes} \end{center}

Both of these have \emph{subscales} that are sometimes important to consider.

Categorical variables can be \textbf{nominal} or \textbf{ordinal}. Nominal variables have no ordering to their categories. For example, a variable asking for your favorite pet. There is no inherent ordering to give pets. Ordinal variables have an ordering but differences between the categories are not necessarily the same. For example, Likert scale data having categories ``strongly disagree,'' ``disagree,'' ``neutral,'' ``agree'', and ``strongly agree.'' There is a clear ordering here but the difference between strongly agree and agree is not necessarily the same as the difference between agree and neutral.

Numerical variables can be \textbf{discrete} or \textbf{continuous}. Discrete variables take on values that can be listed out, although the list may continue on indefinitely. For example, the number of bedrooms in a house. The values (or support) for this variable are 0, 1, 2, 3, \ldots{} but there is not necessarily a known upper limit. Discrete variables don't need to take on just integers and make take on values that are irregularly spaced. A continuous variable is one in which the variable can take on any value in an interval (or union of intervals). For example, the time it takes to complete an online survey. The support for this variable would be the interval from 0 to some large number, often we'd just say infinity for the upper bound.

When summarizing the variables, the main goal is to summarize the \textbf{distribution} or pattern and frequency with which you observe a variable. This involves slightly different summaries dependong on variable type (or combination of variable types).

\begin{itemize}
\item
  Categorical variable - describe relative frequency (or count) in each category
\item
  Numerical variable - describe the shape, center, and spread of the distribution
\end{itemize}

The most common numerical summaries are given below:

\begin{itemize}
\item
  Cateogrical

  \begin{itemize}
  \tightlist
  \item
    Contingency Tables
  \end{itemize}
\item
  Numerical

  \begin{itemize}
  \tightlist
  \item
    Mean/Median\\
  \item
    Standard Deviation/Variance
  \item
    Coefficient of Variation
  \item
    Quantiles/Percentiles/IQR
  \end{itemize}
\end{itemize}

These are certainly not the only summaries you might calculate!

\textbf{This is terribe but I want to put something like it somewhere\ldots{}} If you have two or more categorical variables, contingency tables are still the summary to use. If you have one numerical and one or more categorical variable, you'll often calculate the numerical summaries for each combination of categorical variables levels. If you have multiple numerical variables you'll usually calculate the covariance or correlation, which measures the linear relationship between pairs of variables. This can also be done for different settings of categorical variables as well.

\hypertarget{contingency-tables-for-categorical-variables}{%
\subsubsection{Contingency Tables for Categorical Variables}\label{contingency-tables-for-categorical-variables}}

Let's start by summarizing a categorical variable (entries are a label or attribute) from a dataset on the titanic passengers. The dataset describes attributes of passengers on the titanic. The variables we'll investigate are

\begin{itemize}
\tightlist
\item
  embarked (where journey started)\\
\item
  survived (survive or not)\\
\item
  sex (Male or Female)
\end{itemize}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   pclass = col_double(),
##   survived = col_double(),
##   name = col_character(),
##   sex = col_character(),
##   age = col_double(),
##   sibsp = col_double(),
##   parch = col_double(),
##   ticket = col_character(),
##   fare = col_double(),
##   cabin = col_character(),
##   embarked = col_character(),
##   boat = col_character(),
##   body = col_double(),
##   home.dest = col_character()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 1,310 x 14
##   pclass survived name  sex      age sibsp parch ticket  fare cabin embarked
##    <dbl>    <dbl> <chr> <chr>  <dbl> <dbl> <dbl> <chr>  <dbl> <chr> <chr>   
## 1      1        1 Alle~ fema~ 29         0     0 24160   211. B5    S       
## 2      1        1 Alli~ male   0.917     1     2 113781  152. C22 ~ S       
## 3      1        0 Alli~ fema~  2         1     2 113781  152. C22 ~ S       
## 4      1        0 Alli~ male  30         1     2 113781  152. C22 ~ S       
## 5      1        0 Alli~ fema~ 25         1     2 113781  152. C22 ~ S       
## # ... with 1,305 more rows, and 3 more variables: boat <chr>, body <dbl>,
## #   home.dest <chr>
\end{verbatim}

A contingency table simply shows the frequency (sometimes proportion) of observations falling into the categories of the variable. If we are looking at one variable by itself, the table is called a \textbf{one-way contingency tables}.

One-way contingency table for the \texttt{embarked} variable.

\begin{verbatim}
## 
##   C   Q   S 
## 270 123 914
\end{verbatim}

One-way contingency table for the \texttt{survived} variable.

\begin{verbatim}
## 
##   0   1 
## 809 500
\end{verbatim}

One-way contingency table for the \texttt{sex} variable.

\begin{verbatim}
## 
## female   male 
##    466    843
\end{verbatim}

We can see that these one-way tables allow us to easily see how many values fall in each category. For example, we see that 809 people died and 500 survived.

A \textbf{two-way contingency table} is similar in that it gives the frequencies for combinations of two categorical variables.

Two-way table between \texttt{survived} and \texttt{sex}.

\begin{verbatim}
##    
##     female male
##   0    127  682
##   1    339  161
\end{verbatim}

Two-way table between \texttt{survived} and \texttt{embarked}.

\begin{verbatim}
##    
##       C   Q   S
##   0 120  79 610
##   1 150  44 304
\end{verbatim}

Two-way table between \texttt{sex} and \texttt{embarked}.

\begin{verbatim}
##         
##            C   Q   S
##   female 113  60 291
##   male   157  63 623
\end{verbatim}

With this summary we can easily see the relationship between these pairs of categorical variables. For example, there were 127 females that died and 682 males that died.

This idea can be extended indefinitely and generally we can discuss an \textbf{n-way contingency table}. The major issue with going beyond two- or three-way tables is the difficulty in displaying the information in a easy to digest manner. For example, consider the three-way table between \texttt{sex}, \texttt{embarked}, and \texttt{survived} below.

\begin{verbatim}
## , ,  = 0
## 
##         
##            C   Q   S
##   female  11  23  93
##   male   109  56 517
## 
## , ,  = 1
## 
##         
##            C   Q   S
##   female 102  37 198
##   male    48   7 106
\end{verbatim}

We can now see two two-way tables displayed, one for \texttt{survived\ =\ 0} (died) and one for \texttt{survived\ =\ 1\ (lived)}. We would interpret values from the first table as follows: There were 11 females that embarked at the Cherbourg port that died, there were 23 females that embarked at the Queenstown port that died, and so on.

Sometimes it is useful to look at a one-way table conditional on settings of other variables as well. For instance, we could report a one-way table for the \texttt{survived} variable conditional on looking at males that embarked at Queenstown.

\begin{verbatim}
##  0  1 
## 56  7
\end{verbatim}

Again, contingency tables or n-way tables are the most common summary for combinations of categorical variables.

\hypertarget{measures-of-center-and-spread-for-numeric-variables}{%
\subsubsection{Measures of Center and Spread for Numeric Variables}\label{measures-of-center-and-spread-for-numeric-variables}}

Recall that a numerical variable is one whose entries are a numerical value where math can be performed. The major things we want to describe about a numerical variable's distribution are the shape, center, and spread. Shape is best left to graphical summaries like a histogram or density plot. We'll cover these shortly.

Let's consider a dataset about and experiment on carbon dioxide (CO2) uptake in grass. The three variables we'll investigate are:

\begin{itemize}
\tightlist
\item
  Response recorded: \texttt{uptake} CO2 uptake rates in grass plants\\
\item
  Environment manipulated: \texttt{Treatment} - chilled/nonchilled\\
\item
  Ambient CO2 specified and measured: \texttt{conc}
\end{itemize}

\texttt{uptake} is a numeric variable and will be the variable we want to summarize. \texttt{Treatment} is a categorical variable. \texttt{conc} is a numeric variable but only observed at a few values. This can be treated as either type of variable depending on what your goal is. We will treat \texttt{conc} as numeric.

\begin{verbatim}
## # A tibble: 84 x 5
##   Plant Type   Treatment   conc uptake
##   <ord> <fct>  <fct>      <dbl>  <dbl>
## 1 Qn1   Quebec nonchilled    95   16  
## 2 Qn1   Quebec nonchilled   175   30.4
## 3 Qn1   Quebec nonchilled   250   34.8
## 4 Qn1   Quebec nonchilled   350   37.2
## 5 Qn1   Quebec nonchilled   500   35.3
## # ... with 79 more rows
\end{verbatim}

\hypertarget{measuring-center}{%
\paragraph{Measuring Center}\label{measuring-center}}

The mean of the \texttt{uptake} variable can be calculated to help summarize the center of the \texttt{uptake} variable's distribution. For clarity, let's label the \texttt{uptake} variable with \(Y\). The observed values can be labeled as below:

\(y_1 = 16.0\)\\
\(y_2 = 30.4\)\\
\(y_3 = 34.8\)\\
\ldots{}\\
\(y_{84} = 19.9\)

The sample mean is then simply the sum of these values divided by the total number:

\[\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i = \frac{1}{84}\sum_{i=1}^{84}y_i = \frac{1}{84}\left(16.0 + 30.4 + 34.8 + ... + 19.9)\]

This comes out to be 27.2130952. This value represents one measure of the center or middle of the \texttt{uptake} variable's distribution.

As the actual data values are used in this calculation, one or two very large or small numbers can have a large influence on the value of the sample mean. To counter this, you can calculate a more robust measure called a \textbf{trimmed mean}. This involves removing the highest and lowest values and then calculating the mean with the remaining values. For instance, a 5\% trimmed mean drops the lowest and highest 5\% of data values and then finds the mean with the remaining values.\\
Here 0.05*84 = 4.2. This means we should drop off the lowest four and highest four values and then calculate the mean with the remaining 76 values. The 5\% trimmed mean comes out to be 27.2526316. This is another measure of the center of the \texttt{uptake} variable's distribution.

Lastly, another common measure of center is the median. The median involves sorting the data from largest to smallest and reporting the middle value (if there is an odd number of data points) or the average of the two middle values (if there is an even number of data points). You may notice that having very large or small values in the data set do not matter as much for calculation of the median. The largest value for \texttt{uptake} could be replaced by 10000 and the median wouldn't change. For this reason, the median is also referred to as a robust estimate of the center of the \texttt{uptake} variable's distribution. The value of the median here is 28.3.

\hypertarget{measuring-spread}{%
\paragraph{Measuring Spread}\label{measuring-spread}}

The most common measure of spread is the \textbf{standard deviation} or \textbf{variance}.

116.9515132
10.8144123

19.225
12.36, 15.64

Measures of linear relationship: Covariance, Correlation

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cov}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc, CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1552.687
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc, CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4851774
\end{verbatim}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\tightlist
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\end{itemize}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  \texttt{dplyr} easy to use but can only return one value
\end{itemize}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  \texttt{dplyr} easy to use but can only return one value
\end{itemize}

Idea:

\begin{itemize}
\item
  Use \texttt{group\_by} to create subgroups associated with the data frame
\item
  Use \texttt{summarize} to create basic summaries for each subgroup
\end{itemize}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\tightlist
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(Treatment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(uptake), }\DataTypeTok{med =} \KeywordTok{median}\NormalTok{(uptake), }\DataTypeTok{var =} \KeywordTok{var}\NormalTok{(uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   Treatment    avg   med   var
##   <fct>      <dbl> <dbl> <dbl>
## 1 nonchilled  30.6  31.3  94.2
## 2 chilled     23.8  19.7 118.
\end{verbatim}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\tightlist
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment} and \textbf{Concentration}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(Treatment, conc) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(uptake), }\DataTypeTok{med =} \KeywordTok{median}\NormalTok{(uptake), }\DataTypeTok{var =} \KeywordTok{var}\NormalTok{(uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 14 x 5
## # Groups:   Treatment [2]
##    Treatment   conc   avg   med    var
##    <fct>      <dbl> <dbl> <dbl>  <dbl>
##  1 nonchilled    95  13.3  12.8   5.75
##  2 nonchilled   175  25.1  24.6  32.6 
##  3 nonchilled   250  32.5  32.7  35.1 
##  4 nonchilled   350  35.1  34.5  37.4 
##  5 nonchilled   500  35.1  33.8  31.9 
##  6 nonchilled   675  36.0  35.8  40.2 
##  7 nonchilled  1000  37.4  37.6  49.8 
##  8 chilled       95  11.2  10.6   8.18
##  9 chilled      175  19.4  19.5  34.7 
## 10 chilled      250  25.3  24.2 112.  
## 11 chilled      350  26.2  26.4 117.  
## 12 chilled      500  26.6  26   131.  
## 13 chilled      675  27.9  28.8 120.  
## 14 chilled     1000  29.8  30.3 154.
\end{verbatim}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  Built-in \texttt{aggregate()} function more general
\end{itemize}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  Built-in \texttt{aggregate()} function more general
\item
  Basic use gives response (\texttt{x}) and a \texttt{list} of variables to group \texttt{by}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{by =} \KeywordTok{list}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{Treatment), }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Group.1   x.Min. x.1st Qu. x.Median   x.Mean x.3rd Qu.   x.Max.
## 1 nonchilled 10.60000  26.47500 31.30000 30.64286  38.70000 45.50000
## 2    chilled  7.70000  14.52500 19.70000 23.78333  34.90000 42.40000
\end{verbatim}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  Built-in \texttt{aggregate()} function more general
\item
  Commonly used with \texttt{formula} notation!
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Treatment uptake.Min. uptake.1st Qu. uptake.Median uptake.Mean
## 1 nonchilled    10.60000       26.47500      31.30000    30.64286
## 2    chilled     7.70000       14.52500      19.70000    23.78333
##   uptake.3rd Qu. uptake.Max.
## 1       38.70000    45.50000
## 2       34.90000    42.40000
\end{verbatim}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment}
\item
  Built-in \texttt{aggregate()} function more general
\item
  Commonly used with \texttt{formula} notation!
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\texttt{uptake\ \textasciitilde{}\ Treatment} - formula notation in R

\begin{itemize}
\tightlist
\item
  Idea: uptake (LHS) modeled by Treatment levels (RHS)
\end{itemize}

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment} and \textbf{Concentration}
\item
  Built-in \texttt{aggregate()} function more general
\item
  Commonly used with \texttt{formula} notation!
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment }\OperatorTok{+}\StringTok{ }\NormalTok{conc, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\texttt{uptake\ \textasciitilde{}\ Treatment\ +\ conc} model uptake by levels of Treatment and conc

Numerical summaries: Numerical variables

Usually want summaries for different \textbf{subgroups of data}

\begin{itemize}
\tightlist
\item
  Ex: Get similar uptake summaries for each \textbf{Treatment} and \textbf{Concentration}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment }\OperatorTok{+}\StringTok{ }\NormalTok{conc, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Treatment conc uptake.Min. uptake.1st Qu. uptake.Median uptake.Mean
## 1  nonchilled   95    10.60000       11.47500      12.80000    13.28333
## 2     chilled   95     7.70000        9.60000      10.55000    11.23333
## 3  nonchilled  175    19.20000       20.05000      24.65000    25.11667
## 4     chilled  175    11.40000       15.67500      19.50000    19.45000
## 5  nonchilled  250    25.80000       27.30000      32.70000    32.46667
## 6     chilled  250    12.30000       17.95000      24.20000    25.28333
## 7  nonchilled  350    27.90000       30.45000      34.50000    35.13333
## 8     chilled  350    13.00000       18.15000      26.45000    26.20000
## 9  nonchilled  500    28.50000       31.27500      33.85000    35.10000
## 10    chilled  500    12.50000       18.30000      26.00000    26.65000
## 11 nonchilled  675    28.10000       31.42500      35.80000    36.01667
## 12    chilled  675    13.70000       19.72500      28.80000    27.88333
##    uptake.3rd Qu. uptake.Max.
## 1        15.40000    16.20000
## 2        13.30000    15.10000
## 3        29.62500    32.40000
## 4        23.32500    27.30000
## 5        36.52500    40.30000
## 6        33.82500    38.10000
## 7        40.65000    42.10000
## 8        34.45000    38.80000
## 9        39.27500    42.90000
## 10       37.07500    38.90000
## 11       40.85000    43.90000
## 12       36.97500    39.60000
##  [ reached getOption("max.print") -- omitted 2 rows ]
\end{verbatim}

Recap/Next Up!

\begin{itemize}
\item
  Understand types of data and their distributions
\item
  Numerical summaries

  \begin{itemize}
  \tightlist
  \item
    Contingency Tables: \texttt{table}\\
  \item
    Mean/Median: \texttt{mean}, \texttt{median}
  \item
    Standard Deviation/Variance/IQR: \texttt{sd}, \texttt{var}, \texttt{IQR}
  \item
    Quantiles/Percentiles: \texttt{quantile}
  \end{itemize}
\item
  Across subgroups with \texttt{dplyr::group\_by} and \texttt{dplyr::summarize} or \texttt{aggregate}
\end{itemize}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Graphical summaries (across subgroups)
\end{itemize}
\end{quote}

Table of common summaries

Sample of Random Variable's realizations, sample distribution vs population, modeling ideas

Summaries of distributions (center, spread, graphs)

Good discussion of what makes a good sampling design revisit. Maybe a statified example like the river and selecting houses example as a quick expose of the issues with not doing a truly random sampling technique.

Approx probabilities and quantiles vs theoretical

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Graphical summaries
  Terminology (Raw data, variable, etc.)
  Categorical/Quantitative Variables
  Graphs for Categorical Data
  Graphs for Quantitative Data
  Distributions -- center, spread, shape
\item
  Numerical Summaries
  Measures of central tendency (location)
  Mean, Median
  Measures of variability (spread)
  Range, SD, IQR
  Quartiles
  Properties of the mean and standard deviation
  Boxplots
\end{enumerate}

\hypertarget{suggestions-for-further-readings}{%
\section{Suggestions for Further Readings}\label{suggestions-for-further-readings}}

Exploratory data analysis R book is pretty great.

\hypertarget{software-2}{%
\section{Software}\label{software-2}}

\hypertarget{r-2}{%
\subsection{R}\label{r-2}}

Recall the general workflow for programming in R.

\begin{center}\includegraphics[width=0.8\linewidth]{img/RWorkFlow} \end{center}

This section is meant to instruct on how to produce common numerical and graphical summaries in R.

\hypertarget{numerical-summaries}{%
\subsubsection{Numerical Summaries}\label{numerical-summaries}}

To create numerical summaries in R, you first need to determine the type of data you have. Remember there are two major categories:

\begin{itemize}
\tightlist
\item
  Categorical (Qualitative) variable - entries are a label or attribute\\
\item
  Numerical (Quantitative) variable - entries are a numerical value where math can be performed
\end{itemize}

The main goal is to describe the distribution or pattern and frequency with which you observe the variable. For categorical variables, we used contingency tables to describe the count in each category. In R, this is done using the \texttt{table} function.

We'll recreate the summaries of the titanic dataset (available in the file \texttt{titanic.csv} in the datasets folder).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanicData <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"../../datasets/titanic.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   pclass = col_double(),
##   survived = col_double(),
##   name = col_character(),
##   sex = col_character(),
##   age = col_double(),
##   sibsp = col_double(),
##   parch = col_double(),
##   ticket = col_character(),
##   fare = col_double(),
##   cabin = col_character(),
##   embarked = col_character(),
##   boat = col_character(),
##   body = col_double(),
##   home.dest = col_character()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanicData}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,310 x 14
##   pclass survived name  sex      age sibsp parch ticket  fare cabin embarked
##    <dbl>    <dbl> <chr> <chr>  <dbl> <dbl> <dbl> <chr>  <dbl> <chr> <chr>   
## 1      1        1 Alle~ fema~ 29         0     0 24160   211. B5    S       
## 2      1        1 Alli~ male   0.917     1     2 113781  152. C22 ~ S       
## 3      1        0 Alli~ fema~  2         1     2 113781  152. C22 ~ S       
## 4      1        0 Alli~ male  30         1     2 113781  152. C22 ~ S       
## 5      1        0 Alli~ fema~ 25         1     2 113781  152. C22 ~ S       
## # ... with 1,305 more rows, and 3 more variables: boat <chr>, body <dbl>,
## #   home.dest <chr>
\end{verbatim}

Recall the three variables we created tables for were:

\begin{itemize}
\tightlist
\item
  embarked (where journey started)\\
\item
  survived (survive or not)\\
\item
  sex (Male or Female)
\end{itemize}

To create the \textbf{one-way contingency tables}, we simply pass the column of interest to the \texttt{table} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   C   Q   S 
## 270 123 914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   0   1 
## 809 500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## female   male 
##    466    843
\end{verbatim}

To create a \textbf{two-way contingency table}, we can pass the columns of interest from the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived, titanicData}\OperatorTok{$}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     female male
##   0    127  682
##   1    339  161
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived, titanicData}\OperatorTok{$}\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##       C   Q   S
##   0 120  79 610
##   1 150  44 304
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{sex, titanicData}\OperatorTok{$}\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         
##            C   Q   S
##   female 113  60 291
##   male   157  63 623
\end{verbatim}

This same process can continue to create an \textbf{n-way contingency table}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{sex, titanicData}\OperatorTok{$}\NormalTok{embarked, titanicData}\OperatorTok{$}\NormalTok{survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## , ,  = 0
## 
##         
##            C   Q   S
##   female  11  23  93
##   male   109  56 517
## 
## , ,  = 1
## 
##         
##            C   Q   S
##   female 102  37 198
##   male    48   7 106
\end{verbatim}

These table objects in R are actually arrays - the extension of a matrix to more than two dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{sex, titanicData}\OperatorTok{$}\NormalTok{embarked, titanicData}\OperatorTok{$}\NormalTok{survived)}
\KeywordTok{str}\NormalTok{(tab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  'table' int [1:2, 1:3, 1:2] 11 109 23 56 93 517 102 48 37 7 ...
##  - attr(*, "dimnames")=List of 3
##   ..$ : chr [1:2] "female" "male"
##   ..$ : chr [1:3] "C" "Q" "S"
##   ..$ : chr [1:2] "0" "1"
\end{verbatim}

You can obtain \textbf{conditional} bivariate info by supplying an index.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#returns embarked vs survived table for females}
\NormalTok{tab[}\DecValTok{1}\NormalTok{, , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##       0   1
##   C  11 102
##   Q  23  37
##   S  93 198
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#returns embarked vs survived table for males}
\NormalTok{tab[}\DecValTok{2}\NormalTok{, , ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##       0   1
##   C 109  48
##   Q  56   7
##   S 517 106
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#returns survived vs sex table for embarked "C"}
\NormalTok{tab[, }\DecValTok{1}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         
##            0   1
##   female  11 102
##   male   109  48
\end{verbatim}

We can also obtain \textbf{conditional} univariate info too!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Survived status for males that embarked at "Q"}
\NormalTok{tab[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  0  1 
## 56  7
\end{verbatim}

For numerical variables, recall that we want to describe the shape, center, and spread of the variable's distribution.

Recall the carbon dioxide (CO2) uptake data set

\begin{itemize}
\tightlist
\item
  Response recorded: \texttt{uptake} CO2 uptake rates in grass plants\\
\item
  Environment manipulated: \texttt{Treatment} - chilled/nonchilled\\
\item
  Ambient CO2 specified and measured: \texttt{conc}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2 <-}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{(CO2)}
\NormalTok{CO2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 84 x 5
##   Plant Type   Treatment   conc uptake
##   <ord> <fct>  <fct>      <dbl>  <dbl>
## 1 Qn1   Quebec nonchilled    95   16  
## 2 Qn1   Quebec nonchilled   175   30.4
## 3 Qn1   Quebec nonchilled   250   34.8
## 4 Qn1   Quebec nonchilled   350   37.2
## 5 Qn1   Quebec nonchilled   500   35.3
## # ... with 79 more rows
\end{verbatim}

To investigate the center we looked at the mean, trimmed mean, and the median. The \texttt{mean} and \texttt{median} functions handle these. You simply pass the column of interest. To obtain a trimmed mean, you can use the \texttt{trim} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 27.2131
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#5% trimmed mean}
\KeywordTok{mean}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{trim =} \FloatTok{0.05}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 27.25263
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 28.3
\end{verbatim}

To obtain measures of spread we can use \texttt{var} and \texttt{sd}. Similarly, we pass the column of interest to these functions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 116.9515
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.81441
\end{verbatim}

Quantiles can be found using the \texttt{quantile} function. You pass the column of interest and a vector of probabilities corresponding to the quantiles of interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{probs =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   10%   20% 
## 12.36 15.64
\end{verbatim}

The IQR can be found by passing the \texttt{IQR} function the column of interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{IQR}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19.225
\end{verbatim}

R has some nice functions that return multiple summaries. For instance, the \texttt{summary} function!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#quartiles and mean}
\KeywordTok{summary}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    7.70   17.90   28.30   27.21   37.12   45.50
\end{verbatim}

The \texttt{psych} package has a very nice function called \texttt{describe} that provides many summaries about your numerical variables in the dataset as well. Remember you need to install a package if you've never used it and then read it in to your current R session using the \texttt{library} or \texttt{require} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}\NormalTok{(titanicData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion

## Warning in describe(titanicData): NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf
\end{verbatim}

\begin{verbatim}
## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf

## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf
\end{verbatim}

\begin{verbatim}
##          vars    n  mean    sd median trimmed   mad  min  max range  skew
## pclass      1 1309  2.29  0.84      3    2.37  0.00 1.00    3  2.00 -0.60
## survived    2 1309  0.38  0.49      0    0.35  0.00 0.00    1  1.00  0.49
## name*       3 1309   NaN    NA     NA     NaN    NA  Inf -Inf  -Inf    NA
## sex*        4 1309   NaN    NA     NA     NaN    NA  Inf -Inf  -Inf    NA
## age         5 1046 29.88 14.41     28   29.39 11.86 0.17   80 79.83  0.41
## sibsp       6 1309  0.50  1.04      0    0.27  0.00 0.00    8  8.00  3.84
## parch       7 1309  0.39  0.87      0    0.18  0.00 0.00    9  9.00  3.66
##          kurtosis   se
## pclass      -1.32 0.02
## survived    -1.77 0.01
## name*          NA   NA
## sex*           NA   NA
## age          0.13 0.45
## sibsp       19.93 0.03
## parch       21.42 0.02
##  [ reached 'max' / getOption("max.print") -- omitted 7 rows ]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}\NormalTok{(CO2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            vars  n   mean     sd median trimmed    mad  min    max range  skew
## Plant*        1 84   6.50   3.47    6.5    6.50   4.45  1.0   12.0  11.0  0.00
## Type*         2 84   1.50   0.50    1.5    1.50   0.74  1.0    2.0   1.0  0.00
## Treatment*    3 84   1.50   0.50    1.5    1.50   0.74  1.0    2.0   1.0  0.00
## conc          4 84 435.00 295.92  350.0  408.53 259.45 95.0 1000.0 905.0  0.72
## uptake        5 84  27.21  10.81   28.3   27.33  14.83  7.7   45.5  37.8 -0.10
##            kurtosis    se
## Plant*        -1.26  0.38
## Type*         -2.02  0.05
## Treatment*    -2.02  0.05
## conc          -0.68 32.29
## uptake        -1.35  1.18
\end{verbatim}

Recall, if you had multiple numerical variables we often want to measure the linear relationship. This is done via covariance and correlation. If you pass the \texttt{var} or \texttt{cov} function a data frame with multiple numeric columns, you'll obtain the variance-covariance matrix. The diagonals represent the variance and the off-diagonals the covariances. The \texttt{cor} function similarly gives the correlation matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(}\KeywordTok{select}\NormalTok{(CO2, conc, uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             conc    uptake
## conc   87571.084 1552.6867
## uptake  1552.687  116.9515
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cov}\NormalTok{(}\KeywordTok{select}\NormalTok{(CO2, conc, uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             conc    uptake
## conc   87571.084 1552.6867
## uptake  1552.687  116.9515
\end{verbatim}

Of course, often summaries by a subgroup of the data is of interest. We looked at finding summaries of \texttt{uptake} for each \texttt{Treatment} level. \texttt{dplyr} is easy to use for this purpose but can only use functions that return a single value. The idea is:

\begin{itemize}
\tightlist
\item
  Use \texttt{group\_by} to create subgroups associated with the data frame\\
\item
  Use \texttt{summarize} to create basic summaries for each subgroup
\end{itemize}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(Treatment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(uptake), }\DataTypeTok{med =} \KeywordTok{median}\NormalTok{(uptake), }\DataTypeTok{var =} \KeywordTok{var}\NormalTok{(uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   Treatment    avg   med   var
##   <fct>      <dbl> <dbl> <dbl>
## 1 nonchilled  30.6  31.3  94.2
## 2 chilled     23.8  19.7 118.
\end{verbatim}

To find the summary for different settings of two variables, say \textbf{Treatment} and \textbf{Concentration}, simply pass more than one categorical variable to \texttt{group\_by}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(Treatment, conc) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(uptake), }\DataTypeTok{med =} \KeywordTok{median}\NormalTok{(uptake), }\DataTypeTok{var =} \KeywordTok{var}\NormalTok{(uptake))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 14 x 5
## # Groups:   Treatment [2]
##    Treatment   conc   avg   med    var
##    <fct>      <dbl> <dbl> <dbl>  <dbl>
##  1 nonchilled    95  13.3  12.8   5.75
##  2 nonchilled   175  25.1  24.6  32.6 
##  3 nonchilled   250  32.5  32.7  35.1 
##  4 nonchilled   350  35.1  34.5  37.4 
##  5 nonchilled   500  35.1  33.8  31.9 
##  6 nonchilled   675  36.0  35.8  40.2 
##  7 nonchilled  1000  37.4  37.6  49.8 
##  8 chilled       95  11.2  10.6   8.18
##  9 chilled      175  19.4  19.5  34.7 
## 10 chilled      250  25.3  24.2 112.  
## 11 chilled      350  26.2  26.4 117.  
## 12 chilled      500  26.6  26   131.  
## 13 chilled      675  27.9  28.8 120.  
## 14 chilled     1000  29.8  30.3 154.
\end{verbatim}

An alternative to using \texttt{dplyr} is to use the built-in \texttt{aggregate} function. This function is more flexible and can be used with functions that return more than one value like \texttt{summary}. Basic use gives a response (\texttt{x}) and a \texttt{list} of variables to group \texttt{by}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{by =} \KeywordTok{list}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{Treatment), }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Group.1   x.Min. x.1st Qu. x.Median   x.Mean x.3rd Qu.   x.Max.
## 1 nonchilled 10.60000  26.47500 31.30000 30.64286  38.70000 45.50000
## 2    chilled  7.70000  14.52500 19.70000 23.78333  34.90000 42.40000
\end{verbatim}

Alternatively, \texttt{formula} notation can be used. We can supply \texttt{uptake\ \textasciitilde{}\ Treatment} and the data object. The idea of formula notation is essentially a shorthand for modeling the left-hand side (uptake here) by the right-hand side (Treatment here). In this case, this implies finding \texttt{uptake} values returned by \texttt{FUN} for the different levels of \texttt{Treatment}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Treatment uptake.Min. uptake.1st Qu. uptake.Median uptake.Mean
## 1 nonchilled    10.60000       26.47500      31.30000    30.64286
## 2    chilled     7.70000       14.52500      19.70000    23.78333
##   uptake.3rd Qu. uptake.Max.
## 1       38.70000    45.50000
## 2       34.90000    42.40000
\end{verbatim}

A second variable can be given on the right hand side. The summaries will then be found for every combination of the values of the variables on the right hand side. Since \texttt{conc} is only observed at a few levels, this is reasonable here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment }\OperatorTok{+}\StringTok{ }\NormalTok{conc, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{FUN =}\NormalTok{ summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Treatment conc uptake.Min. uptake.1st Qu. uptake.Median uptake.Mean
## 1  nonchilled   95    10.60000       11.47500      12.80000    13.28333
## 2     chilled   95     7.70000        9.60000      10.55000    11.23333
## 3  nonchilled  175    19.20000       20.05000      24.65000    25.11667
## 4     chilled  175    11.40000       15.67500      19.50000    19.45000
## 5  nonchilled  250    25.80000       27.30000      32.70000    32.46667
## 6     chilled  250    12.30000       17.95000      24.20000    25.28333
## 7  nonchilled  350    27.90000       30.45000      34.50000    35.13333
## 8     chilled  350    13.00000       18.15000      26.45000    26.20000
## 9  nonchilled  500    28.50000       31.27500      33.85000    35.10000
## 10    chilled  500    12.50000       18.30000      26.00000    26.65000
## 11 nonchilled  675    28.10000       31.42500      35.80000    36.01667
## 12    chilled  675    13.70000       19.72500      28.80000    27.88333
##    uptake.3rd Qu. uptake.Max.
## 1        15.40000    16.20000
## 2        13.30000    15.10000
## 3        29.62500    32.40000
## 4        23.32500    27.30000
## 5        36.52500    40.30000
## 6        33.82500    38.10000
## 7        40.65000    42.10000
## 8        34.45000    38.80000
## 9        39.27500    42.90000
## 10       37.07500    38.90000
## 11       40.85000    43.90000
## 12       36.97500    39.60000
##  [ reached getOption("max.print") -- omitted 2 rows ]
\end{verbatim}

\hypertarget{graphical-summaries}{%
\subsubsection{Graphical Summaries}\label{graphical-summaries}}

There are three major systems for plotting in R.

\begin{itemize}
\item
  Base R (built-in functions)

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{plot}, \texttt{barplot}, \texttt{hist}, etc. to start a plot\\
  \item
    Annotate the plot using functions like \texttt{text}, \texttt{lines}, \texttt{points}, etc.
  \end{itemize}
\item
  Lattice (not covered here)
\item
  ggplot2 (sort of part of the tidyverse - \href{https://www.google.com/url?sa=t\&rct=j\&q=\&esrc=s\&source=web\&cd=2\&cad=rja\&uact=8\&ved=2ahUKEwjFu_ep-rroAhXnknIEHf13Bq4QFjABegQICBAB\&url=https\%3A\%2F\%2Frstudio.com\%2Fwp-content\%2Fuploads\%2F2016\%2F11\%2Fggplot2-cheatsheet-2.1.pdf\&usg=AOvVaw1-6jSR5VfWs7V62OYxtDtG}{Cheat Sheet})

  \begin{itemize}
  \tightlist
  \item
    \texttt{ggplot(data\ =\ data\_frame)} creates a plot instance\\
  \item
    Add ``layers'' to the system (geoms or stats)
  \end{itemize}
\end{itemize}

There is a great \href{https://bookdown.org/rdpeng/exdata/plotting-systems.html}{reference book here} for plotting in R and general exploratory data analysis!

We'll start by going through common \texttt{Base\ R} plotting for both categorical and numerical variables. After that, we'll cover the same plots using \texttt{ggplot2}.

\hypertarget{base-r-plotting}{%
\paragraph{\texorpdfstring{\texttt{Base\ R} Plotting}{Base R Plotting}}\label{base-r-plotting}}

Let's start by graphing categorical variables (variables in which entries are a label or attribute). Recall that the main goal of summarizing categorical variables is to look at counts for each level or combination of levels for the variables in question.

Again, we'll look at the titanic data set.

\begin{verbatim}
## Parsed with column specification:
## cols(
##   pclass = col_double(),
##   survived = col_double(),
##   name = col_character(),
##   sex = col_character(),
##   age = col_double(),
##   sibsp = col_double(),
##   parch = col_double(),
##   ticket = col_character(),
##   fare = col_double(),
##   cabin = col_character(),
##   embarked = col_character(),
##   boat = col_character(),
##   body = col_double(),
##   home.dest = col_character()
## )
\end{verbatim}

\begin{verbatim}
## # A tibble: 1,310 x 14
##   pclass survived name  sex      age sibsp parch ticket  fare cabin embarked
##    <dbl>    <dbl> <chr> <chr>  <dbl> <dbl> <dbl> <chr>  <dbl> <chr> <chr>   
## 1      1        1 Alle~ fema~ 29         0     0 24160   211. B5    S       
## 2      1        1 Alli~ male   0.917     1     2 113781  152. C22 ~ S       
## 3      1        0 Alli~ fema~  2         1     2 113781  152. C22 ~ S       
## 4      1        0 Alli~ male  30         1     2 113781  152. C22 ~ S       
## 5      1        0 Alli~ fema~ 25         1     2 113781  152. C22 ~ S       
## # ... with 1,305 more rows, and 3 more variables: boat <chr>, body <dbl>,
## #   home.dest <chr>
\end{verbatim}

The function to create a barplot in R is \texttt{barplot}. Essentially, we can just input the output from the \texttt{table} function into \texttt{barplot}!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   0   1 
## 809 500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(}\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-276-1} \end{center}

To visualize a two-way table, a stacked barplot can be created. Again, we simply pass a two-way table to the \texttt{barplot} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{twoTable <-}\StringTok{ }\KeywordTok{table}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived, titanicData}\OperatorTok{$}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(twoTable)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-278-1} \end{center}

Of course, a legend is needed. Common arguments to \texttt{barplot} include:

\begin{itemize}
\tightlist
\item
  \texttt{legend\ =\ TRUE}
\item
  `args.legend = list(title = ``\ldots{}'')
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(twoTable, }\DataTypeTok{legend =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{args.legend =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Survived"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-279-1} \end{center}

To create a \textbf{Side-by-side barplot}, simply pass a two-way table to \texttt{barplot} with an additional input of \texttt{beside\ =\ TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(twoTable, }\DataTypeTok{beside =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{legend =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{args.legend=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Survived"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-280-1} \end{center}

The legend can be specified more clearly by providing a character vector corresponding to the values of the first variable given to the \texttt{table} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(twoTable, }\DataTypeTok{beside =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Died"}\NormalTok{, }\StringTok{"Survived"}\NormalTok{),}
        \DataTypeTok{args.legend=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Survived"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-281-1} \end{center}

To visualize a three-way table, we essentially want to recreate this plot for each setting of the third variable. This is a little difficult in \texttt{Base\ R} and will be taken up in the \texttt{ggplot} section.

Next we'll create common plots for numerical variables (variables in which entries are a numerical value where math can be performed). Recall that the main goal of summarizing numerical variables is to describe the shape, center, and spread of the distribution. Generally, this is done using a histogram, boxplot, or, in the case of two numerical variables, scatterplot.

The general process for creating these plots using \texttt{Base\ R}:

\begin{itemize}
\item
  Use \texttt{hist}, \texttt{boxpot}, \texttt{plot}, etc. to start a plot
\item
  Annotate the plot using functions like \texttt{text}, \texttt{lines}, \texttt{points}, etc.
\end{itemize}

We'll again look at the carbon dioxide (CO2) uptake data set.

\begin{verbatim}
## # A tibble: 84 x 5
##   Plant Type   Treatment   conc uptake
##   <ord> <fct>  <fct>      <dbl>  <dbl>
## 1 Qn1   Quebec nonchilled    95   16  
## 2 Qn1   Quebec nonchilled   175   30.4
## 3 Qn1   Quebec nonchilled   250   34.8
## 4 Qn1   Quebec nonchilled   350   37.2
## 5 Qn1   Quebec nonchilled   500   35.3
## # ... with 79 more rows
\end{verbatim}

The \texttt{hist} function creates a histogram and its main argument is a numeric vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-283-1} \end{center}

There are many ways to modify the histogram. Some common things to change would be the title (\texttt{main}), the bin calculation (\texttt{breaks}), and perhaps the color of the bars (\texttt{col}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{main =} \StringTok{"Histogram of Uptake Values"}\NormalTok{, }\DataTypeTok{breaks =} \DecValTok{15}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-284-1} \end{center}

Often there is a value that you'd like to highlight by adding a line. This can be done using the \texttt{abline} function (\texttt{lty} specifies line type, \texttt{lwd} the line width, \texttt{col} the color, etc. - see \texttt{help(par)} for more information). \texttt{abline} will add a line to the most recently created plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{main =} \StringTok{"Histogram of Uptake Values"}\NormalTok{, }\DataTypeTok{breaks =} \DecValTok{15}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Red"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{mean}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake), }\DataTypeTok{lwd =} \DecValTok{3}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-285-1} \end{center}

A kernel plot (or smoothed histogram plot) can be created by using the \texttt{plot} function on the output created by the \texttt{density} function. \texttt{density} will comput kernel density estimates, by default using the normal distribution for weighting. As with \texttt{hist}, we pass \texttt{density} the numeric vector of interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{density}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  density.default(x = CO2$uptake)
## 
## Data: CO2$uptake (84 obs.);  Bandwidth 'bw' = 4.012
## 
##        x                y            
##  Min.   :-4.337   Min.   :2.286e-05  
##  1st Qu.:11.132   1st Qu.:2.863e-03  
##  Median :26.600   Median :2.125e-02  
##  Mean   :26.600   Mean   :1.615e-02  
##  3rd Qu.:42.068   3rd Qu.:2.649e-02  
##  Max.   :57.537   Max.   :3.095e-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake), }\DataTypeTok{lwd =} \DecValTok{3}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Blue"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-286-1} \end{center}

To overlay a kernel smoother on top of a histogram, we first create a histogram and specify \texttt{freq\ =\ FALSE} in order to put the histogram on the same scale as a density plot. Then, we use \texttt{lines} instead of \texttt{plot} on the output of the \texttt{density} call. \texttt{lines} is a function similar to \texttt{abline} in that it will overlay a (more general) `line' on the most recently created plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{main =} \StringTok{"Histogram of Uptake Values"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Red"}\NormalTok{, }\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake), }\DataTypeTok{lwd =} \DecValTok{3}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Blue"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-287-1} \end{center}

To create a boxplot, use the \texttt{boxplot} function. We can specify a single numeric vector as the argument. This will create a single boxplot. To create side-by-side boxplots, the formula notation we've used before comes in handy! If we want a boxplot of the \texttt{uptake} variable for each level of the \texttt{Treatment} variable, we pass \texttt{boxplot} \texttt{uptake\ \textasciitilde{}\ Treatment} and specify the data set being used (\texttt{main} again will change the title).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }\DataTypeTok{main =} \StringTok{"Boxplot of Uptake by Treatment"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-288-1} \end{center}

It is easy to change the orientation using the \texttt{horizontal} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }
        \DataTypeTok{main =} \StringTok{"Boxplot of Uptake by Treatment"}\NormalTok{, }\DataTypeTok{horizontal =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-289-1} \end{center}

Most of the graphical parameters used previously can be changed here too because \texttt{boxplot} has the ellipsis argument \texttt{...}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data =}\NormalTok{ CO2, }
        \DataTypeTok{main =} \StringTok{"Boxplot of Uptake by Treatment"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Grey"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-290-1} \end{center}

The ellipsis argument exists in many functions including \texttt{hist}, \texttt{boxplot}, \texttt{abline}, \texttt{lines}, and \texttt{plot} This argument means additional arguments not named in the inputs of the function being called can be passed. These arguments may be used by function calls within one of these functions! You can see the code used in each function by running the function with no \texttt{()} after it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{read_csv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), 
##     na = c("", "NA"), quoted_na = TRUE, quote = "\"", comment = "", 
##     trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, 
##         n_max), progress = show_progress(), skip_empty_rows = TRUE) 
## {
##     tokenizer <- tokenizer_csv(na = na, quoted_na = quoted_na, 
##         quote = quote, comment = comment, trim_ws = trim_ws, 
##         skip_empty_rows = skip_empty_rows)
##     read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, 
##         locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, 
##         comment = comment, n_max = n_max, guess_max = guess_max, 
##         progress = progress)
## }
## <bytecode: 0x0000000019f77f08>
## <environment: namespace:readr>
\end{verbatim}

Some functions are `generic' such as \texttt{mean} and \texttt{plot}. You need to know which mean or plot method you are trying to get the code. \texttt{plot.default} is the default!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot.default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (x, y = NULL, type = "p", xlim = NULL, ylim = NULL, 
##     log = "", main = NULL, sub = NULL, xlab = NULL, ylab = NULL, 
##     ann = par("ann"), axes = TRUE, frame.plot = axes, panel.first = NULL, 
##     panel.last = NULL, asp = NA, xgap.axis = NA, ygap.axis = NA, 
##     ...) 
## {
##     localAxis <- function(..., col, bg, pch, cex, lty, lwd) Axis(...)
##     localBox <- function(..., col, bg, pch, cex, lty, lwd) box(...)
##     localWindow <- function(..., col, bg, pch, cex, lty, lwd) plot.window(...)
##     localTitle <- function(..., col, bg, pch, cex, lty, lwd) title(...)
##     xlabel <- if (!missing(x)) 
##         deparse(substitute(x))
##     ylabel <- if (!missing(y)) 
##         deparse(substitute(y))
##     xy <- xy.coords(x, y, xlabel, ylabel, log)
##     xlab <- if (is.null(xlab)) 
##         xy$xlab
##     else xlab
##     ylab <- if (is.null(ylab)) 
##         xy$ylab
##     else ylab
##     xlim <- if (is.null(xlim)) 
##         range(xy$x[is.finite(xy$x)])
##     else xlim
##     ylim <- if (is.null(ylim)) 
##         range(xy$y[is.finite(xy$y)])
##     else ylim
##     dev.hold()
##     on.exit(dev.flush())
##     plot.new()
##     localWindow(xlim, ylim, log, asp, ...)
##     panel.first
##     plot.xy(xy, type, ...)
##     panel.last
##     if (axes) {
##         localAxis(if (is.null(y)) 
##             xy$x
##         else x, side = 1, gap.axis = xgap.axis, ...)
##         localAxis(if (is.null(y)) 
##             x
##         else y, side = 2, gap.axis = ygap.axis, ...)
##     }
##     if (frame.plot) 
##         localBox(...)
##     if (ann) 
##         localTitle(main = main, sub = sub, xlab = xlab, ylab = ylab, 
##             ...)
##     invisible()
## }
## <bytecode: 0x000000001a380920>
## <environment: namespace:graphics>
\end{verbatim}

Here you can see that some of the functions called by \texttt{plot.default} also have the \texttt{...} argument being used. This is why the arguments like \texttt{lty} and \texttt{col} can be used even though they aren't always named in the function inputs explicity. When working with \texttt{Base\ R} graphics, getting to know the graphical parameters (see \texttt{help(par)}) is important!

The \texttt{plot} function itself is very flexible. General use:

\begin{itemize}
\tightlist
\item
  Call \texttt{plot}\\
\item
  Give coordinates for points to be plotted as \texttt{x} and \texttt{y} (usually)\\
\item
  Change plot type with \texttt{type}\\
\item
  Change graphical parameters such as line type \texttt{lty}, point type \texttt{pch}, color \texttt{color}, etc.\\
\item
  Add a legend with \texttt{legend}
\end{itemize}

To create a basic scatterplot, just specify two numeric vectors of the same length.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-293-1} \end{center}

By default the \texttt{plot} type is `p' for points but many types of plots can be made by changing the \texttt{type} argument. The labels on the axis can be changed using \texttt{xlab} and \texttt{ylab}, the title with \texttt{main}, the plotting character \texttt{pch}, etc.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{xlab =} \StringTok{"Concentration"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Uptake"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Scatter Plot of Concentration and Uptake"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-294-1} \end{center}

To change the plotting symbol (or color, etc.) based on another variable, create appropriate vectors that match the length of the data. For instance, we can create a \texttt{symbol} column on our data frame that is \texttt{3} if the \texttt{Treatment} is `chilled' and \texttt{2} if not. This symbol column is of course the same length as the data. If we pass the \texttt{pch} argument \texttt{symbol}, \texttt{plot} will assign the plotting character \texttt{3} to chilled observations and \texttt{2} if not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CO2}\OperatorTok{$}\NormalTok{symbol <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{Treatment }\OperatorTok{==}\StringTok{ "chilled"}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\KeywordTok{select}\NormalTok{(CO2, uptake, Treatment, symbol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 84 x 3
##   uptake Treatment  symbol
##    <dbl> <fct>       <dbl>
## 1   16   nonchilled      2
## 2   30.4 nonchilled      2
## 3   34.8 nonchilled      2
## 4   37.2 nonchilled      2
## 5   35.3 nonchilled      2
## # ... with 79 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{pch =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{symbol)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-295-1} \end{center}

To instruct the reader of the plot, a legend should be added. A manual legend can be created with \texttt{legend}. You can use keywords to specify its location or give an \texttt{x}-\texttt{y} pair. As with the legend for \texttt{barplot}, we can specify the names that display. To match the plot, we should specify the \texttt{pch} corresponding to the levels in the same manner (this would be the same if we changed the color too!).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{pch =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{symbol)}
\KeywordTok{legend}\NormalTok{(}\DataTypeTok{x =} \DecValTok{810}\NormalTok{, }\DataTypeTok{y =} \DecValTok{18}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Chilled"}\NormalTok{, }\StringTok{"Non-Chilled"}\NormalTok{), }\DataTypeTok{pch =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-296-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Note that colors specified as integers come from the `palette` function }
\CommentTok{#2 is 'red' and 3 is 'green3'}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{col =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{symbol)}
\KeywordTok{legend}\NormalTok{(}\DataTypeTok{x =} \DecValTok{810}\NormalTok{, }\DataTypeTok{y =} \DecValTok{18}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Chilled"}\NormalTok{, }\StringTok{"Non-Chilled"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-296-2} \end{center}

Plots can be adorned using \texttt{abline}, \texttt{lines}, and \texttt{text} to name a few. For instance, a trend line can be added with \texttt{abline}. We'll use \texttt{lm} and our formula notation to fit a simple linear regression model (covered later) and \texttt{abline} knows how to plot an object of class \texttt{lm}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{xlab =} \StringTok{"Concentration"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Uptake"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Scatter Plot of Concentration and Uptake"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)))}
\KeywordTok{abline}\NormalTok{(}\KeywordTok{lm}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{conc, }\DataTypeTok{data =}\NormalTok{ CO2), }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-297-1} \end{center}

When adding text, the \texttt{text} function can be used. \texttt{paste} or \texttt{paste0} can come in handy. These functions `paste' together strings. \texttt{paste} allows the user to specify the character that separates the pasted together strings and \texttt{paste0} is a special case where there is no separating character.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{"Hi"}\NormalTok{, }\StringTok{"What"}\NormalTok{, }\StringTok{"Is"}\NormalTok{, }\StringTok{"Going"}\NormalTok{, }\StringTok{"On"}\NormalTok{, }\StringTok{"?"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hi What Is Going On ?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{"Hi"}\NormalTok{, }\StringTok{"What"}\NormalTok{, }\StringTok{"Is"}\NormalTok{, }\StringTok{"Going"}\NormalTok{, }\StringTok{"On"}\NormalTok{, }\StringTok{"?"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hi.What.Is.Going.On.?"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste0}\NormalTok{(}\StringTok{"Hi"}\NormalTok{, }\StringTok{"What"}\NormalTok{, }\StringTok{"Is"}\NormalTok{, }\StringTok{"Going"}\NormalTok{, }\StringTok{"On"}\NormalTok{, }\StringTok{"?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "HiWhatIsGoingOn?"
\end{verbatim}

We can add the correlation value to the plot using \texttt{text} and assigning the \texttt{x}-\texttt{y} coordinate for the value. Here we round the value to two decimal places using \texttt{round}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlation <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc, CO2}\OperatorTok{$}\NormalTok{uptake)}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{xlab =} \StringTok{"Concentration"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Uptake"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Scatter Plot of Concentration and Uptake"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)))}
\KeywordTok{abline}\NormalTok{(}\KeywordTok{lm}\NormalTok{(uptake }\OperatorTok{~}\StringTok{ }\NormalTok{conc, }\DataTypeTok{data =}\NormalTok{ CO2), }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"Blue"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x =} \DecValTok{750}\NormalTok{, }\DataTypeTok{y =} \DecValTok{10}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{label =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"Correlation = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(correlation, }\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-299-1} \end{center}

To plot character strings instead of points, \texttt{text} can be used after creating an `empty' \texttt{plot} call. \texttt{plot} should be called and the plotting window set, but the values to plot should be \texttt{NULL} (a special object in R used for things that are undefined). Next, call the text function using the usual \texttt{x}-\texttt{y} coordinates along with a \texttt{label} vector the same length as the data. For example, we could plot the \texttt{Plant} for each data point instead of a symbol.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc), }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc)), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake), }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)), }
     \DataTypeTok{main =} \StringTok{"Scatter Plot of Concentration and Uptake"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Conc"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Uptake"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{label =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{Plant)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-300-1} \end{center}

Note that you could specify a single character value for \texttt{label} and the plot would still work. This is because R `recycles' the value. This is a powerful but dangerous aspect of R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc), }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc)), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake), }\KeywordTok{max}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{uptake)), }
     \DataTypeTok{main =} \StringTok{"Scatter Plot of Concentration and Uptake"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Conc"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Uptake"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{conc, }\DataTypeTok{y =}\NormalTok{ CO2}\OperatorTok{$}\NormalTok{uptake, }\DataTypeTok{label =} \StringTok{"S"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-301-1} \end{center}

The last \texttt{Base\ R} plot functionality we'll mention is the \texttt{pairs} function. This function creates basic plots for all pairs of variables when given a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(iris[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-302-1} \end{center}

To recap \texttt{Base\ R} plotting:

\begin{itemize}
\item
  Barplot (\texttt{barplot} on a \texttt{table} object)
\item
  Histogram (\texttt{hist}), Density (\texttt{plot(density)} or \texttt{lines(density)})
\item
  Boxplot (\texttt{boxplot})
\item
  Scatter plot (\texttt{plot})
\item
  Useful functions: \texttt{lines}, \texttt{abline}, \texttt{points}, \texttt{text}
\item
  Graphical parameters: \texttt{lty}, \texttt{lwd}, \texttt{pch}, \texttt{cex}, \texttt{color}
\end{itemize}

\texttt{Base\ R} creates superb plots. The main issue is that much of the plotting customizations have to be manually created. It is also a bit more work to create the same plot for each level of another variable. \texttt{ggplot} will automate these processes.

\hypertarget{ggplot-plotting}{%
\paragraph{\texorpdfstring{\texttt{ggplot} Plotting}{ggplot Plotting}}\label{ggplot-plotting}}

Now that we've covered \texttt{Base\ R} plotting, we'll look at plotting through \texttt{ggplot2} (sort of part of the tidyverse - \href{https://www.google.com/url?sa=t\&rct=j\&q=\&esrc=s\&source=web\&cd=2\&cad=rja\&uact=8\&ved=2ahUKEwjFu_ep-rroAhXnknIEHf13Bq4QFjABegQICBAB\&url=https\%3A\%2F\%2Frstudio.com\%2Fwp-content\%2Fuploads\%2F2016\%2F11\%2Fggplot2-cheatsheet-2.1.pdf\&usg=AOvVaw1-6jSR5VfWs7V62OYxtDtG}{Cheat Sheet}). The main idea is outlined below:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot(data\ =\ data\_frame)} creates a plot instance\\
\item
  Add ``layers'' to the system (geoms or stats)

  \begin{itemize}
  \tightlist
  \item
    Creates a visualization of the data\\
  \end{itemize}
\item
  Modify layer ``mapping'' args (aes)

  \begin{itemize}
  \tightlist
  \item
    Ex: size, color, and x, y location(s)\\
  \end{itemize}
\item
  Coordinate system (mostly use Cartesian plane)\\
\item
  Optional: Titles, etc.
\end{itemize}

We'll go through creation of the same plots in the \texttt{Base\ R} section.

To create a barplot we can use \texttt{ggplot\ +\ geom\_bar}. To start, we create a base plotting object with \texttt{ggplot}. Here we can specify attributes of the plot in the \texttt{aes} function. For instance, to create a vertical bar chart we can specify the cateogrical variable of interest as \texttt{x}. Let's do this for the \texttt{survived} variable from the titanic data set. \texttt{survived} is actually stored as a \texttt{dbl} or double in the dataset since it is coded as a 0/1 variable. The plots will look better if we explicity change this as a \texttt{factor} first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanicData}\OperatorTok{$}\NormalTok{survived <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(titanicData}\OperatorTok{$}\NormalTok{survived)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-303-1} \end{center}

Notice that no plot is made! We haven't added a \texttt{geom} or \texttt{stat} layer yet. These are generally what you add in order to actually see something created with the plot. The base plotting object just let's you specify global attributes like what goes on the x-axis. Having the cheat sheet nearby is highly recommend when learning to use \texttt{ggplot}! The \texttt{geom} layer we want to add is \texttt{geom\_bar}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-304-1} \end{center}

The usual practice is to save the base object and then add to it. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived))}
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We won't do this here just to help make the plots as clear as possible.

You may wonder how to modify the plot. \texttt{aes()} defines visual properties of objects in the plot. This is where a lot of the modification occurs. Changing these is similar to changing values of \texttt{par} in \texttt{Base\ R} plotting. The cheat sheet gives the most commonly used aesthetics arguments for each \texttt{geom}. Under \texttt{geom\_bar} we see

\begin{quote}
\texttt{d\ +\ geom\_bar()}~\\
\texttt{x,\ alpha,\ color,\ fill,\ linetype,\ size,\ weight}
\end{quote}

The \texttt{data} and \texttt{aes} values can be set in two ways:

\begin{itemize}
\item
  `globally' (for all layers) via the \texttt{ggplot} statement
\item
  `locally' (for just that layer) via the \texttt{geom}, \texttt{stat}, etc. layer
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#global }
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\CommentTok{#local }
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived))}
\end{Highlighting}
\end{Shaded}

To set an attribute that doesn't depend on the data (i.e.~\texttt{color\ =\ \textquotesingle{}blue\textquotesingle{}}), you generally place these outside of the \texttt{aes}. Usually only aesthetics that need to depend on the data should go in the \texttt{aes} function.

Back to our barplot. We can improve the plot in a number of ways. First, we can remove the \texttt{NA} (missing value) category by using the \texttt{drop\_na} function. Here we'll remove all rows where any of the three variables we are investigating have missing values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanicData <-}\StringTok{ }\NormalTok{titanicData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{drop_na}\NormalTok{(survived, sex, embarked)}
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived))}
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-307-1} \end{center}

Next, we can add better labels and a title by adding the appropriate layers (new layers --\textgreater{} see cheat sheet!). The layers we want here are \texttt{labs} to set the title and \texttt{x} (or \texttt{y}) labels. To change the names on the legend we need to look at the \texttt{scale\_x\_discrete} layer. This is because the \texttt{aes} value's scale we are trying to modify is `x' and we are doing so for a layer with a `discrete' variable (their word for categorical).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.character}\NormalTok{(survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Survival Status"}\NormalTok{, }\DataTypeTok{title =} \StringTok{"Bar Plot of Survival for Titanic Passengers"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Died"}\NormalTok{, }\StringTok{"Survived"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-308-1} \end{center}

A \textbf{stacked barplot} can be created by using the \texttt{fill} aesthetic and the same process.

\begin{itemize}
\tightlist
\item
  Create base object\\
\item
  Add geoms\\
\item
  Use aes to specify aspects of the plot
\end{itemize}

The truly wonderful things about \texttt{ggplot} are the automatic assignment of colors, the creation of legends, and things of that nature for \texttt{aes} elements. This takes a lot of work from the user. Note: the \texttt{group} aestethic does not automatically generate a legend.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-309-1} \end{center}

Again, we can add custom labels by adding more layers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Survival Status"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"Bar Plot of Survival Status for Titanic Passengers"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Died"}\NormalTok{, }\StringTok{"Survived"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Sex"}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-310-1} \end{center}

Notice the similar adjustment for the scale using \texttt{scale\_*\_discrete}. For our aestethics, we set \texttt{aes(x\ =\ survived,\ fill\ =\ sex)}. This means if we want to change one of those auto-generated scales we should use \texttt{scale\_aes-value\_discrete}.

\begin{quote}
\texttt{scale\_x\_discrete(labels\ =\ c("Died",\ "Survived"))}
\texttt{scale\_fill\_discrete(name\ =\ "Sex",\ labels\ =\ c("Female","Male"))}
\end{quote}

To turn the plot we can add the \texttt{coord\_flip} layer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Survival Status"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"Bar Plot of Survival Status for Titanic Passengers"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Died"}\NormalTok{, }\StringTok{"Survived"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Sex"}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{,}\StringTok{"Male"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-311-1} \end{center}

One thing that can be quite confusing when you are first using the \texttt{ggplot} function is that most geoms have a corresponding stat that can be used instead. For instance, look at the help for \texttt{geom\_bar}:

\texttt{geom\_bar(mapping\ =\ NULL,\ data\ =\ NULL,\ stat\ =\ "count",\ \ \ position\ =\ "stack",\ ...,\ width\ =\ NULL,\ binwidth\ =\ NULL,\ na.rm\ =\ FALSE,\ \ \ show.legend\ =\ NA,\ inherit.aes\ =\ TRUE)}

There is a \texttt{stat\ =\ "count"} default argument there. An equivalent plot could be create using a \texttt{stat} layer instead.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{()}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }\KeywordTok{stat_count}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

Sometimes you'll need to modify the \texttt{stat}. For example, if you have summary data to begin with rather than the raw data, change the stat to \texttt{identity}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sumData <-}\StringTok{ }\NormalTok{titanicData }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(survived, sex) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{count =} \KeywordTok{n}\NormalTok{())}
\KeywordTok{ggplot}\NormalTok{(sumData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{y =}\NormalTok{ count, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-313-1} \end{center}

The \textbf{side-by-side barplot} can be created using the \texttt{position} aesthetic. \texttt{position} has the following values:

\begin{itemize}
\tightlist
\item
  \texttt{dodge} for side-by-side bar plot\\
\item
  \texttt{jitter} for continuous data with many points at same values\\
\item
  \texttt{fill} stacks bars and standardises each stack to have constant height\\
\item
  \texttt{stack} stacks bars on top of each other
\end{itemize}

As the value of this aestethic doesn't depend on the data, we do not place this argument in the \texttt{aes} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-314-1} \end{center}

If we use \texttt{position\ =\ fill} the bars will be stacked and the bars will be standardized to have constant height. This is really useful when there are an equal number of observations (or close to it) in each category on the x-axis (for instance with some Likert scale data surveys).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-315-1} \end{center}

Where else does \texttt{ggplot} shine? What if we want to make the same type of barplot for each \texttt{embarked} value? Here we can use \textbf{faceting}! This is just another layer on the plot call.

\texttt{facet\_wrap(\textasciitilde{}\ var)} creates a plot for each setting of the variable \texttt{var}. You can specify \texttt{nrow} and \texttt{ncol} or let R figure it out.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{sex), }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-316-1} \end{center}

\texttt{facet\_grid(var1\ \textasciitilde{}\ var2)} creates a grid of plots for each combination of the variables \texttt{var1} and \texttt{var2}. \texttt{var1} values go across rows and \texttt{var2} values go across columns. To have side-by-side or a vertical column of plots use \texttt{.\ \textasciitilde{}\ var2} or \texttt{var1\ \textasciitilde{}\ .}, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{sex), }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-317-1} \end{center}

Here we'll create a grid of plots for all \texttt{pclass} and \texttt{embarked} combinations (so easy!).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanicData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ survived)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{sex), }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(pclass }\OperatorTok{~}\StringTok{ }\NormalTok{embarked)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-318-1} \end{center}

Below is a recap of creating a barplot with \texttt{ggplot}:

\begin{itemize}
\item
  Use \texttt{ggplot(data\ =\ ...)\ +\ geom\_bar(aes(x\ =\ ...))}
\item
  With summary data, use \texttt{geom\_bar(stat\ =\ identity)}
\item
  Stacked barplot via \texttt{aes(fill\ =\ )}
\item
  Side-by-side barplot via \texttt{position\ =\ dodge}
\item
  Filled barplot via \texttt{position\ =\ fill}
\end{itemize}

Recapping plotting with \texttt{ggplot} so far:

\begin{itemize}
\item
  Can set local or global \texttt{aes}
\item
  Modify titles/labels by adding more layers
\item
  Use either stat or geom layer
\item
  Faceting (multiple plots) via \texttt{facet\_grid} or \texttt{facet\_wrap}
\item
  Only need \texttt{aes} if setting a mapping value that is dependent on the data (or you want to create a custom legend!)
\end{itemize}

Now we'll create similar plots to the \texttt{Base\ R} section for our numeric variables from the \texttt{CO2} uptake dataset. The process is the same:

\begin{itemize}
\tightlist
\item
  Create base object\\
\item
  Add geoms\\
\item
  Use aes to specify aspects of the plot
\end{itemize}

Let's start by creating a histogram. This can be done with \texttt{geom\_bar}. The easiest way to get started with a new \texttt{geom} is to look at the cheat sheet and the common \texttt{aes} values:

\begin{quote}
\texttt{c\ +\ geom\_histogram(binwidth\ =\ 5)}
\texttt{x,\ y,\ alpha,\ color,\ fill,\ linetype,\ size,\ weight}
\end{quote}

For \texttt{geom\_hist}, only \texttt{x} is really needed. Now we'll create the histogram for the \texttt{uptake} variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake))}
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-319-1} \end{center}

We might modify a few attributes of the plot that are not dependent on the data. Remember, these generally are set outside of the \texttt{aes} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{binwidth =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-320-1} \end{center}

To create the kernel smoother plot (smoothed histogram), we use the \texttt{geom\_density} layer. The common \texttt{aes} values from the cheat sheet are:

\begin{quote}
\texttt{c\ +\ geom\_density(kernel\ =\ "gaussian")}
\end{quote}

\begin{quote}
\texttt{x,\ y,\ alpha,\ color,\ fill,\ group,\ linetype,\ size,\ weight}
\end{quote}

As with \texttt{geom\_hist}, only \texttt{x\ =} is really needed. The kernel smoother for the \texttt{uptake} variable is created below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_density}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-321-1} \end{center}

This is not a pretty plot. The \texttt{fill} aesthetic is usually used with \texttt{geom\_density}. Here, we'll fill by the \texttt{Treatment} variable. We'll also change the \texttt{alpha} (transparency value, between 0 and 1) and the \texttt{adjust} (modify the kernel density calculations - 0.5 implies half the default bandwidth) aestethics.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{adjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Treatment))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-322-1} \end{center}

We may wish to stack the two density estimates on top of each other. Recall the \texttt{position} aestethic and the choices of \texttt{dodge}, \texttt{jitter}, \texttt{fill}, and \texttt{stack}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{adjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =} \StringTok{"stack"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Treatment))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-323-1} \end{center}

Plots can easily be overlayed, as long as they are on the same scale, by simply adding the appropriate layers. To put the histogram on the same scale as a density, we can set \texttt{y\ \ =\ ..density..} on the \texttt{geom\_hist} layer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density.., }\DataTypeTok{x =}\NormalTok{ uptake, }\DataTypeTok{fill =}\NormalTok{ Treatment)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{adjust =} \FloatTok{0.25}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =} \StringTok{"stack"}\NormalTok{, }
               \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake, }\DataTypeTok{fill =}\NormalTok{ Treatment)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-324-1} \end{center}

For these two plots, you can see it is a little beneficial to set global \texttt{aes()} options here. This would allow for an easy change of the \texttt{fill} variable for all layers!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ uptake, }\DataTypeTok{fill =}\NormalTok{ Treatment)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{adjust =} \FloatTok{0.25}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{position =} \StringTok{"stack"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To create a boxplot we can use \texttt{geom\_boxplot}. The common \texttt{aes} values from the cheat sheet are:

\begin{quote}
\texttt{f\ +\ geom\_boxplot()}
\end{quote}

\begin{quote}
\texttt{x,\ y,\ lower,\ middle,\ upper,\ ymax,\ ymin,\ alpha,\ color,\ fill,\ group,\ linetype,\ shape,\ size,\ weight}
\end{quote}

For \texttt{geom\_boxplot}, only \texttt{x\ =,\ y\ =} are really needed. To create a single vertical boxplot, use \texttt{x\ =\ ""}. We'll create side-by-side boxplots of \texttt{uptake} for each \texttt{Treatment} level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Treatment, }\DataTypeTok{y =}\NormalTok{ uptake))}
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-326-1} \end{center}

Sometimes it is useful to overlay the data values themselves on a boxplot in order to see the shape of the distribution more clearly. This can be done using the \texttt{geom\_jitter} layer. The \texttt{width} argument sets how widely the points are scattered (1 = width of box).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Treatment, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-327-1} \end{center}

Note that the order of the layers is important!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Treatment, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-328-1} \end{center}

If we wanted to create these same side-by-side boxplots for each value of another variable, we can again use faceting. For instance, we can use \texttt{facet\_wrap} with the \texttt{Type} variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Treatment, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"grey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{Type)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-329-1} \end{center}

Scatterplots can be created using \texttt{geom\_point}. The common \texttt{aes} values from the cheat sheet are:

\begin{quote}
\texttt{e\ +\ geom\_point()}
\texttt{x,\ y,\ alpha,\ color,\ fill,\ shape,\ size,\ stroke}
\end{quote}

For \texttt{geom\_point}, only \texttt{x\ =,\ y\ =} are really needed.

Below is a scatterplot of the \texttt{conc} and \texttt{uptake} variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ conc, }\DataTypeTok{y =}\NormalTok{ uptake)) }
\NormalTok{g }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-330-1} \end{center}

A trend lines can be easily added with \texttt{geom\_smooth}. The default is a smoothed line. To request the simple linear regression fit, we can use \texttt{method\ =\ lm}. Both of these lines are added to the plot below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ conc, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{  }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{col =} \StringTok{"loess"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{col =} \StringTok{"linear"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\DataTypeTok{name =} \StringTok{'Smoother'}\NormalTok{, }\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\StringTok{'loess'}\NormalTok{ =}\StringTok{ 'purple'}\NormalTok{, }\StringTok{'linear'}\NormalTok{ =}\StringTok{ 'red'}\NormalTok{), }
                      \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{'GAM'}\NormalTok{, }\StringTok{'Linear'}\NormalTok{), }\DataTypeTok{guide =} \StringTok{'legend'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'loess' and formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-331-1} \end{center}

You may notice something a bit odd in the call above. The value of \texttt{col} (color) was set inside the \texttt{aes} function even though it was not set to a value corresponding to the data set. This is an exception to that rule. If you want to utilize \texttt{ggplot}`s automatic creation of legends, you can use the method above where labels are given to the color for each line ('loess' and `linear', respectively). \texttt{scale\_colour\_manual} is then used to create the legend. Notice the \texttt{values} being set assign colors to the lines here.

Text can also be added to the plot with \texttt{geom\_text}. The correlation is added to the plot in a similar way to how it was done in the \texttt{Base\ R} section. The \texttt{size} argument being set to a value larger than 1 makes the text bigger.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlation <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(CO2}\OperatorTok{$}\NormalTok{conc, CO2}\OperatorTok{$}\NormalTok{uptake)}
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ conc, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm, }\DataTypeTok{col =} \StringTok{"Red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\DataTypeTok{x =} \DecValTok{750}\NormalTok{, }\DataTypeTok{y =} \DecValTok{10}\NormalTok{, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{, }
            \DataTypeTok{label =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"Correlation = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(correlation, }\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-332-1} \end{center}

Using text for the points is very easy with \texttt{geom\_text}. Just use this layer with a \texttt{label} variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ conc, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ Plant))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-333-1} \end{center}

Again, if we want to create similar scatterplots for each value of another variable, we can do so with faceting. (Note: the \texttt{cut} function is very useful for categorizing a numeric variable.) Here we create similar scatterplots for each level of the \texttt{Treatment} variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(CO2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ conc, }\DataTypeTok{y =}\NormalTok{ uptake)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ Type), }\DataTypeTok{size =} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{Treatment)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-334-1} \end{center}

Lastly, we saw the useful \texttt{pairs} function from \texttt{Base\ R}. There are many extension packages for \texttt{ggplot} that give great functionality. The \texttt{GGally} package has a function called \texttt{ggpairs} that is similar to pairs but retains the same syntax and functionality of \texttt{ggplot}. Remember to install the package if you haven't done so and then read it in with the library command (or call the function directly using \texttt{::}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GGally)}
\KeywordTok{ggpairs}\NormalTok{(iris, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =}\NormalTok{ Species, }\DataTypeTok{alpha =} \FloatTok{0.4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{StatisticalMethods_files/figure-latex/unnamed-chunk-335-1} \end{center}

To recap numerical variable plots using \texttt{ggplot}, the most common \texttt{geom}s are:

\begin{itemize}
\item
  Histogram (\texttt{geom\_hist}), Density (\texttt{geom\_density})
\item
  Boxplot (\texttt{geom\_boxplot})
\item
  Scatter plot (\texttt{geom\_point}), Smoothers (\texttt{geom\_smooth})
\item
  Jittered points (\texttt{geom\_jitter})
\item
  Text on plot (\texttt{geom\_text})
\end{itemize}

The syntax and functionality of \texttt{ggplot} is recapped below again.

\begin{itemize}
\item
  Can set local or global \texttt{aes}
\item
  Modify titles/labels by adding more layers
\item
  Use either stat or geom layer
\item
  Faceting (multiple plots) via \texttt{facet\_grid} or \texttt{facet\_wrap}
\item
  Only need \texttt{aes} if setting a mapping value that is dependent on the data (or you want to create a custom legend!)
\item
  General syntax:
\end{itemize}

\texttt{ggplot(data\ =\ \textless{}DATA\textgreater{})\ +}\\
\hspace*{0.333em}\hspace*{0.333em}\texttt{\textless{}GEOM\_FUNCTION\textgreater{}(mapping\ =\ aes(\textless{}MAPPINGS\textgreater{}),\ stat\ =\ \textless{}STAT\textgreater{},\ position\ =\ \textless{}POSITION\textgreater{})\ +}\\
\hspace*{0.333em}\hspace*{0.333em}\texttt{\textless{}COORDINATE\_FUNCTION\textgreater{}\ +}\\
\hspace*{0.333em}\hspace*{0.333em}\texttt{\textless{}FACET\_FUNCTION\textgreater{}\ +}\\
\hspace*{0.333em}\hspace*{0.333em}\texttt{\textless{}LABEL\_FUNCTION\textgreater{}}\\
\texttt{)}

There are many other useful extensions such as \texttt{gganimate}, \texttt{ggthemes}, \texttt{ggmap}, and \texttt{ggrepel}.

\hypertarget{sas-2}{%
\subsection{SAS}\label{sas-2}}

This section is meant to instruct on how to produce common numerical and graphical summaries in SAS.

\hypertarget{point-estimates}{%
\chapter{Point Estimates}\label{point-estimates}}

Learning objectives for this lesson:
- How to estimate a mean
- Definition of ``convenience sample''
- Definition of ``systematic sample''
- Benefits/drawbacks to both approaches
- Understand how to estimate a mean
- Understand how to estimate a quantile
- Understand implicit assumptions for these approaches

\hypertarget{estiamte-with-means}{%
\section{Estiamte with means}\label{estiamte-with-means}}

\hypertarget{experiment-background}{%
\subsection{Experiment background}\label{experiment-background}}

Someone wants to know how much of something they need to satisfy some population
To get a good estimate of this, we can use the average amount for each one and then multiply by the whole population

\hypertarget{estimate-with-quantiles}{%
\section{Estimate with quantiles}\label{estimate-with-quantiles}}

\hypertarget{experiment-background-1}{%
\subsection{Experiment background}\label{experiment-background-1}}

Big Deborah's is making new packaging for their cookies. The engineer responsible for the new desing needs to make sure that the packaging fits the new cookies. While the cookie manufacturing process is standardized, there's inevitably some degree of variation in cookie size. After discussing the issue with corporate, the engineer decides that a the new cookie sleeves should be large enough to fit 95\% of cookies that are baked. (The largest five percent will be marketed separately as ``JUMBO'' cookies.)

\hypertarget{define-the-object-of-the-experiment}{%
\subsection{Define the object of the experiment}\label{define-the-object-of-the-experiment}}

The Engineer is tasked with determining how large the cooke sleeve needs to be. There's no way for her to know the size of every cookie that Big Deborah's has made (or will make going forward!), so she'll need to collect data on existing cookies to inform her cookie sleeve size determination.

\hypertarget{select-appropriate-response-variables}{%
\subsection{Select appropriate response variables}\label{select-appropriate-response-variables}}

If the maximum distance from any one point on the (round) cookie's perimeter to any other point is smaller than the diameter of the cookie sleeve, then the cookie will fit. This makes ``cookie diameter'' a good measure for this test. It is easy to measure for each cookie and is directly relevant to the experiment's objective.

{[}probably have something in here about {]}

\hypertarget{identify-sources-of-variation}{%
\subsection{IDentify sources of variation}\label{identify-sources-of-variation}}

While the manufacturing process is standardized, there is variation in size from one cookie to the next. This is one source of variation. The engineer isn't sure of any others. However, she knows that cookies are made in multiple factories, and that each factory has multiple ovens. Ovens and factories could also be sources of variation.

\hypertarget{choose-an-experimental-design}{%
\subsection{Choose an experimental design}\label{choose-an-experimental-design}}

The Engineer knows that she needs to look at multiple cookies, since she knows that there is variation in diameter from one cookie to the next. One option would be to just use the remaining cookies in the box she has in her office (22 of the 25-count box remain). {[}something about convenience sample{]} However, she knows that cookies from the same oven are typically packaged together. If there is variation from one oven to the next, looking at the cookies she has in her office may not tell the whole story.

Instead, she chooses to take every 20th cookie manufactured off the assembly line until she gets 500 cookies. {[}something about systematic sample{]}

\hypertarget{perform-the-test}{%
\subsection{Perform the test}\label{perform-the-test}}

The day of the test comes, and the Engineer starts collecting cookies. However, problems arise! The plan has to shut down half-way through, so she only gets 431 cookies instead of the 500 she thought she would. However, she measures the diameters of each cookie and records the data in a spreadsheet.

\hypertarget{statistically-analyze-the-data}{%
\subsection{Statistically analyze the data}\label{statistically-analyze-the-data}}

The initial plan had been to rank-order the 500 cookies and estimate the 95th percentile using the diamter of the 475th largets cookie. Since we didn't get all of our data, we have to improvise. 431 doesn't neatly yield a value such that exactly 95\% are less than or equal and 5\% are greater than or equal. One option is to choose the 410th largest cookie to estimate our percentile. Slightly more than 95\% of cookies will have smaller diameters than this. Alternatively, we could interpolate between the 409th and 410th cookies. {[}reasons and logic and math for each of these{]}

\hypertarget{draw-conclusions}{%
\subsection{Draw conclusions}\label{draw-conclusions}}

Based on this study, the Engineer concludes that a cookie sleeve large enough for a cookie of diameter XX will be big enough to contain 95\% of Big Deborah cookies.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

\begin{itemize}
\tightlist
\item
  pros and cons to the approach chosen
\item
  generalizing to other types of point estimates
\end{itemize}

\hypertarget{accounting-for-uncertainty}{%
\chapter{Accounting for Uncertainty}\label{accounting-for-uncertainty}}

Some \emph{significant} applications are demonstrated in this chapter.

\hypertarget{example-one}{%
\section{Example one}\label{example-one}}

\hypertarget{example-two}{%
\section{Example two}\label{example-two}}

\hypertarget{CI}{%
\chapter{Inference via Confidence Intervals for One Sample}\label{CI}}

There are many ways to build confidence intervals for sample proportions. Here are a few:

\hypertarget{the-normal-approximation}{%
\section{The normal approximation}\label{the-normal-approximation}}

This is the basic interval they've taught in introductory statistics courses since time immamorial. Or at least the past few decades, I'd have to know the history of Stats Ed to give the real timeframe. Anyway, this confidence interval uses the fact from the Central Limit Theorem, that, as \(n \rightarrow \infty\), the sampling distribution for \(\hat\pi = x/n\) closely resembles a Normal distribution.

Based on that, you get the equation:

\[\hat\pi \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat\pi (1 - \hat\pi)}{n}}\]

\hypertarget{analog-ci}{%
\subsection{Analog CI}\label{analog-ci}}

We can build this CI in R pretty easily by inputting the values for the sample size, \(n\), and the number of ``successes'' or ``1''s from our binary response variable. One example from class discusses a poll of 2500 people with 400 responding ``Satisfactory''. For a 90\% confidence interval, we have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{2500}
\NormalTok{x <-}\StringTok{ }\DecValTok{400}
\NormalTok{pihat <-}\StringTok{ }\NormalTok{x}\OperatorTok{/}\NormalTok{n}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.1} \CommentTok{# 90% CI --> alpha = 1 - .9 = 0.1}

\NormalTok{lower_bound <-}\StringTok{ }\NormalTok{pihat }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((pihat }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{pihat)}\OperatorTok{/}\NormalTok{n))}
\NormalTok{upper_bound <-}\StringTok{ }\NormalTok{pihat }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((pihat }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{pihat)}\OperatorTok{/}\NormalTok{n))}

\KeywordTok{c}\NormalTok{(lower_bound, upper_bound)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1479397 0.1720603
\end{verbatim}

\hypertarget{easy-mode}{%
\subsection{Easy mode}\label{easy-mode}}

But it's much easier to just use the \texttt{binom} library, which contains the function \texttt{binom.confint()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("binom")}
\KeywordTok{library}\NormalTok{(binom)}

\KeywordTok{binom.confint}\NormalTok{(}\DataTypeTok{x =} \DecValTok{400}\NormalTok{, }\DataTypeTok{n =} \DecValTok{2500}\NormalTok{, }\DataTypeTok{conf.level =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{method =} \StringTok{"asymptotic"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       method   x    n mean     lower     upper
## 1 asymptotic 400 2500 0.16 0.1479397 0.1720603
\end{verbatim}

Much easier! But now that we're using \texttt{binom.confint()}, we discover that we have to specify \texttt{method\ =\ "asymptotic"}. But that implies that there are alternatives! And indeed, if we just remove that statement, we see that there are almost a DOZEN different methods that \texttt{binom.confint()} will compute for you!

\hypertarget{other-types-of-binomial-confidence-intervals}{%
\section{Other types of binomial confidence intervals}\label{other-types-of-binomial-confidence-intervals}}

First off, most of these aren't useful in most cases. They're in there because (1) they're not very hard to program, so the authors figured, ``Why not?'' and (2) in most cases, there \emph{is} at least one circumstance where each one is the best option. (Or they're included for historical reasons.)

\hypertarget{exact-cis-aka-clopper-pearson}{%
\subsection{Exact CIs, aka Clopper-Pearson}\label{exact-cis-aka-clopper-pearson}}

For one simple example, recall the assumption that we always have to make for our Normal approximation method: \(n * \hat\pi > 5\) and \(n * (1 - \hat\pi) > 5\). This is required when we use the Normal approximation. It means we can't build CIs for small-ish samples. But other methods don't have this problem!

\texttt{method\ =\ "exact"} uses what's called the Clopper-Pearson method, which uses the Binomial distribution to calculate an ``exact'' confidence interval rather than rely on an approximation.

While being ``exact'' sounds better than ``approximate'', the truth of the matter is that the Clopper-Pearson interval is generally wider than it needs to be, meaning you get a less precise interval:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}

\KeywordTok{binom.confint}\NormalTok{(}\DataTypeTok{x =} \DecValTok{400}\NormalTok{, }\DataTypeTok{n =} \DecValTok{2500}\NormalTok{, }\DataTypeTok{conf.level =} \FloatTok{0.9}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\StringTok{`}\DataTypeTok{CI Width}\StringTok{`}\NormalTok{ =}\StringTok{ }\NormalTok{upper }\OperatorTok{-}\StringTok{ }\NormalTok{lower) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(method, lower, upper, }\StringTok{`}\DataTypeTok{CI Width}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\StringTok{`}\DataTypeTok{CI Width}\StringTok{`}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           method     lower     upper   CI Width
## 1          bayes 0.1480550 0.1721635 0.02410856
## 2        cloglog 0.1481500 0.1722628 0.02411279
## 3        profile 0.1481871 0.1723036 0.02411651
## 4         wilson 0.1483082 0.1724269 0.02411870
## 5         probit 0.1482369 0.1723573 0.02412042
## 6     asymptotic 0.1479397 0.1720603 0.02412053
## 7          logit 0.1483044 0.1724312 0.02412679
## 8  agresti-coull 0.1483026 0.1724325 0.02412988
## 9            lrt 0.1481877 0.1723265 0.02413880
## 10         exact 0.1480388 0.1725544 0.02451559
## 11     prop.test 0.1459601 0.1750977 0.02913765
\end{verbatim}

Since we have a large sample, the differences aren't very large, but there are times when you want every ounce of precision you can get!

\hypertarget{bayesian-intervals}{%
\subsection{Bayesian intervals}\label{bayesian-intervals}}

Bayesian statistics is a school of thought that says we should try to incorporate our prior knowledge about a problem when making a decision instead of letting the data stand on its own.I don't want to get into why some folks prefer Bayesian intervals, but if you want to, just specify \texttt{method\ =\ "bayes"} to get a Bayesian CI.

\hypertarget{a-good-general-use-ci}{%
\subsection{A good general-use CI}\label{a-good-general-use-ci}}

My go-to for a simple binomial confidence interval is the Agresti-Coull method, \texttt{method\ =\ "agresti-coull"}. It's one of the weirder ones (Seriously, go look at the equation for it!), but generally performs as well or better than the competition across most scenarios. It's more precise than \texttt{method\ =\ "exact"}, doesn't fail in small samples like \texttt{method\ =\ "asymptotic"}, and doesn't rely on a Bayesian approach.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{HT}{%
\chapter{Inference via Hypothesis Tests for One Sample}\label{HT}}

\hypertarget{example-detection-probability}{%
\section{Example: Detection Probability}\label{example-detection-probability}}

An important problem for the US Army is protecting soldiers in forward positions. One of main threats to these soldiers are explosive projectiles launched from great distances. These are often refered to as ``indirect fires'', since the folks launching the projectiles (``firing'' the projectiles) may not have direct visibility (``line of sight'') to their target. For example, artillery like the US M777 Light Towed Howitzer can fire projectiles over 40 km, far beyond the distance a soldier operating the machine could aim it. Instead, the gun crew relies on information about where to shoot provided to them by other units on the battlefield. This allows them to effect the battle from far away and without alerting their target.

To protect soldiers from similar threats, the US Army developed the Q-53 Counterfire Radar. One of this system's primary functions is to detect incoming indirect fires and pinpoint the source of those shots. Because soliders will be relying on the Q-53 in combat, the US Government tested it extensively to better understand how it would perform in an operational setting. The testers fired shells from systems that closely resembled the artillery that might be seen on the battlefield while actual soldiers operated a Q-53. For each shell that was fired, the testers recorded whether or not the Q-53 crew was able to detect the projectile with their system.

Using data from this test, evaluators were able to the likelihood that the Q-53 would detect an incoming projectile.

\hypertarget{understanding-system-performance}{%
\subsection{Understanding system performance}\label{understanding-system-performance}}

Let's take a step back and consider the data we're getting from this test and try to understand why the testers did things the way they did.

First, let's consider the goal of this test. At the end of the day, the US Army wants to know how effective the Q-53 is at it's job. This job is to help protect soldiers by giving them early warning if they're under attack from indirect fires like artillery. Therefore, it's vital that the Q-53 detect incoming projectiles with high probability.

Once we understand this, the choice to measure the detection probability for incoming projectiles is completley logical. This measure ties directly to the goal of the experiment. You won't always be able to find a response variable that ties so directly to the goal of your test, but when you do, rejoice.

There are some other measures the testers could've used. For example, they could've looked at how long a projectile was detected prior to its impact on the target. Similarly, they could've measured the distance from the target at time of detection. Both of these measures would give more detailed information than detection alone. However, they'd be harder to measure precisely. Addiing additional instrumentation to the Q-53 and to the test projectiles being fired at the range would add elements to the experiment that would make it less realistic. Perhaps instrumenting the projectiles would make them easier to detect. Regardless, the testers deemed the detection probability adequate to make their assessment of the system.

\hypertarget{data-analysis}{%
\subsection{Data Analysis}\label{data-analysis}}

Having identified their response variable, the testers collected data on which projectils were detected. Table 1 shows the first 20 shots collected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"datasets/Experimental/counterfire-radar/counterfire-radar.csv"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(Mission }\OperatorTok{%in%}\StringTok{  }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{))}

\KeywordTok{hist}\NormalTok{(tb}\OperatorTok{$}\NormalTok{Detection)}
\end{Highlighting}
\end{Shaded}

\includegraphics{StatisticalMethods_files/figure-latex/unnamed-chunk-29-1.pdf}

It's clear that the system detects these projectiles with a high probability, but what conclusions can we draw from these data? Does the system detect projectiles at a high enough rate to be useful to a commander in the field?

Typically, you'll want to think about these questions before designing a test or analyzing your data. In the case of this counterfire radar, the US Army has specified in requirements documents what it deems to be ``good enough'' in terms of detection probability. Suppose the Army requires that the system be able to detect at least 80\% of incoming projectiles.

Using that requirement, we can compare the probabiltiy of detection we observed in our sample and get an idea of whether or not the system is good enough:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(tb)}
\NormalTok{phat <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(tb}\OperatorTok{$}\NormalTok{Detection)}

\NormalTok{phat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.804878
\end{verbatim}

Let the true probability of detection for this radar system be \(\pi\), a value somewhere between \(0\) and \(1\). Then our estimate for \(\pi\) is denoted:

\[\hat\pi = \frac{1}{n}\sum_{i = 1}^n x_i\]

In this equation, \(x_i\) is 1 or a 0 depending on whether or not we detected the \(i\)th projectile, and \(n\) is the total number of observations we have. We can use this notation to state the research problem we describe above more formally: We want to determine if \(\pi > 0.8 = 80%
\). To do this, we're going to use our estimat, \(\hat \pi\).

The simplest approach is to just say that \(\hat \pi\) is our best guess for \(\pi\), so let's just ask if \(\hat \pi> 0.8\). Based on our data above, we have: \$\hat\pi = \$0.804878. This is larger than 0.8, so it looks like we're good to go!

Unforunately, \(\hat \pi\) is an \emph{estimate} of \(\pi\) and not the true value. It is based on a \emph{random sample}, which are subject to variability. Before drawing any conclusions, it's important to ask the question, ``How sure are we that \(\hat\pi\) is close to \(\pi\)?''

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

The framework used throughout modern science to answer these sorts of question is known as \emph{Null Hypothesis Significance Testing} (NHST), or more concisely, \emph{hypothesis testing}. The basic approach is to frame our research question as a comparison between two possible realities and then decide, based on the data, if one of those realities is sufficiently implausible to discard as an actual possibility.

For our application, the two competing realities are one in which the radar's performance meets the desired performance of the Army (that is, \(\pi > 0.8\)) and one where it does not (\(\pi \le 0.8\)). These possibilities are derived directly from our research question, and are typically phrased explicitly as a \emph{Null Hypothesis} (denoted \(H_0\)) and an \emph{Alternative Hypothesis} (denoted \(H_1\) or \(H_a\)). The null hypothesis is typically written in such a way that, if we can prove it is not true, we will have learned important information. For this example, if we can show that the system's probability of detection isn't worse than \(0.8\), we'll know that it's at least good enough to meet the Army's desired level of performance. Therefore, we write our null and alternative thus:

\[H_0: \pi \le 0.8\]
\[H_1: \pi > 0.8\]

Our goal is to determine, based on the sample we've collected, whether \(H_0\) is plausible. There are many ways that statisticians have developed over the years of determining this, but we'll focus on one for now.

The first thing to do is consider our data, which are records of whether or not the radar system detected each of the 41 projectiles shot during the test event. We must make some general assumptions about these data if we're to get anyhwere. Plausible assumptions include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  One shot is independent from the next
\item
  The underlying probability of detection for each shot is the same
\end{enumerate}

If these assumptions hold, than each observation is a Bernoulli random variable with \(P(Detect) = \pi\) and \(P(NoDetect) = 1 - \pi\).

This is very useful knowledge, since we know a lot about how Bernoulli random variables behave. For example, software can simulate Bernoulli random very easily. The function \texttt{rbernoulli} in the \texttt{purrr} package will simulate the outcomes of \texttt{n} random draws from a Bernoulli distribution with probability of success \texttt{pi}. The function will output \texttt{TRUE} for each success and \texttt{FALSE} for each failure. For our example, \texttt{TRUE} will represent the case where the radar detected the projectile and \texttt{FALSE} will represent the case where the radar failed to detect the projectile. This allows us to simulate what might have happened in our experiment. To do this, we need to provide values for \texttt{n} and \texttt{pi}. Luckily, we know the number of observations we had in our experiment (\$n = \$ 41), and we can postulate a value for \texttt{pi} based on our hypotheses above.

Recall that our goal is to determine whether \(H_0\) is plausible. If the outcome of our actual data is consistent with what the outcome might have looked like if \(H_0\) was true, then we'll conclude that \(H_0\) is plausible. So let's plug in a value for \texttt{pi} based on the values of \(\pi\) given in \(H_0\). The highest value of \(\pi\) included in \(H_0\) is \(0.8\), so we'll go with that. (If we chose a lower value, our comparison would be less compelling, so when we take this approach, we'll always choose the value under \(H_0\) that's closest to the values included under \(H_a\).)

We can now simulate what the outcome of our experiment might have looked like if \(H_0\) was true:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{20200401}\NormalTok{)}
\NormalTok{simulated_trials <-}\StringTok{ }\NormalTok{purrr}\OperatorTok{::}\KeywordTok{rbernoulli}\NormalTok{(n, }\FloatTok{.8}\NormalTok{)}

\KeywordTok{mean}\NormalTok{(simulated_trials)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8292683
\end{verbatim}

But this is just one possible outcome. Each trial is random, so the overall outcome is random, too. This means that we should look at many possible results from the experiment to get a more complete picture of what things might look like under \(H_0\). The code below simulates 10,000 experiments and calcualtes the sample proportion for each. These sample proportions are then plotted in a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nsim <-}\StringTok{ }\DecValTok{10000}
\NormalTok{simulation_results <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\StringTok{`}\DataTypeTok{Sample Statistic}\StringTok{`}\NormalTok{ =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, nsim))}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nsim)\{}
\NormalTok{  simulation_results}\OperatorTok{$}\StringTok{`}\DataTypeTok{Sample Statistic}\StringTok{`}\NormalTok{[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(purrr}\OperatorTok{::}\KeywordTok{rbernoulli}\NormalTok{(n, }\FloatTok{.8}\NormalTok{))}
  
\NormalTok{\}}

\NormalTok{simulation_results  }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{`}\DataTypeTok{Sample Statistic}\StringTok{`}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{StatisticalMethods_files/figure-latex/unnamed-chunk-32-1.pdf}

\hypertarget{wald-tests}{%
\subsection{Wald tests}\label{wald-tests}}

Luckily, we have tools available to help us answer that question! Specifically, we can use the \emph{standard error}, which was introduced in Chapter 4. Recall that the equation for the standard error of \(\hat\pi\) is

\[SE(\hat\pi) = \sqrt{\frac{\hat\pi (1 - \hat\pi)}{n}}\]

For our observed data, \(SE(\hat\pi) =\) 0.06. Using these values, we can determine with greater precision whether the true performance of the radar system meets the desired level of performance.

--\textgreater{}

--\textgreater{}

--\textgreater{}

\hypertarget{twocategorical}{%
\chapter{Inference for Two Categorical Variables}\label{twocategorical}}

We have finished a nice book.

\hypertarget{anova}{%
\chapter{One-Way ANOVA}\label{anova}}

Learning objectives for this lesson:
- Write one-way ANOVA model
- Define terms
- state assumptions
- interpret results
- Interpret ANOVA table
- Describe SSE, SST, MSE
- F-statistic
- degrees of freedom
- understand how all of these interrelate
- Understand how to compare mulitple group means how ANOVA is similar/different to t-tests
- Understand partitioning of variation and coefficient of determination

\hypertarget{motivating-example}{%
\section{Motivating example}\label{motivating-example}}

The United States Air Force Academy has 24 sections of Calculus I, taught by three different types of instructors: In-uniform instructors, full-time civilian instructors, and visiting faculty. The Dean of Students wants to give students the best experience possible and make sure that all three types of instructors are doing a good job. There are plausible reasons why any one of the three could be doing well: In-uniform instructors are all members of the Air Force, and students may be extra attention in these classes because they know that these instructors rank above them in their chain of command. On the other hand, full-time instructors have been aroudn the Academy for many years and understand the Cadets and their workloads. Alternatively, visiting facutly tend to come from prestigeous institutions and may be familiar with more recently-developed pedagogical techniques. Regardless, the Dean wants to understand if there is any variation in end-of-semester grades of classes taught by these three types of instructors. At the end of the semester, she collects the average grades from each of the 24 sections. How can she go about investigating this question?

Recall from Chapter 6 that we can use t-tests to compare two group means. In this case, we'd like to do a comparison across three groups, and instead of looking at a direct comparison of one group to another, what the Dean is interested in is whether there's an \emph{overall} difference across the three groups.

One option might be to just do a bunch of different t-tests. We could first compare classes taught by in-uniform instructors to classes taught by full-time civilians, then compare the classes taught by the in-uniform instructors to the classes taugth by the visiting instructors, and then finally compare the classes taugth by the full-time civilains with the classes taught by the visiting facutly. We'd end up with three p-values, each addressing different questions than the one we initially set out to answer.

We could do the same thing, except comparing courses taught by one type of instructor to the combined group of courses taught by the other two, and this gets a bit closer to the mark. But we're still doing three tests that individually fail to answer the Dean's question.

What we'd like instead is a single hypothesis that we could test that direclty gets at the Dean's concern about whether the three types of instructors were producing end-of-semester grades that were, on average, the same. {[}Need to make that motivation clearer above.{]}

\hypertarget{simple-model-for-the-data}{%
\section{Simple model for the data}\label{simple-model-for-the-data}}

Narrative explanation that instructor type might matter, there shold be some variation from class to class.
- write some things in greek, including model without any difference by instructor type
- wirte model with differences by instructor type
- note that we can use Gaussian errors b/c Academy grades do actually tend to be centered around a C, particularly for classes like Calc
- discuss model assumptions in general sense

\hypertarget{exploratory-analysis}{%
\section{exploratory analysis}\label{exploratory-analysis}}

\begin{itemize}
\tightlist
\item
  course-to-course variability is expected
\item
  maybe show a plot of it or something
\item
  visualize groups using box-and-whisker plots
\end{itemize}

\hypertarget{sources-of-variation-1}{%
\section{sources of variation}\label{sources-of-variation-1}}

Things like student population, time of day, etc. But we'll throw this all into an error term and focus on the main one, instructor type

\hypertarget{statistical-model-and-analysis}{%
\section{statistical model and analysis}\label{statistical-model-and-analysis}}

\begin{itemize}
\tightlist
\item
  ANVOA model explicit w/ assumptions
\item
  variation around overall mean w/ no groups
\item
  variation around group means
\item
  introduce idea of reference level
\end{itemize}

\hypertarget{compare-analyses}{%
\section{compare analyses}\label{compare-analyses}}

\begin{itemize}
\tightlist
\item
  t-test methods from above
\item
  ANOVA method
\item
  compare and contrast results, interpretations, etc.
\end{itemize}

\hypertarget{multiway}{%
\chapter{Multi-way ANOVA}\label{multiway}}

We have finished a nice book.

\hypertarget{block}{%
\chapter{Block Designs}\label{block}}

We have finished a nice book.

\hypertarget{regression}{%
\chapter{Regression Models}\label{regression}}

We have finished a nice book.

\hypertarget{glm}{%
\chapter{The General Linear Model}\label{glm}}

We have finished a nice book.

\hypertarget{mixedmodels}{%
\chapter{Mixed Models}\label{mixedmodels}}

We have finished a nice book.

\hypertarget{repeatedmeasures}{%
\chapter{Split Plot and Repeated Measures Designs}\label{repeatedmeasures}}

We have finished a nice book.

\hypertarget{logistic}{%
\chapter{Logistic Regression and Generalized Linear Models}\label{logistic}}

\hypertarget{stuff-here}{%
\section{Stuff here}\label{stuff-here}}

We have finished a nice book.

\hypertarget{glmm}{%
\chapter{Generalized Linear Mixed Models}\label{glmm}}

We have finished a nice book.

\hypertarget{learningobj}{%
\chapter{Appendix - Learning Objectives}\label{learningobj}}

\hypertarget{book-level}{%
\section{Book-level}\label{book-level}}

After reading this book you will be able to:

\begin{itemize}
\item
  identify relevent sources of variability for a potential study and, if applicable, utilize principles of design to plan a reasonable experiment to help answer questions of interest

  \begin{itemize}
  \tightlist
  \item
    covariates
  \item
    noise variables
  \item
    random effects
  \item
    variance of indidvidual observations vice variance of summary statistics
  \item
    randomization
  \item
    systematic variation of factors/covariates
  \item
    factor identifiability
  \item
    understand issues surrounding multiple comparisons

    \begin{itemize}
    \tightlist
    \item
      Bonferroni correction
    \item
      at least one other method (Tukey?)
    \end{itemize}
  \item
    tradeoffs from replication within groups vice getting more groups
  \item
    compare and contrast methods for designing an experiment when the goal of a study is prediction versus when the goal is statistical inference
  \end{itemize}
\item
  explain the general concept of point estimation and how to account for sampling variability

  \begin{itemize}
  \tightlist
  \item
    definition
  \item
    identify the right point estimate for your response variable of interest
  \item
    estimating uncertainty for point estimates

    \begin{itemize}
    \tightlist
    \item
      normal approximation
    \item
      bootstrap CI
    \item
      others?
    \end{itemize}
  \item
    Types of point estimates:

    \begin{itemize}
    \tightlist
    \item
      means

      \begin{itemize}
      \tightlist
      \item
        Simple effects
      \item
        interaction effects
      \item
        main effects
      \end{itemize}
    \item
      standard deviations/variance components
    \item
      correlation coefficients
    \item
      quantiles/percentiles from distributions
    \item
      probabilities
    \item
      parameters of a distribution
    \item
      model parameters
    \end{itemize}
  \end{itemize}
\item
  describe relevant properties of random variables and probabilities

  \begin{itemize}
  \tightlist
  \item
    Distinguish between mutually exclusive and independent events.
  \item
    Calculate probability for a given scenario, either numerically or using a Venn diagram.
  \item
    Apply the General Addition Rule to solve probability problems.
  \item
    Apply the Rules for Probability Distributions to create a probability distribution for a given scenario.
  \item
    Use the complement of an event to solve probability problems.
  \item
    Apply the Multiplication Rule for Independent Processes to solve probability problems.
  \item
    random variables

    \begin{itemize}
    \tightlist
    \item
      have a defined set of possible outcomes (``sample space'')
    \item
      Discrete vs.~continuous RVs
    \item
      others???
    \end{itemize}
  \item
    probabilities/PDFs

    \begin{itemize}
    \tightlist
    \item
      between 0 and 1 inclusive
    \item
      sum of probability of all possible events is 1
    \item
      \(P(A) + P(A^c) = 1\), where \(A\) is an event and \(A^c\) is the complement of A
    \end{itemize}
  \end{itemize}
\item
  explain the importance of statistical distributions when conducting statistical inference

  \begin{itemize}
  \tightlist
  \item
    normal distribution and approximations plus properties

    \begin{itemize}
    \tightlist
    \item
      robustness
    \item
      generality
    \item
      CLT
    \end{itemize}
  \item
    costs and benefits of using nonparametric approaches
  \end{itemize}
\item
  describe the fundamental inferential techniques of hypothesis testing and confidence intervals as well as compare and contrast their uses and interpretations
\item
  identify a null and alternative for a given problem
  - interpret hypotheses
  - characterize the test statistic under the null
  - explain what a rejection region and be able to identify one
  - define statistical power
  - calculate statistical power for one- and two-sample tests of continuous and binary random variables
  - define statistical confidence\\
  - identify when using a CI and NSHT will result in the same conclusion
  - explain when you can use a confidence interval to test for differences (e.g., comparing a single point estimate to a threshold) and when you can't (e.g., when you have CIs for two different means)
\item
  choose appropriate numerical summaries and graphical displays for a set of data and create these using software

  \begin{itemize}
  \tightlist
  \item
    when to use tables vs.~a picture
  \item
    types of graphical displays

    \begin{itemize}
    \tightlist
    \item
      bar charts
    \item
      pie charts
    \item
      plotting data vice just predictions/conclusions
    \item
      when to include uncertainty bounds
    \item
      five-number summaries
    \item
      means vs.~medians
    \item
      general plotting recommendations
    \item
      use of colors in you plots (discrete vs.~divergent vs.~continuous color scales, gray-scale, color-blind-friendly scales)
    \end{itemize}
  \item
    use of annotations
  \item
    general graphical design philosophy (building a chart to illustrate a conclusion)
  \item
    trade-offs between detail and interpretability
  \item
    not screwing up your axes
  \end{itemize}
\item
  fit statistical models in software and interpret their output

  \begin{itemize}
  \tightlist
  \item
    Which PROCs from SAS? REG, GLM, MIXED, GLIMMIX, others??
  \item
    \texttt{lm()}, \texttt{glm()}, \texttt{anova()} \ldots{}. \texttt{broom}? \texttt{modelr}? \texttt{ciTools}?
  \item
    p-values, point estimates, standard errors, f-statistics, chi-square-statistics, degrees of freedom, SS/MS, residual plots
  \end{itemize}
\item
  connect common statistical methods under the linear model framework

  \begin{itemize}
  \tightlist
  \item
    Write statistical models using matrix representaiton
  \item
    identify models written in matrix representation with their representation in software
  \item
    identify when models written in different notation are the same or different
  \item
    describe when specific models will give you the same results

    \begin{itemize}
    \tightlist
    \item
      ANOVA w/ 2 factors and a t-test or a SLR
    \item
      ANCOVA and MLR
    \item
      random effects vs.~fixed effects
    \item
      split plots vs.~more general mixed models
    \item
      logistic regression w/ categorical factors vice contingency table analysis
    \end{itemize}
  \item
    discuss differences in assumptions associated with ANOVA vice SLR/MLR
  \end{itemize}
\item
  articulate the scope of inferential conclusions in light of the method of data collection, the experimental design used, the assumptions made, and the statistical analysis applied

  \begin{itemize}
  \tightlist
  \item
    limitations due to sampling/sample frame
  \item
    missing data
  \item
    modeling assumptions
  \item
    sampling assumptions
  \item
    requirements for causal inference
  \end{itemize}
\end{itemize}

\hypertarget{topic-level}{%
\section{Topic-level}\label{topic-level}}

\hypertarget{chapter-2---sampling-design-and-exploratory-data-analysis}{%
\subsection{Chapter 2 - Sampling, Design, and Exploratory Data Analysis}\label{chapter-2---sampling-design-and-exploratory-data-analysis}}

\hypertarget{chapter-3---point-estimation}{%
\subsection{Chapter 3 - Point Estimation}\label{chapter-3---point-estimation}}

\hypertarget{chapter-4---accounting-for-uncertainty-in-estimation}{%
\subsection{Chapter 4 - Accounting for Uncertainty in Estimation}\label{chapter-4---accounting-for-uncertainty-in-estimation}}

\hypertarget{chapter-5---inference-via-hypothesis-testing-for-a-proportion-or-mean}{%
\subsection{Chapter 5 - Inference via Hypothesis Testing for a Proportion or Mean}\label{chapter-5---inference-via-hypothesis-testing-for-a-proportion-or-mean}}

\hypertarget{chapter-6---inference-via-confidence-intervals-for-a-proportion-or-mean}{%
\subsection{Chapter 6 - Inference via Confidence Intervals for a Proportion or Mean}\label{chapter-6---inference-via-confidence-intervals-for-a-proportion-or-mean}}

\hypertarget{chapter-7---inference-on-two-categorical-variables}{%
\subsection{Chapter 7 - Inference on Two Categorical Variables}\label{chapter-7---inference-on-two-categorical-variables}}

\hypertarget{chapter-8---inference-for-multiple-means}{%
\subsection{Chapter 8 - Inference for Multiple Means}\label{chapter-8---inference-for-multiple-means}}

\hypertarget{chapter-9---multiway-anova}{%
\subsection{Chapter 9 - Multiway ANOVA}\label{chapter-9---multiway-anova}}

\hypertarget{chapter-10---block-designs}{%
\subsection{Chapter 10 - Block Designs}\label{chapter-10---block-designs}}

\hypertarget{chapter-11---regression}{%
\subsection{Chapter 11 - Regression}\label{chapter-11---regression}}

\hypertarget{chapter-12---the-general-linear-model}{%
\subsection{Chapter 12 - The General Linear Model}\label{chapter-12---the-general-linear-model}}

\hypertarget{chapter-13---mixed-models}{%
\subsection{Chapter 13 - Mixed Models}\label{chapter-13---mixed-models}}

\hypertarget{chapter-14---repeated-measures-and-split-plot-designs}{%
\subsection{Chapter 14 - Repeated Measures and Split Plot Designs}\label{chapter-14---repeated-measures-and-split-plot-designs}}

\hypertarget{chapter-15---logistic-regression-and-generalized-linear-models}{%
\subsection{Chapter 15 - Logistic Regression and Generalized Linear Models}\label{chapter-15---logistic-regression-and-generalized-linear-models}}

\hypertarget{chapter-16---generalized-linear-mixed-models}{%
\subsection{Chapter 16 - Generalized Linear Mixed Models}\label{chapter-16---generalized-linear-mixed-models}}

\hypertarget{from-st512}{%
\section{From ST512}\label{from-st512}}

WE NEED TO ORGANIZE THESE UNDER DIFFERENT CHAPTERS AT SOME POINT
Learning Objectives

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Recognize a completely randomized design with one treatment factor and write the corresponding one-way analysis of variance model, with assumptions
\item
  Estimate treatment means
\item
  Estimate the variance among replicates within a treatment
\item
  Construct the analysis of variance table for a one factor analysis of variance, including computing degrees of freedom, sums of squares, mean squares, and F-ratios
\item
  Interpret results and draw conclusions from a one-factor analysis of variance
\item
  Estimate differences between two treatment means in a one factor analysis of variance
\item
  Test differences between two treatment means in a one factor analysis of variance
\item
  Construct a contrast to estimate or test a linear combination of treatment means
\item
  Estimate the standard error of a linear combination of treatment means
\item
  Make inferences about linear combinations of treatment means, including contrasts.
\item
  Obtain and understand SAS output for linear combinations of treatment means, including contrasts.
\item
  Explain when and why corrections for multiple comparisons are needed
\item
  Know when and how to use Tukey's correction for all pairwise comparisons
\item
  Compute Bonferroni confidence intervals
\item
  Create and interpret orthogonal contrasts.
\item
  Define main effects and interactions
\item
  Write contrasts to estimate main effects and interactions
\item
  Estimate these contrasts and their standard errors
\item
  Compute sums of squares associated with these contrasts
\item
  Test hypotheses about the main effects and interactions.
\item
  Identify and define simple effects.
\item
  Identify and define interaction effects.
\item
  Identify and define main effects.
\item
  Understand when to use simple, interaction, and main effects when drawing inferences in a two-way ANOVA.
\item
  Write the analysis of variance model and SAS code for a completely randomized design with two factors
\item
  Test hypotheses and interpret the analysis of variance for a factorial experiment.
\item
  Explain the appropriate use of correlations and compute the correlation coefficient
\item
  Read and interpret a scatterplot and guess the correlation coefficient by examination of a scatter plot
\item
  Interpret the strength and direction of association indicated by the correlation coefficient and judge when a correlation coefficient provides an appropriate summary of a bivariate relationship
\item
  Test the hypothesis that the correlation coefficient is zero using either a t-test or the Fisher z transformation, Compute confidence intervals using Fisher's z transformation
\item
  Write a statistical model for a straight line regression or a multiple regression and explain what all the terms of the model represent
\item
  Explain the assumptions underlying regression models, evaluate whether the assumptions are met
\item
  Estimate the intercept, slope and variance for a simple linear regression model
\item
  Fit a multiple regression model in SAS and interpret the output, use the coefficient of determination to evaluate model fit
\item
  Use a regression model to predict Y for new values of X
\item
  Estimate the variance and standard error of parameters in regression models, test hypotheses about the parameters, and construct confidence intervals for the parameters.
\item
  Explain the difference between a confidence interval and a prediction interval and know when to use each of them
\item
  Construct a confidence interval for the expected value of Y at a given value of X
\item
  Construct a prediction interval for a new value of Y at a given value of X
\item
  Write a linear model in matrix notation
\item
  Find the expectation and variance of a linear combination of random variables, a'Y
\item
  Set up the expressions to calculate parameter estimates and predicted values using the matrix form of the model
\item
  Estimate standard errors for parameter estimates and predicted values
\item
  Use extra sums of squares to test hypotheses about subsets of parameters
\item
  Construct indicator variables for including categorical regressor variables in a linear model
\item
  Understand how to interpret parameters of a general linear model with indicator variables
\item
  Estimate contrasts of treatment means and their standard errors using the general linear model notation and matrix form of the model
\item
  Compare nested models with a lack of fit test to select a model
\item
  Explain what a covariate is and how they are used
\item
  Explain the assumptions of the analysis of covariance model and determine when these assumptions are met
\item
  Fit an analysis of covariance model in SAS and conduct appropriate tests for treatment effects
\item
  Estimate and interpret treatment means and their standard errors adjusted for covariates using SAS, Construct confidence intervals for adjusted treatment means
\item
  Construct and estimate contrasts of treatment means adjusted for covariates and estimate the standard errors and confidence intervals of such contrasts.
\end{enumerate}

Analysis of variance and design of experiments
Recognize each of the following types of experimental designs and determine when each type would be advantageous.
1. completely randomized design
2. randomized complete block design
3. split plot design
Recognize whether factors should be considered fixed effects or random effects and explain the scope of inference for each case.
Recognize whether factors are crossed or nested.
For all of the designs listed and for experiments with crossed and/or nested fixed factors, random factors, or a combination of fixed and random effects, be able to
1. Write the corresponding analysis of variance model, with assumptions, and define all terms
2. Estimate treatment means and their standard errors
3. Construct the analysis of variance table, including computing degrees of freedom, sums of squares, mean squares, and F-ratios
4. Determine whether the assumptions of the model are satisfied
5. Interpret results and draw conclusions
6. Construct and estimate linear combinations of treatment means and their standard errors
7. Test hypotheses and construct confidence intervals about linear combinations of treatment means
8. Explain when and why corrections for multiple comparisons are needed, know when and how to use Tukey's correction for all pairwise comparisons, compute Bonferroni confidence intervals
9. Create and interpret orthogonal contrasts.
10. Define and interpret main effects, simple effects and interactions
11. Use a table of expected mean squares to estimate variance components and determine appropriate F-statistics for testing effects in the analysis of variance
12. Interpret variance components and estimate and interpret the intraclass correlation coefficient.
Regression and correlation
Explain the appropriate use of correlations and compute the correlation coefficient, read and interpret a scatterplot and guess the correlation coefficient by examination of a scatter plot, test the hypothesis that the correlation coefficient is zero using either a t-test or the Fisher z transformation, compute confidence intervals using Fisher's z transformation
You should be able to do the following for fitting models to describe the relationships of one or several variables to a response variable. The regressor variables may be continuous or categorical or a mix of the two (e.g., analysis of covariance models)
1. Write a general linear model, including assumptions, in standard or matrix notation, and explain what all the terms and assumptions represent. Be able to handle models that contain interaction terms, polynomial terms, and dummy variables.
2. Evaluate whether the model assumptions are met
3. Fit a general linear model in SAS and interpret the output
4. Work with the general linear model in matrix form, including finding the expectation and variance of a linear combination of regression coefficients or treatment means
5. Test hypotheses and construct confidence intervals for linear combinations of the parameters
6. Construct and interpret a confidence interval for the expected value of Y at a given value of X
7. Construct and interpret a prediction interval for a new value of Y at a given value of X
8. Use extra sums of squares to test hypotheses about subsets of parameters.
9. Explain what a covariate is and how covariates are used

\hypertarget{for-point-estimates-chapter}{%
\section{For Point Estimates Chapter}\label{for-point-estimates-chapter}}

\begin{itemize}
\item
  Definitions for Mean, Median, Quantile, Percentile
\item
  Explain uses for the above
\item
  Identify the correct point estimate to use for a given test
\item
  Define Systematic Random Sample and Convenience Sample
\item
  Explain strengths and weaknesses of each
\item
  Identify conditions when Systematic and Convenience Sampling may not provide representitive samples
\end{itemize}

\hypertarget{notation}{%
\chapter{Appendix - Notation}\label{notation}}

\hypertarget{standard-notation}{%
\section{Standard notation}\label{standard-notation}}

Vectors of variables are denoted with Roman letters, such as \(x\) and \(Y\). Capital letters denote random variables while lower case letters denote fixed variables. Note that these vectors may be of length 1 depending on context. Bolded values (\textbf{\(x\)}) denote matrices, and in the case of \textbf{\(Y\)}, possibly single-column matrices.

Unknown parameters are denoted with Greek letters, with boldface font indicating matrices.

In most models, \(Y\) will denote the univariate response, \textbf{\(x\)} will describe a matrix of predictor variables, and \(E\) a vector of random errors. The Greek letter \(\beta\) will be commonly used for regression parameters (either with subscripts for each values as in \(\beta_0 + \beta_1 X_1\) or as a vector (as in \(X\beta\)). The letters \(i, j, k,\) and \(l\) will be most commonly used as subscripts or indices. \(N\) will typically denote a sample size (not a random vector), with subscripted versions (\(n_i\)) describing the number of observations in a group, and \(p\) describing the number of parameters in a model beyond the intercept.

We may therefore describe a simple linear regresion model as:

\[Y = x\beta + E\]

In this model, \(Y\) is a \(N\times 1\) random vector, \textbf{\(x\)} is a \(N\times (p + 1)\) matrix of fixed values, and \(E\) is a \(N \times 1\) vector.

\(\pi\) is typically used to describe probability parameters, as in Bernoulli or binomial random variables.

\hypertarget{mixed-models}{%
\section{Mixed models}\label{mixed-models}}

Still need to add something for this

\hypertarget{effects-model-representation}{%
\section{Effects model representation}\label{effects-model-representation}}

In the effects formulation of ANOVA models, additional greek letters (\(\alpha\), \(\gamma\), etc.) will appear as parameter effects, as will \(\mu\), which will typically represent the grand mean. Group-specfic means will be denoted via subscripts: \(\mu_{ij}\). When using this representation, it is convenient to describe a single observation as \(Y_{ijk}\), which is the \(k\)th observation from the group with with the \(i\)th level of the first factor and the \(j\)th level of the second factor. In the main effects version of this model, we have:

\[Y_{ijk} = \mu + \alpha_i + \gamma_j + E_{ijk}\]

We can therefore estimate \(\mu_{ij}\) as \(\hat \mu_{ij} = \frac{1}{n}\sum_{k = 1}^n Y_{ijk} = \bar{Y}_{ij\cdot}\). This ``dot'' notation can be extended to any subscript and indicates summing over the index that has been replaced by the dot. Further note that the ``hat'' over a paremeter value denotes the estimator for that parameter value, and the ``bar'' indicates an average. These features are used generally throughout this book.

\hypertarget{estimators-vs.-estimates}{%
\section{Estimators vs.~Estimates}\label{estimators-vs.-estimates}}

If we want to get pedantic, we can differentiate between estimates and estimators in our notation. Estimators are functions of random variables used to estimate parameters. Estimates are realized values of estimators. To differentiate these, we use Roman letters with hats to represent estimators (\(\hat B = (x'x )^{-1}x'Y\)) and Greek letters with hats to represent estimates (\(\hat \beta = 1.52\)).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \includegraphics{img/conclude-communicate.png} Define the objective of the experiment and understand the background (Define Objective \& Background)\\
\item
  Select appropriate response variables (Select Response)\\
\item
  Identify sources of variation (Determine Sources of Variation)\\
\item
  Choose sampling scheme and/or experimental design (Select Design)\\
\item
  Carry out the study (Do Study)
\item
  Statistically analyze the data (Do Statistical Analysis)\\
\item
  Draw conclusions from the analysis while considering limitations and the steps above as well as communicate results (Draw Conclusions \& Communicate)
\end{enumerate}

  \bibliography{book.bib,packages.bib}

\end{document}
