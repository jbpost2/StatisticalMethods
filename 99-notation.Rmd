# Appendix - Notation {#notation}


## Standard notation

Variables are denoted with Roman letters, such as $x$ and $Y$. Lower case letters denote random variables while capital letters denote fixed variables. Note that these vectors may be of length 1 depending on context. Bolded values (**$x$**) denote matrices, and in the case of **$Y$**, possibly single-column matrices.

Unknown parameters are denoted with Greek letters, with boldface font indicating vectors/matrices. 


In most models, $Y$ will denote the univariate response, **$x$** will describe a matrix of predictor variables, and $e$ a vector of random errors. The Greek letter $\beta$ will be commonly used for regression parameters (either with subscripts for each values as in $\beta_0 + \beta_1 X_1$ or as a vector (as in **$X\beta$**). The letters $i, j, k,$ and $l$ will be most commonly used as subscripts or indices. $N$ will typically denote a sample size, with subscripted versions ($N_i$) describing the number of observations in a group, and $p$ describing the number of parameters in a model beyodn the intercept. 

We may therefore describe a simple linear regresion model as: 

$$Y = x\beta + E$$

In this model, **$Y$** is a $N\times 1$ random vector, **$x$** is a $N\times (p + 1)$ matrix of fixed values, and $E$ is a $N \times 1$ vector. 

$\pi$ is typically used to describe probability parameters, as in Bernoulli or binomial random variables. 

## Mixed models

Still need to add something for this

## Effects model representation

In the effects formulation of ANOVA models, additional greek letters ($\alpha$, $\gamma$, etc.) will appear as parameter effects, as will $\mu$, which will typically represent the grand mean. Group-specfic means will be denoted via subscripts:  $\mu_{ij}$. When using this representation, it is convenient to describe a single observation as $Y_{ijk}$, which is the $k$th observation from the group with with the $i$th level of the first factor and the $j$th level of the second factor. In the main effects version of this model, we have:

$$Y_{ijk} = \mu + \alpha_i + \gamma_j + E_{ijk}$$ 

We can therefore estimate $\mu_{ij}$ as $\hat \mu_{ij} = \frac{1}{n}\sum_{k = 1}^n Y_{ijk} = \bar{Y}_{ij\cdot}$. This "dot" notation can be extended to any subscript and indicates summing over the index that has been replaced by the dot. Further note that the "hat" over a paremeter value denotes the estimator for that parameter value, and the "bar" indicates an average. These features are used generally throughout this book. 

## Estimators vs. Estimates

If we want to get pedantic, we can differentiate between estimates and estimators in our notation. Estimators are functions of random variables used to estimate parameters. Estimates are realized values of estimators. To differentiate these, we use Roman letters with hats to represent estimators ($\hat B = (x'x )^{-1}x'Y$) and Greek letters with hats to represent estimates ($\hat \beta = 1.52$). 
