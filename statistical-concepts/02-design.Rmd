### Fundamentals of Designed Experiments  

To describe the methods for creating a well-designed experiment, we first need to recap some definitions from earlier:

- **Response Variable** - variable(s) of interest that characterizes performance or behavior  
- **Explanatory Variable** - variable(s) of interest with regard to their relationship with the response variable  
- **Covariate** - Quantitative (numerical) explanatory variable (usually observed and not set by the researcher)  
- **Factor** - explanatory variable that takes on a finite number of values (a Categorical or Qualitative variable) 
- **Level** -	setting a factor can take on
- **Treatment** - specific experimental condition, either the level of a factor (if only 1 factor) or the combinations of the levels from a number of factors  
    + **Control Treatment** - benchmark treatment sometimes necessary for comparison (to avoid the placebo effect)  
- **Experimental units (EUs)** - units on which the treatments are assigned  
- **Observational units (OUs)** - units on which measurements are taken  

There is clearly a lot of jargon to remember when learning about experimental design.  We'll do our best to remind the reader of these terms as we go along.  

Before we dive into experimental designs, we should again discuss the reason we need statistical methods with a little more detail.  

#### Sources of Variation  

We've discussed that data is variable.  If we repeat an experimental study, even with identical conditions, we are likely to obtain different data for the response.  This has to do with the sample members being selected differently each time and possibly different units assigned a given treatment.  Let's identify the sources variation in our response.  

- **Treatment effect** - an effect due to the variables assigned by the researcher (treatments) in an experiment.  
    
    + This is an effect we are usually hoping to see and quantify!

- (Other) **Recorded Variables** - some variables that are not of interest are recorded because we may know or think they are associated with variability in the response.  

    + These may be covariates measured along the way (like temperature) or a variable with only a few values like the soil type a crop is grown in.  

- **Unaccounted for Variables** - everything else causing variation.  

    + This variation is estimated and used as a reference with which to compare the treatment variation.  

Consider a simplified example where a gardener wants to know what water (low or high) and fertilizer (A or B) conditions are better in terms of producing greater crop yield (dried weight).  

- Response variable - the dried weight of the crop after growth
- The explanatory variables are the two factors:  
    + Water (with levels low and high)  
    + Fertilizer (with levels A or B)  
- Treatments:
    + low water and fertilizer A  
    + low water and fertilizer B  
    + high water and fertilizer A  
    + high water and fertilizer B  

The gardener has two greenhouses each with 16 rows for growing crops.  Within each greenhouse, they randomly assign the four treatments to the 16 rows.  After 45 days, the crops are harvested, dried, and weighed.   

- The experimental units are the rows within the greenhouses  
- The observational units are all the plants grown in a given row (so we may say the row is also the OU)  

The sources of variation in the dried weight of the crop are:  

- Treatment variation - variation we believe we'll see in dried weight from the differences in water and fertilizer applications.  
- Other recorded variable - greenhouse in which the crops are grown is recorded.  Greenhouse is not of interest but may play a role in the variation in dried weight.
- Unaccounted for variables - Amount of sunlight received, temperature/humidity differentials within a greenhouse, possible differences in the method of application of the treatment (fertilizer and water), and many others may also have an association with dried crop weight.  These sources make up a sort of reference variability we can compare our treatment variation to.  

The variation from unaccounted variables can generally be broken down into four cateogries:  

- Inherent variability in EUs (units assigned a treatment).  
    + No two people, paper towels, concrete blocks, or  lab rats are exactly the same so they may respond differently to the same treatment.  

- Measurement error - Multiple measurements of a same experimental unit may differ.  
    + Two blood pressure readings a few minutes apart may give different readigns or if you break a water sample in two and measure each for bacteria, you may see different measurements  

- Variations in applying or creating treatments.  
    + Occasionaly a treatment protocol is not clearly defined, leaving room for interpretation.  Perhaps applying a fertilizer before or after applying irrigation can cause a difference.

- Other unknown variables sometimes called lurking variables.

No matter how hard we try, some of these unaccounted for variables (and hence variation in the response) will remain. What we can do is use good experimental design techniques to try and minimize the effects of from these variables.  

Good experimental designs generally have the following key attributes:

- **Randomization** - treatments are randomly assigned to experimental units.

This process must use a random number mechanism or software to allocate the treatments.  Randomization makes sure that every EU has a chance to get a different treatment.  This helps to protect the results of the analysis against a systematic influence of lurking variables.  For example, if a doctor is assigning drug A or B to a patient without use of a random number mechanism, they may unwittingly assign drug A to patients they deem more likely to recover due to implicit biases.  

- **Replication** - multiple (independent) experimental units are assigned the same treatment.  

EUs that receive the same treatment are called **replicates**.  By having replication we are able to create an estimate of variability due to our unaccounted for variables.   Comparing our treatment variation to this variation is what allows us to have faith in the reliability of our conclusions.  

Note that replication does not mean that we measure the same EUs multiple times!  That is called a repeated measures design.  Observations from repeated measures experiments cannot usually be considered independent.

- **Controlling Variables** - holding certain variables constant across the EUs.  
Generally, we're not interested in the effects of these variables on the response.  These variables affect the response in exactly the same manner, so that we	don't see the effects on the conclusions. Unfortunately, we don`t get information on what happens at settings of the variables other than the fixed ones.  This decreases generalizability, but reduces overall variation.  Experimental designs such as a randomized complete block design attempt to control variables while also maintaining generalizability.  


There are also methods for dealing with some explained experimental error during the analysis stage - Namely ANCOVA.

> A poorly designed study can never be saved, but a poorly analyzed one has the possibility of being reanalyzed.

Good discussion of what makes a good sampling design revisit.  Maybe a statified example like the river and selecting houses example as a quick expose of the issues with not doing a truly random sampling technique.  

Analysis in future chapters...

#### CRD  

    + Completely Randomized Design (CRD) - for t treatments, replicated $n_t$ times each, use a random number generator to assign the treatments to the EUs.  
    + Most basic randomization design - assumes all EUs are exchangeable.

Example:  We are doing an experiment to determine the effect of nutrition (3 different diets) on weight gain in humans.  How can we perform a CRD for this study?


Ideally we want to have as many replicates for each treatment as we can afford.   Think if we had 3 diets and 3 EUs.  Diet 1 was better than diet 2 and diet 3, not a very reliable conclusion, perhaps person 1 just loses weight more easily.  Now if 100 people at each diet and on average diet 1 was much better, more reliable conclusion.  

Power and sample size calculations stuff dealt with later.  



#### Blocks  

- Blocking - Divide subjects with similar characteristics into `blocks', and then within each block, randomly assign subjects to treatment groups.

- Blocks - Groups of EUs sharing a common level of a confounding variable.

```{r, echo = FALSE, fig.align='center', out.width="80%"}
knitr::include_graphics("img/block.png")
```

Similar to controlling, but allows for increased generalizability.  EUs within a block are very similar (decreases experimental error there as all the EUs in a block are affected similarly by the confounding variable).  By having enough blocks to cover the range of the population you can still generalize.)


Example:  Two new types of material are developed (type A and type B) for use as a dash board in a car.  The material must withstand high temperatures due to the sun and the greenhouse effect.  
- To test which material holds up better, the manufacturer randomly selects 20 pieces of type A and 20 pieces of type B material.  
- The company has 4 large ovens (oven 1, 2, 3, and 4) for testing.  Each oven has 5 pieces of type A and 5 of type B randomly placed into each.   
- After 24 hours, the amount of degradation is measured.  

What are the blocks?  How many replicates do we have?  How many replicates in each block?

### Latin Squares
#### Split plot

#### Strip plot

#### RCBSP  

#### 2^k

#### Response Surface  

...?

A major goal of a study is usually to conduct statistical inference.  Inference can involve determine which variables are important in relation to a response variable and/or predicting a response variable.  To formally do inference we need to define the population of interest as well as **parameters** we want to study.  

- Population - all the values, items, or individuals of interest

- Parameter - a (usually) unknown summary value about the population

Ideally, we'd be able to measure every member of the population and exactly calculate the value of any population parameter.  This would involved conducing a census.  A census is usually not feasible.  Instead we take a subset of the population and try to use these observations to make statements or claims about the population.

- Sample - a subset of the population we observe data on

- Statistic - a summary value calculated from the sample observations

Example - A political scientist surveys 400 people randomly from a list of all registered voters in a particular county.  He asks the people if they plan to vote in the upcoming election and 312 say they do.

**Relate above to definitions...**

To discuss paramters and statistics more easily, symbols are used to denote them.  Luckily, there is a common notation that, for the most, is consistent across statistical literature. 

<!-- Name | Parameter | Statistic | Quantity Measured -->
<!-- -------------------------------------------------- -->
<!-- Mean | $\mu$     | $\bar{Y}$ or $\bar{y}$ or $\bar{X}$ or $\bar{x}$ | Center or Location -->
<!-- Proportion | $p$ or $\pi$ | $\hat{P}$ or $\hat{p}$ or $\hat{\pi}$ | Location or Frequency -->
<!-- Standard Deviation (SD) | $\sigma$ | $S$ or $s$ | Variability or spread -->
<!-- Variance (Var) | $\sigma^2$ | $S^2$ or $s^2$ | Variability or spread -->

Note: $\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i$ and $S^2=\frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$ where $n$ is the sample size (or number of observed values in the sample).  Sometimes you'll see other notation for proportions **(....... fill in and change table)**

Question of interest will lead you to which parameter you have interest in.  Careful consideration of what your study goals are will also most likely lead you to which type of data you will collect.


Examples of why Observational Studies Can be Bad  

George Will WP column about SAT vs amount of money spent
Pisani and Perv?  many bad examples of inference

Stuff about power/sample size calculation somewhere?



Similarities and differences between sampling and DOE randomization 
SRS vs CRD comparison
Strat vs Block comparison  


#### Sampling Schemes vs Randomization Methods  

Notes about randomization:  

- CRD similar in spirit to a SRS in the fact that it does not guarantee balance of lurking variables.  However, if we have a large enough study we should balance out the lurking variables between the treatment groups.  

Randomization is different than random sampling!  

- With an Random Sample, chance determines who will be included in the sample. 
- Once we have our sample, Randomization determines which EUs get which treatments.
