[
["index.html", "Statistical Methods Chapter 1 Introduction 1.1 About the book 1.2 Software 1.3 R", " Statistical Methods Post, Avery, Osborne 2020-03-11 Chapter 1 Introduction 1.1 About the book The goal in creating this book is to provide a thorough treatment of applied statistical methodologies geared toward analyzing designed experiments. Our approach emphasizes the problems researchers encounter rather than provide a litany of solutions with only modest context. We discuss a real scientific problem, thoughfully consider the data that could be used to answer that problem, investigate experimental designs that would be useful, and pursue statistical models to make informed decisions in the context of how the data was collected. The focus of the book is on the statistical modeling portion but problems are viewed holistically. We purposefully introduce the linear model framework in matrix form early on and place most of the methodologies under that umbrella. This helps the reader to see the methods as a part of a general framework as opposed to a set of tools within a toolbox. We believe that the book should be appropriate for a graduate level student that has some comfort in mathematics, particularly linear algebra. Both SAS and R are used throughout to make sure the book works for a wide audience of practitioners. 1.2 Software At this point software is a requirement for statistics in practice. There are many available software solutions ranging from point and click to full on programming. We’ve decided to focus on R and SAS for this book. R is an open source, platform agnostic, software that is widely used by statisticians. We’ll use the RStudio interactive development environment to write and execute our R code. SAS requires a license but is an extremely powerful software for doing modeling and is used widely enough to merit inclusion here. For those without a license, SAS University Edition can be installed for free and is also platform agnostic. We’ll use the SAS Studio environment that comes with Univeristy Edition. As we progress through the book we’ll include graphs, descriptive statistics, and analyses from R and/or SAS. At the end of each chapter a section explaining how to create these in both R and SAS is included. The following sections give a brief introduction to each software that should prepare you for what’s ahead! 1.3 R The general workflow for programming in R involves taking raw data and importing it into R. Once that data is imported we often create numerical and graphical summaries of the data. The appropriate model or statistical method is then applied. At the end of this section the reader should be able to do the following: install R and RStudio read and write basic R programs import well formatted data into R do basic data manipulation in R As the book progresses the steps of summarizing and analyzing the data will be covered. Let’s get started! 1.3.1 Installing R and RStudio The R software itself can be downloaded and installed by visiting the Comprehensive R Archive Network (Cran) website. Here there are links to install R for Linux, Mac, and Windows based machines. For Windows users, follow the inital ‘Download R for Windows’ link and then click ‘install R for the first time.’ From here you should now see a Download R X.x.x for Windows link that will download a .exe file. Once downloaded run that file and follow the prompts. For Mac users, follow the inital ‘Download R for (Mac) OS X’ link and click on the link near the ‘Latest Release’ section similar to R-x.x.x.pkg. Once downloaded, you should be able to install by double clicking on the file. For Linux users, follow the inital ‘Download R for Linux’ link. Choose your OS and instructions are given on how to download R. Once you’ve installed R you’ll want to install RStudio. RStudio is a well developed environment that makes programming in R much easier! To download head to RStudio’s download page. From here choose RStudio Desktop (Open Source License) and a page with appropriate links to install are provided. 1.3.2 Using RStudio To program in R you’ll want to open RStudio. RStudio will submit R code for you so you never actually need to open R itself. There are four main ‘areas’ of the RStudio IDE: Console (&amp; Terminal) Scripting and Viewing Window Plots/Help (&amp; Files/Packages) Environment (&amp; Connections/Git) You may wish to rearrange the panes. This can be done via the menus at the top. Choose “Tools –&gt; Global Options”. Other useful global options to chnage are under the appearance tab (font size, theme) and under the code tab (editing –&gt; soft-wrap, display –&gt; show whitespace). 1.3.2.1 Console To evaluate code you can type directly into the console. #simple math operations # &lt;-- is a comment - code not evaluated 3 + 7 ## [1] 10 10 * exp(3) #exp is exponential function ## [1] 200.8554 log(pi^2) #log is natural log by default ## [1] 2.28946 mean(cars$speed) ## [1] 15.4 hist(cars$speed) In the R sections of the book we spend much of our time learning the R syntax needed to create the appropriate summaries or analysis. 1.3.2.2 Scripting and Viewing Window Usually you don’t want to type code directly into the console because there isn’t an easy way to get the code for later use. Instead code is usually written in an R ‘script’ which is then saved. From an R script you can send code to console via: “Run” button (runs current line) CTRL+Enter (PC) or Command+Enter (MAC) Highlight section and do above To create a new R script you can use the menus at the top and go to File –&gt; New File –&gt; R Script. Take a moment and do this! Type the following into your script: View(cars) (note capital V) plot(cars) Submit it to the console using a button or hot key! 1.3.2.3 Plots/Help Created plots are stored in the Plots tab. This is a nice feature that allows you to cycle through past plots and easily save plots via menus. In this pane there is also a Help tab that will enable you to learn about R functions. In the console type help(hist) for instance. Information about the hist function is presented. Being able to parse these types of help files is a really useful skill! For every R function there are a few sections: Description - What the function is intended for. Usage - How to call the function, inputs required, and which inputs have default arguments. Here we see hist(x, ...). This implies there is only one required input, x, and there is no default. Below you see a more detailed call to hist that includes other inputs. Each of these inputs has an equal sign with a value after it. This is the default value for that input (since there is a default value you don’t have to specify it when you call). For instance the breaks = &quot;Sturges&quot; input implies that the “Sturges” method is the default for determining how the bins of the histogram are created. Arguments - Describes the input requirements in more detail. Details - Information about how the function works. Values - Information about what is returned to the user. References See Also - Related functions. Examples - Highly useful section giving code you can copy and paste to see an example of how the function can be used. 1.3.2.4 Environment R stores data/info/functions/etc. in R objects. An object is a data structure having attributes and methods (more on this shortly). You can create an R object via &lt;- (recommended) or =. #save for later avg &lt;- (5 + 7 + 6) / 3 #call avg object avg ## [1] 6 #strings (text) can be saved as well words &lt;- c(&quot;Hello there!&quot;, &quot;How are you?&quot;) words ## [1] &quot;Hello there!&quot; &quot;How are you?&quot; Notice that when you send the line avg &lt;- (5+ 7 + 6) / 3 to the console (i.e. create the object avg) that nothing prints out. This is common behavior when storing the object. The output or information is saved for later use in the object. To see the output or information you then simply call the object (a default printing method is used to display it). You can look at all current objects with ls(). ls() ## [1] &quot;avg&quot; &quot;words&quot; Use rm() to remove an object. rm(avg) ls() ## [1] &quot;words&quot; Built-in objects exist like letters and cars. letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; head(cars, n = 3) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 The function data() shows available built-in datasets. You should now be roughly familiar with the four main ‘areas’ of the RStudio IDE: Console (&amp; Terminal) Scripting and Viewing Window Plots/Help (&amp; Files/Packages) Environment (&amp; Connections/Git) 1.3.3 R Objects and Classes R has strong Object Oriented Programming (OOP) tools. Object: data structure with attributes (class) Method: procedures (functions) that act on object based on attributes R functions like print() or plot() act differently depending on an object’s class. class(cars) ## [1] &quot;data.frame&quot; plot(cars) class(exp) ## [1] &quot;function&quot; plot(exp) Many R functions exist to help understand an R Object. str() (structure) str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... class() class(cars) ## [1] &quot;data.frame&quot; typeof() typeof(cars) ## [1] &quot;list&quot; We’ll use these functions later to help us know how to extra information from an R object. Recall that we can create an R object via &lt;- (recommended) or =. This allocates computer memory to object. The object’s attributes depend on how you created it. vec &lt;- c(1, 4, 10) class(vec) ## [1] &quot;numeric&quot; fit &lt;- lm(dist ~ speed, data = cars) class(fit) ## [1] &quot;lm&quot; 1.3.4 Data Objects To understand how to use R for data analysis we need to understand commonly used data structures: 1. Atomic Vector (1D) 2. Matrix (2D) 3. Array (nd) (not covered) 4. Data Frame (2D) 5. List (1D) 1.3.4.1 Atomic Vector Let’s start with the most basic object and work our way up. An atomic vector is a 1D group of elements with an ordering. All of the elements must be same ‘type’. Types include numeric (integer or double), character, or logical. We create an atomic vector with the c() function (‘combine’). #vectors (1 dimensional) objects x &lt;- c(17, 22, 1, 3, -3) y &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;bird&quot;, &quot;frog&quot;) x ## [1] 17 22 1 3 -3 y ## [1] &quot;cat&quot; &quot;dog&quot; &quot;bird&quot; &quot;frog&quot; In addition, many ‘functions’ output a numeric vector. Functions are at the heart of R so it is vital to understand them. The concept of a function is that there the function takes an input or inputs and maps those inputs to some output(s). As an example, one function that outputs a numeric vector is the seq or sequence function. To know about a function you need to know about the inputs and ouputs. For seq we have the following: + Inputs = from, to, by (among others) + Output = a sequence of numbers v &lt;- seq(from = 1, to = 5, by = 1) v ## [1] 1 2 3 4 5 str(v) ## num [1:5] 1 2 3 4 5 str tells about the object v: num says it is numeric [1:5] implies one dimensional with elements 1, 2, 3, 4, 5 The seq function is used quite a bit. There is a shorthand way to create an integer sequence using :. 1:20 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 It is also important to know how R does math on its objects. R does elementwise addition/subtraction and multiplication/division to vectors, matrices, and data frames. (The matrix multiplicaiton operator is %*%.). 1:20/20 ## [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 ## [16] 0.80 0.85 0.90 0.95 1.00 1:20 + 1 ## [1] 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 As we mentioned earlier, understanding help files is really useful to being about to program in R. As functions are ubiquitous in R we often need to learn about their inputs (or arguments) and we can do so using help. To recap, our first commonly used R object for storing data is an atomic vectore. This is a 1D group of elements with an ordering where all of the elements are of the same type. Generally vectors are useful to know about but not usually useful for a storing a dataset exactly. They can often be considered as the ‘building blocks’ for other data types. 1.3.4.2 Matrix A Matrix is a 2D data structure in R whose elements are all of the same type. The first dimension refers to the rows and the second dimension refers to the columns. A 2D data object is very common. The rows often represent the observations and the columns represent the variables. Although not technically right, it is useful to think of the columns of a matrix as vectors of the same type and length. For instance, consider the three vectors created here: #populate vectors x &lt;- c(17, 3, 13, 11) y &lt;- rep(-3, times = 4) z &lt;- 1:4 These are all of the same type. This can be checked with an is. (read as ‘is dot’) function. #check &#39;type&#39; is.numeric(x) ## [1] TRUE is.numeric(y) ## [1] TRUE is.numeric(z) ## [1] TRUE Not only are these three objects the same type but they are also the same length. This can be checked using the length function. #check &#39;length&#39; length(x) ## [1] 4 length(y) ## [1] 4 length(z) ## [1] 4 Again, it is useful to visualize the columns of a potential matrix as these vectors. We can create the matrix using the matrix function. The matrix function requires us to give the data as one vector. We can combine the x, y, and z objects into one vector using the c funciton. This is the first argument to the matrix function. The only other argument required is to either specify the number of rows (nrow =) or the number of columns (ncol =) (R will attempt to figure out the one that is not given using the total length of the specified data vector). #combine in a matrix matrix(c(x, y, z), ncol = 3) ## [,1] [,2] [,3] ## [1,] 17 -3 1 ## [2,] 3 -3 2 ## [3,] 13 -3 3 ## [4,] 11 -3 4 A matrix can also store character data as well. An example of this is given below and the number of rows is specified rather than the number of columns. Note the use of is.character from the is. family of functions. x &lt;- c(&quot;Hi&quot;, &quot;There&quot;, &quot;!&quot;) y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) z &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;) is.character(x) ## [1] TRUE matrix(c(x, y, z), nrow = 3) ## [,1] [,2] [,3] ## [1,] &quot;Hi&quot; &quot;a&quot; &quot;One&quot; ## [2,] &quot;There&quot; &quot;b&quot; &quot;Two&quot; ## [3,] &quot;!&quot; &quot;c&quot; &quot;Three&quot; To recap, a Matrix is a 2D data structure where we can think of the columns as vectors of the same type and length. These are useful for some datasets but most datasets have some numeric and some character variables. Another 2D object called a data frame is perfect for this type of data! 1.3.4.3 Data Frame A Data Frame is a 2D data structure where elements within a column must be of the same type but the columns themselves can differ in type. When thinking of a data frame, consider them as a collection (list) of vectors of the same length. A data frame can be created with the data.frame function. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) y &lt;- c(1, 3, 4, -1, 5, 6) z &lt;- 10:15 data.frame(x, y, z) ## x y z ## 1 a 1 10 ## 2 b 3 11 ## 3 c 4 12 ## 4 d -1 13 ## 5 e 5 14 ## 6 f 6 15 You can also easily name the columns during creation. data.frame(char = x, data1 = y, data2 = z) ## char data1 data2 ## 1 a 1 10 ## 2 b 3 11 ## 3 c 4 12 ## 4 d -1 13 ## 5 e 5 14 ## 6 f 6 15 Notice that char, data1, and data2 become the variable names for the data frame. To recap, consider a data frame as a collection (list) of vectors of the same length. Tis type of data structure is perfect for most data sets! Most functions that read 2D data into R store it as a data frame. 1.3.4.4 List A List is a 1D group of objects with ordering. Really it is a vector that can have differing elements. Think of this in a similar way to the atomic vector previously discussed except the elements are really flexible. A list can be created with the list function. You specify the elements you want to include, separated by commas. list(1:3, rnorm(2), c(&quot;!&quot;, &quot;?&quot;)) ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] 0.7563253 0.2884680 ## ## [[3]] ## [1] &quot;!&quot; &quot;?&quot; Similar to a data frame, you can add names to the list elements during creation. list(seq = 1:3, normVals = rnorm(2), punctuation = c(&quot;!&quot;, &quot;?&quot;)) ## $seq ## [1] 1 2 3 ## ## $normVals ## [1] 0.8170276 -0.1555313 ## ## $punctuation ## [1] &quot;!&quot; &quot;?&quot; To recap, a list is a very flexible 1D object. It is really useful for more complex types of data. The table below gives a summary of the data objects we’ve covered. For most data analysis you’ll use data frames. Dimension Homogeneous Heterogeneous 1d Atomic Vector List 2d Matrix Data Frame Next we look at how to access or change parts of our these common data objects. 1.3.5 Accessing Common Data Objects When we are dealing with a data object (1D or 2D) we may want to extract a single element, certain columns, or certain rows. In this section we’ll look at how to subset or extract information from each of the common data objects covered in the previous section. 1.3.5.1 Atomic Vector (1D) For atomic vectors (and lists, see later) you can return elements using square brackets []. You may notice that when R prints a vector to the console you often see [1] next to the first element and perhaps a [#] where R has to break and move to the next line of the console. The [1] implies the element printed next is the first element of the vector (R starts its counting at 1 not 0 like some other languages). The [#] implies that the element printed to the right is the # element of the vector. This is a good reminder of how to extract values from an atomic vector. As an example, here we extract from a built-in R object called letters that is a vector of length 26 containing the letters of the alphabet. letters #built-in vector ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; letters[1] #R starts counting at 1! ## [1] &quot;a&quot; letters[26] ## [1] &quot;z&quot; To obtain more than one element you can ‘feed’ in a vector of indices to that you’d like to return. letters[1:4] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; letters[c(5, 10, 15, 20, 25)] ## [1] &quot;e&quot; &quot;j&quot; &quot;o&quot; &quot;t&quot; &quot;y&quot; x &lt;- c(1, 2, 5) letters[x] ## [1] &quot;a&quot; &quot;b&quot; &quot;e&quot; If you’d like to return all values except a certain subset, you can use negative indices. letters[-(1:4)] ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; ## [20] &quot;x&quot; &quot;y&quot; &quot;z&quot; x &lt;- c(1, 2, 5) letters[-x] ## [1] &quot;c&quot; &quot;d&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; ## [20] &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; 1.3.5.2 Matrices (2D) For rectangular data like a matrix you can return rectangular subsets using square brackets with a comma [ , ]. Notice default row and column names when R prints a matrix! mat &lt;- matrix(c(1:4, 20:17), ncol = 2) mat ## [,1] [,2] ## [1,] 1 20 ## [2,] 2 19 ## [3,] 3 18 ## [4,] 4 17 This is a nice reminder of how to index a matrix. The value prior to the columns represents which row(s) you want to return and the value after the comma which column(s). If an index is left blank then all of that corresponding dimension (row or column) is returned. mat[c(2, 4), ] ## [,1] [,2] ## [1,] 2 19 ## [2,] 4 17 mat[, 1] ## [1] 1 2 3 4 mat[2, ] ## [1] 2 19 mat[2, 1] ## [1] 2 Notice that R simplifies the result where possible. That is, returns an atomic vector if you have only 1 dimension and a matrix if two. This can be changed by adding an additional argument to the [ function. mat[ , 1, drop = FALSE] ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 Also, if you only give a single value in the [] then R uses the count of the value in the matrix. Counts go down columns first. mat[5] ## [1] 20 If your matrix has column names associated with it, you can also use those to return columns of interest. To add column names we can look run help(matrix) to learn how! Notice the dimnames argument. You can specify names for the rows and columns by using a list with two vectors. The first vector indicating row names and the second column names. If we don’t want to give rownames we can give a NULL (a special value in R that is used for undefined values - here giving no specification of row names). We can do this and give a character vector for the column names. mat&lt;-matrix(c(1:4, 20:17), ncol = 2, dimnames = list(NULL, c(&quot;First&quot;, &quot;Second&quot;)) ) mat ## First Second ## [1,] 1 20 ## [2,] 2 19 ## [3,] 3 18 ## [4,] 4 17 Now we can request columns be using a single name or a character vector of names. mat[, &quot;First&quot;] ## [1] 1 2 3 4 To return all but certain parts of a matrix you can still use negative indices but note that this won’t work with column names. mat[-c(1,3), -&quot;First&quot;] ## Error in -&quot;First&quot;: invalid argument to unary operator mat[-c(1,3), &quot;First&quot;] ## [1] 2 4 1.3.5.3 Data Frames (2D) Since a data frame is also a rectangular data object you can return rectangular subsets using square brackets with a comma [ , ]! As an example, we’ll subset the built-in iris data frame. To get an idea about this object we can run str(iris). str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... We can see this is a data frame with a few columns, four are numeric and one is a factor (a special type of character vector essentially - these will be covered when we discuss plotting). iris[1:4, 2:4] ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 iris[1, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa iris[, 1] ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 Notice the simplification done when a single column is selected. R will simplify to a vector unless drop = FALSE is included as done in the matrix section. (The simplification doesn’t occur when a single row is selected because data frames are actually lists - we’ll discuss this more in the list section!) You can use columns names to subset as well. iris[1:10 , c(&quot;Sepal.Length&quot;, &quot;Species&quot;)] ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa The most common way to access a single columns is to use the dollar sign operator. iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 A nice benefit of using RStudio is that column names will be filled in automatically as you type. In your console do the following: Type iris$ If no choices - hit tab Scroll up and down or continue typing to highlight the column of interest Hit tab again to choose 1.3.5.4 Lists (1D) As a list is a 1D data object we can use single square brackets [ ] for multiple list elements. x &lt;- list(&quot;HI&quot;, c(10:20), 1) x ## [[1]] ## [1] &quot;HI&quot; ## ## [[2]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 ## ## [[3]] ## [1] 1 x[2:3] ## [[1]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 ## ## [[2]] ## [1] 1 We can use double square brackets [[ ]] (or [ ]) to return a single list element. The major difference is in whether or not a list with the element chosen is returned or just the element itself. [[ will return just the element requested. x &lt;- list(&quot;HI&quot;, c(10:20), 1) x[1] ## [[1]] ## [1] &quot;HI&quot; x[[1]] ## [1] &quot;HI&quot; x[[2]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 x[[2]][4:5] ## [1] 13 14 Recall we could name our list elements. If they are named we can use the $ similar to a data frame. x &lt;- list(&quot;HI&quot;, c(10:20), 1) str(x) ## List of 3 ## $ : chr &quot;HI&quot; ## $ : int [1:11] 10 11 12 13 14 15 16 17 18 19 ... ## $ : num 1 x &lt;- list(First = &quot;Hi&quot;, Second = c(10:20), Third = 1) x$Second ## [1] 10 11 12 13 14 15 16 17 18 19 20 Under the hood a data frame is just a list of equal length vectors! str(x) ## List of 3 ## $ First : chr &quot;Hi&quot; ## $ Second: int [1:11] 10 11 12 13 14 15 16 17 18 19 ... ## $ Third : num 1 str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... typeof(x) ## [1] &quot;list&quot; typeof(iris) ## [1] &quot;list&quot; This means we can index a data frame in a similar way to how we index a list if we want. iris[[2]] ## [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 3.7 3.4 3.0 3.0 4.0 4.4 3.9 3.5 ## [19] 3.8 3.8 3.4 3.7 3.6 3.3 3.4 3.0 3.4 3.5 3.4 3.2 3.1 3.4 4.1 4.2 3.1 3.2 ## [37] 3.5 3.6 3.0 3.4 3.5 2.3 3.2 3.5 3.8 3.0 3.8 3.2 3.7 3.3 3.2 3.2 3.1 2.3 ## [55] 2.8 2.8 3.3 2.4 2.9 2.7 2.0 3.0 2.2 2.9 2.9 3.1 3.0 2.7 2.2 2.5 3.2 2.8 ## [73] 2.5 2.8 2.9 3.0 2.8 3.0 2.9 2.6 2.4 2.4 2.7 2.7 3.0 3.4 3.1 2.3 3.0 2.5 ## [91] 2.6 3.0 2.6 2.3 2.7 3.0 2.9 2.9 2.5 2.8 3.3 2.7 3.0 2.9 3.0 3.0 2.5 2.9 ## [109] 2.5 3.6 3.2 2.7 3.0 2.5 2.8 3.2 3.0 3.8 2.6 2.2 3.2 2.8 2.8 2.7 3.3 3.2 ## [127] 2.8 3.0 2.8 3.0 2.8 3.8 2.8 2.8 2.6 3.0 3.4 3.1 3.0 3.1 3.1 3.1 2.7 3.2 ## [145] 3.3 3.0 2.5 3.0 3.4 3.0 Lastly, one nice thing about lists (and data frames) is that you can use partial matching with [[ and $. iris$Sp[1:10] ## [1] setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica iris[[&quot;Petal.Len&quot;, exact = FALSE]] ## [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4 ## [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2 ## [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0 ## [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0 ## [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0 ## [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3 ## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0 ## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 ## [145] 5.7 5.2 5.0 5.2 5.4 5.1 This is less important now that RStudio can auto-complete long column names. 1.3.6 Recap! RStudio IDE (Integrated Development Environment) R Objects and Classes Data Objects &amp; Basic Manipulation Dimension Homogeneous Heterogeneous 1d Atomic Vector List 2d Matrix Data Frame Basic access via Atomic vectors - x[ ] Matrices - x[ , ] Data Frames - x[ , ] or x$name Lists - x[ ], x[[ ]], or x$name 1.3.7 R Reading Data 1.3.8 R Manipulating Data 1.3.9 SAS Section designed to demonstrate basic use of SAS via SAS Studio. By the end of this section students will be able to: write basic SAS programs in SAS Studio using SAS University edition import well formatted data into SAS using the import wizard, PROC IMPORT, or a DATA step create common numerical and graphical summaries of data understand the capabilities of SAS program (something like this but a little clearer maybe) SAS Programming Topics Common elements of SAS programs: PROC and DATA steps Steps, statements, keywords, and options SAS windows Submitting SAS code, checking logs, and viewing output GPP elements Module 2 - Importing Data Topics SAS library Data set names (one vs two level) Methods for importing data: Instream, Import Wizard, and Data step Delimiters and common data formats Types of variables in SAS Explorer and PROC PRINT/CONTENTS for investigating a SAS data set Module 3 - Manipulating Data Topics very basic stuff "],
["sampling-experiments-and-exploratory-data-analysis.html", "Chapter 2 Sampling, Experiments, and Exploratory Data Analysis 2.1 Data in the Wild 2.2 Descriptive Study - Farmer’s Market 2.3 Statistical Testing Ideas - Simulated Experiment 2.4 Statistical Ideas and Concepts 2.5 Software", " Chapter 2 Sampling, Experiments, and Exploratory Data Analysis 2.1 Data in the Wild Data is a collection of information about a group, which may include both quantitative and qualitative variables. Data is ubiquitous in today’s society. Healthcare, marketing, history, biology, … basically every field has a quantitative aspect. The quality of data varies greatly from study to study. 2.1.1 Data from Experiments Some data comes from a well-designed experiment where a researcher uses sound principles to select units and conduct interventions. For example, a mechanical engineer wants to determine which variables influence overall gas mileage of a certain year and model of a car. Gas mileage would be referred to as the response variable for this study. After careful consideration, the engineer chooses to investigate a few explanatory variables. They looked at the following factors that they believed may affect the overall gas mileage: Tire pressure (low, standard) Octane rating of fuel (regular, midgrade, premium) Type of driving (defensive, aggressive) They also choose to control or hold constant the following variables during the implementation of the study: Weather conditions Route Tire type Past car usage The engineer randomly selects 24 cars from the assembly line for that year and model of car (we’ll learn more about the importance of selecting a representative sample of cars shortly). Software is used to randomly assign a treatment or combination of the factors to each car of the 24 cars. For instance, low tire pressure, regulare octane fuel, and defensive driving would be a treatement. The cars would be called the experimental units or (EUs) as they are the unit the treatments are assigned to. The experiment is run and the gas mileage found for each car. As the car is being measured we’d refer to the car as the observational unit. This short description exhibits three important concepts in experimental design that we’ll come back to many times. Experimental Study - researchers manipulate the conditions in which the study is done. Pillars of experimental design: (Put an outer block around this) Randomization - treatments are randomly assigned to the experimental units Replication - multiple (independent) experimental units are assigned the same treatment Control - study conditions are held constant where possible to reduce variability in the response 2.1.2 Data from Observational Studies Some data comes from an observational study where the researcher collects data without imposing any changes. For example, an economist wants to investigate the effects of recently added tariffs on agricultural products to the amount and value of such products that are traded between the United States and Asia. This study would have two response variables, amount and value of each product traded between the two parties. In order to take into account season variation and time of year, the economist decides to compare the two response variables from the current year - 6 months worth of data - to the average values of the two response variables during the same 6 month periods for the past 5 years. We would refer to the time frame of the data as an explanatory variable. This time frame could be labeled to take on one of two values: no-tariff (past) or tariff (current). The researcher obtains the data from the census bureau and conducts their analysis. Notice that the researcher, while certainly being actively involved in the careful consideration of the data to be collected, does not actively intervene or impose a change. This is the key component of an observational study. Observational Study - researchers collects data without imposing any changes on the study environment. 2.1.3 Observational vs Experimental You may have noticed that both types of studies have some things in common. For instance, both studies have response (??? so I was thinking about maybe bolding most stats words as we go to point them out to students… thoughts???) variables that characterizes the performance of the study in some sense. Importantly, these response variables have variation. That is, observing the variable is non-deterministic even under identical situations. There are also explanatory variables that the researcher is interested in with regard to their relationship with the response variable. Beyond that, both studies hope to make conclusions about a larger group using data. This is the idea of statistical inference (??? Do we want to talk about the differences between prediction and inference here? - later???). More formally the group of values, items, or individuals defines the a population of interest and the data collected represents the sample. For the gas mileage example, the population would be all cars of the year and make in question and the sample would be the data on the 24 cars. For the tariff example, the population would be a conceptual population of all future agricultural products traded between the United States and Asia and the sample would be the information from the six years of trade data. Population - (Possibly conceptual) group of units of interest Sample - Subset of the population on which we observe data Statistical Inference - Process of using sample data to make statements or claims about a population (???Usually with the goal of determing which variables are important for a response???) Both of these studies had to determine how to obtain their observations. For the experiment, 24 cars were used. For the observational study, six years of data were collected. How this data is collected can be extremely important in terms of the types of conclusions that can be made. Data needs to be unbiased and representative of the population in which the researcher hopes to make inference otherwise the conclusions made are likely invalid. We’ll discuss the idea of what makes a good and bad sampling scheme later. The major difference between the two studies was the active (experimental) and passive (observational) roles played by the researcher. This difference is also of vital importance to the types of conclusions that can be made from the study. A well-designed experiment can often infer causation to the treatments where an observational study cannot. The conclusions a researcher can make based on how the data were collected and the type of study are outlined in the table below. (??? Probably just remake this table ourselves with our own words. This isn’t exactly ‘their’ original thought or something we need to attribute. ???) Figure 2.1: Scope of Inference, cite: Khan Academy Doing an observational study doesn’t mean that your study is bad! An observational study is sometimes done out of necessity when an experiment wouldn’t be ethical or feasible. For the tariff example, there really isn’t a way to conduct an experiment. If we wanted to design an experiment to see if smoking causes lung cancer, that would be unethical because we can’t force people to smoke. The key point is that the implications we can draw will differ greatly between experimental and observational studies and will depend heavily on the quality (in relation to the population) of the data you have. 2.1.4 The Role of Statistics Statistics is the science of learning from data. It encompasses the collection of data, the design of an experiment, the summarization of data, and the modeling or analysis used in order to make a decision or further scientific knowledge. (???I feel like this definition doesn’t quite get the sampling part right or maybe the holistic process or something - update as needed! JP???) (This will be changed to a different style of callout - maybe “note”?) Statistics in every day use usually refers to simply summaries about data (means/averages, proportions, or counts). Statistics as a field encompasses a much larger range of ideas including how to collect data, model data, and make decisions or come to conclusions when faced with uncertainty. Statistical methods are needed because data is variable. If we again collected data about the gas mileage of vehicles under the exact same study conditions we’ll get slightly different results. If we observed another six month period of trade data we’ll see different amounts and values. Accounting for this variability in data is a key component of a statistical analysis. Generally, one should try to take a holistic view of a study. Before any data is collected it is vital to understand the goals and background of the study. These will inform the data you ideally want to collect as well as the data that you are able to collect - which may need to act as a proxy. A plan should be determined for the actual collection and storing of the data. The entire study design will then inform the statistical analysis and conclusions that can be drawn. Taking this bigger picture view of the problem, we can usually follow these steps (we’ll try to follow these throughout the book!): Define the objective of the experiment and understand the background (Define Objective &amp; Background) Select appropriate response variables (Select Response) Identify sources of variation (Determine Sources of Variation) Choose sampling scheme and/or experimental design (Select Design) Carry out the study (Do Study) Statistically analyze the data (Do Statistical Analysis) Draw conclusions from the analysis while considering limitations and the steps above as well as communicate results (Draw Conclusions &amp; Communicate) We’ll focus on this entire process in our big chapter examples and mostly investigate designed experiments. We attempt to tackle each topic in this text with a problem-based approach. That is, we identify a real-world problem and discuss the relevant statistical ideas in context. At the end of each chapter we provide a recap the main statistical ideas and discuss other important related topics. Where applicable we include a section that outlines some of the mathematical concepts as well as a section to outline software related to the topic. 2.2 Descriptive Study - Farmer’s Market 2.2.1 Define Objective &amp; Background A nutrition scientist wanted to understand the cleanliness and food hygiene of the vendors at the North Carolina State Farmer’s Market (henceforth the farmer’s market). Secondarily, she wanted to learn about vendor sales to see if there was a relationship with their cleanliness and food hygiene. The researcher had access to the names of each vendor’s business, their general purpose, and the products they sold. The researcher needed to decide the scope of their study. Formally, they needed to define the population of interest. The population is the group of people or units of interest to the researcher. As her interest centered around food related businesses, she restricted to looking at the vendors which sold horticultural crops. She hoped that conclusions made by her study could apply to all horticulture vendors at the farmer’s market - thus, this is her population. Note: One could try to do a study at just the North Carolina State Farmer’s Market and extend the results to all farmer’s market in the state or in the south, but that would require many assumptions to be valid. A list of the horticultural products sold and their is availability is reproduced below. 2.2.2 Select Response The researcher needed to determine the variables to collect that would best help to answer their questions of interest. These variables that characterize the experiment are called response or target variables. To investigate the knowledge of hygiene and safety, a short questionnaire was developed to allow the vendor’s head manager (or similar employee) to describe their safety protocol and knowledge: For your produce with signs that say “clean” or “washed”, what does this mean? How are the foods transported to the market? eg: refrigerated/closed storage What food safety risks do you as a vendor worry about? Do you require one-use gloves to be used? (Yes or No) Do you designate a person in charge of money transactions? (Yes or No) The researcher also planned to do an assessment of the cleanliness of each vendor’s station at different times. Her team would pick 30 days during the summer in which they’d walk through the vendor stations and collect the following information: Overall is the station clean (Yes or No) Is anyone smoking around the food products? (Yes or No) Are tables covered? (Yes or No) If so, what is the material? Do employees appear to be clean? (Yes or No) Are one-use gloves used? (Yes or No) Is there a designated person in charge of money transactions (Yes or No) She noted that there is a yearly cycle to the products sold and decided to collect vendors sales information by looking at the (AMT) amount sold in the last year (in dollars), the (PURCHASE) total number of purchases made in the last year, and the (NUM_ITEMS) total number of items sold in the last year. For the last variable they had to decide how to measure the number of items sold for the different types of crops. For most of the crops looking at the total weight (in lbs) sold made sense. For some other measures were needed. For example, for sweet corn the number of ears sold would be recorded. You can see that there are many decisions that the researcher must make in simply deciding the response variables to collect! A poor choice here can make or break a study. 2.2.3 Determine Sources of Variation The response variables clearly have some relationship to other variables that could be collected. For instance, the NUM_ITEMS variable is clearly going to be different based upon what crops the vendor sells. The AMT variable would differ depending on the size of the vendor’s inventory. These are examples of explanatory variables or variables that define the study conditions. Explanatory variables go by many names such as predictors, features, or independent variables. A main consideration about whether or not to record a variable is whether or not the variable would be related to a variation in a response variable. Since the response variables are truly what is of interest, there is really not much of a point in recording variables that likely have no relationship with it. Choosing the explanatory variables can also indicate further questions of interest. For instance, the researcher may want to compare the percent of “Yes” for the overall cleanliness score for vendors that mainly sell vegetables to those that mainly sell fruit leading to a comparison across groups being of interest. She may want to try to model the AMT of canteloupe sold as a function of cleanliness score. The average amount for the population or a subpopulation would be referred to as a parameter of interest. Formally, a parameter is a summary measure about a population. Common parameters investigated include a mean, proportion, median, or variance of different subgroups of the population. The explanatory variables she collected about the vendors included the types of crops sold, the services they provide (grow, pack, and/or ship), and whether or not they are a “Got to be NC member”. For the questionnaire, she added the additional questions below: Are there any organic or synthetic chemicals/fertilizers/pesticides/manures used on the products? Are all foods grown/processed by the vendors? What kind of soil were the products grown in? eg: organic/compost/plant material For the assessment of cleanliness, she added the following question: How many people are working? Should we talk about formalizing the other questions they want to answer here?? 2.2.4 Select Design For this study the researchers aren’t interested in doing an intervention so an observational study was being done. The major task to consider for the observational study is how to select participants from the population. The subset of the population we (attempt to) observe our data on is called the sample. The sample size is the number of measurements in the sample. Ideally we would measure every member of our population. This is called a census. If a census can be done then values of a population’s parameters can be found exactly by simply summarizing the population data. However, conducting a census can be extremely costly or time intensive so most of the time a census cannot be done. This means that the information we collect would likely be different if we collected it again. Accounting for this variability is the main reason statistical analysis is needed. How the researcher selects their sample is extremely important. This method is often referred to as the sampling scheme. Using a statistically valid sampling scheme is vital to the assumptions made when doing statistical inference. A valid sampling scheme implies that every member of the population has a known and non-zero chance of inclusion in the sample. There are many good ways to select the sample and many bad ways. Need to get more info about the farmer’s market to finish this part (Talk about bad first and why bad - visuals too) Talk about good and why good - visuals too. This idea is further fleshed out at the end of the chapter. (reference/link this) Here they chose to do a stratified sample to make sure that they didn’t leave out any important subgroups. Should we talk about formalizing the other questions they want to answer here?? 2.2.5 Do Study Go and talk to chosen vendors. May have some non-response issues. Ideally a contingency for this should be developed when considering the sampling scheme. Should we talk about formalizing the other questions they want to answer here?? 2.2.6 Do Statistical Analysis Should we talk about formalizing the other questions they want to answer here?? The major goals of this study were simply to describe the vendors at the farmer’s market. In this case we can produce numerical and graphical summaries. Careful discussion of not selecting a modeling technique based on this unless it is a pilot study or an exploratory study else we have increased our nominal type I error rate… Spend a lot of time here talking about graphs of different types. Sample means, sample variances, etc. Discuss population curves vs sample histograms and the relationship. Not a formal test here but comparisons of interest etc. 2.2.7 Draw Conclusions &amp; Communicate What actionable things have we found? Likely some trends to investigate further. Perhaps run an experiment to formally see if some alteration can be effective. What can we conclude realistically from this data? To what population are we talking? 2.3 Statistical Testing Ideas - Simulated Experiment 2.3.1 Define Objective &amp; Background 2.3.2 Select Response 2.3.3 Determine Sources of Variation 2.3.4 Select Design 2.3.5 Do Study 2.3.6 Do Statistical Analysis 2.3.7 Draw Conclusions &amp; Communicate 2.4 Statistical Ideas and Concepts 2.4.1 Study Purpose Prediction vs inference Param, sample, etc. 2.4.2 Summarizing Data Terminology of variables Numerical and graphical summaries Sample of Random Variable’s realizations, sample distribution vs population, modeling ideas Approx probabilities and quantiles vs theoretical Summaries of distributions (center, spread, graphs) 2.4.3 Study Types Ob vs exp Good discussion of what makes a good sampling design. Maybe a statified example like the river and selecting houses example as a quick expose of the issues with not doing a truly random sampling technique. 2.4.3.1 Examples of why Observational Studies Can be Bad George Will WP column about SAT vs amount of money spent Pisani and Perv? many bad examples of inference 2.4.4 Suggestions for Further Readings 2.5 Software 2.5.1 R 2.5.1.1 Numerical Summaries 2.5.1.2 Graphical Summaries 2.5.2 SAS 2.5.2.1 Numerical Summaries 2.5.2.2 Graphical Summaries "],
["point-estimates.html", "Chapter 3 Point Estimates 3.1 Estiamte with means 3.2 Estimate with quantiles", " Chapter 3 Point Estimates Learning objectives for this lesson: - How to estimate a mean - Definition of “convenience sample” - Definition of “systematic sample” - Benefits/drawbacks to both approaches - Understand how to estimate a mean - Understand how to estimate a quantile - Understand implicit assumptions for these approaches 3.1 Estiamte with means 3.1.1 Experiment background Someone wants to know how much of something they need to satisfy some population To get a good estimate of this, we can use the average amount for each one and then multiply by the whole population 3.2 Estimate with quantiles 3.2.1 Experiment background Big Deborah’s is making new packaging for their cookies. The engineer responsible for the new desing needs to make sure that the packaging fits the new cookies. While the cookie manufacturing process is standardized, there’s inevitably some degree of variation in cookie size. After discussing the issue with corporate, the engineer decides that a the new cookie sleeves should be large enough to fit 95% of cookies that are baked. (The largest five percent will be marketed separately as “JUMBO” cookies.) 3.2.2 Define the object of the experiment The Engineer is tasked with determining how large the cooke sleeve needs to be. There’s no way for her to know the size of every cookie that Big Deborah’s has made (or will make going forward!), so she’ll need to collect data on existing cookies to inform her cookie sleeve size determination. 3.2.3 Select appropriate response variables If the maximum distance from any one point on the (round) cookie’s perimeter to any other point is smaller than the diameter of the cookie sleeve, then the cookie will fit. This makes “cookie diameter” a good measure for this test. It is easy to measure for each cookie and is directly relevant to the experiment’s objective. [probably have something in here about ] 3.2.4 IDentify sources of variation While the manufacturing process is standardized, there is variation in size from one cookie to the next. This is one source of variation. The engineer isn’t sure of any others. However, she knows that cookies are made in multiple factories, and that each factory has multiple ovens. Ovens and factories could also be sources of variation. 3.2.5 Choose an experimental design The Engineer knows that she needs to look at multiple cookies, since she knows that there is variation in diameter from one cookie to the next. One option would be to just use the remaining cookies in the box she has in her office (22 of the 25-count box remain). [something about convenience sample] However, she knows that cookies from the same oven are typically packaged together. If there is variation from one oven to the next, looking at the cookies she has in her office may not tell the whole story. Instead, she chooses to take every 20th cookie manufactured off the assembly line until she gets 500 cookies. [something about systematic sample] 3.2.6 Perform the test The day of the test comes, and the Engineer starts collecting cookies. However, problems arise! The plan has to shut down half-way through, so she only gets 431 cookies instead of the 500 she thought she would. However, she measures the diameters of each cookie and records the data in a spreadsheet. 3.2.7 Statistically analyze the data The initial plan had been to rank-order the 500 cookies and estimate the 95th percentile using the diamter of the 475th largets cookie. Since we didn’t get all of our data, we have to improvise. 431 doesn’t neatly yield a value such that exactly 95% are less than or equal and 5% are greater than or equal. One option is to choose the 410th largest cookie to estimate our percentile. Slightly more than 95% of cookies will have smaller diameters than this. Alternatively, we could interpolate between the 409th and 410th cookies. [reasons and logic and math for each of these] 3.2.8 Draw conclusions Based on this study, the Engineer concludes that a cookie sleeve large enough for a cookie of diameter XX will be big enough to contain 95% of Big Deborah cookies. 3.2.9 Discussion pros and cons to the approach chosen generalizing to other types of point estimates "],
["accounting-for-uncertainty.html", "Chapter 4 Accounting for Uncertainty 4.1 Example one 4.2 Example two", " Chapter 4 Accounting for Uncertainty Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["HT.html", "Chapter 5 Inference via Hypothesis Tests for One Sample 5.1 Example: Detection Probability 5.2 Marketing Example 5.3 Statistical Testing Ideas 5.4 Software 5.5 Finding a specific value from a Probability Distribution Function or Probability Mass Function 5.6 Finding the area under the curve 5.7 Finding quantiles of distributions 5.8 Generating random numbers 5.9 Other distributions", " Chapter 5 Inference via Hypothesis Tests for One Sample 5.1 Example: Detection Probability An important problem for the US Army is protecting soldiers in forward positions. One of main threats to these soldiers are explosive projectiles launched from great distances. These are often refered to as “indirect fires”, since the folks launching the projectiles (“firing” the projectiles) may not have direct visibility (“line of sight”) to their target. For example, artillery like the US M777 Light Towed Howitzer can fire projectiles over 40 km, far beyond the distance a soldier operating the machine could aim it. Instead, the gun crew relies on information about where to shoot provided to them by other units on the battlefield. This allows them to effect the battle from far away and without alerting their target. To protect soldiers from similar threats, the US Army developed the Q-53 Counterfire Radar. One of this system’s primary functions is to detect incoming indirect fires and pinpoint the source of those shots. Because soliders will be relying on the Q-53 in combat, the US Government tested it extensively to better understand how it would perform in an operational setting. The testers fired shells from systems that closely resembled the artillery that might be seen on the battlefield while actual soldiers operated a Q-53. For each shell that was fired, the testers recorded whether or not the Q-53 crew was able to detect the projectile with their system. Using data from this test, evaluators were able to the likelihood that the Q-53 would detect an incoming projectile. 5.1.1 Understanding system performance Let’s take a step back and consider the data we’re getting from this test and try to understand why the testers did things the way they did. First, let’s consider the goal of this test. At the end of the day, the US Army wants to know how effective the Q-53 is at it’s job. This job is to help protect soldiers by giving them early warning if they’re under attack from indirect fires like artillery. Therefore, it’s vital that the Q-53 detect incoming projectiles with high probability. Once we understand this, the choice to measure the detection probability for incoming projectiles is completley logical. This measure ties directly to the goal of the experiment. You won’t always be able to find a response variable that ties so directly to the goal of your test, but when you do, rejoice. There are some other measures the testers could’ve used. For example, they could’ve looked at how long a projectile was detected prior to its impact on the target. Similarly, they could’ve measured the distance from the target at time of detection. Both of these measures would give more detailed information than detection alone. However, they’d be harder to measure precisely. Addiing additional instrumentation to the Q-53 and to the test projectiles being fired at the range would add elements to the experiment that would make it less realistic. Perhaps instrumenting the projectiles would make them easier to detect. Regardless, the testers deemed the detection probability adequate to make their assessment of the system. 5.1.2 Data Analysis Having identified their response variable, the testers collected data on which projectils were detected. Table 1 shows the first 20 shots collected. x &lt;- rbinom(20, 1, .8) hist(x) It’s clear that the system detects these projectiles with a high probability, but what conclusions can we draw from these data? Does the system detect projectiles at a high enough rate to be useful to a commander in the field? 5.1.3 Data from Experiments Some data comes from a well-designed experiment where a researcher uses sound principles to select units and conduct interventions. For example, a mechanical engineer wants to determine which variables influence overall gas mileage of a certain year and model of a car. Gas mileage would be referred to as the response variable for this study. After careful consideration, the engineer chooses to investigate a few explanatory variables. They looked at the following factors that they believed may affect the overall gas mileage: Tire pressure (low, standard) Octane rating of fuel (regular, midgrade, premium) Type of driving (defensive, aggressive) They also choose to control or hold constant the following variables during the implementation of the study: Weather conditions Route Tire type Past car usage The engineer randomly selects 24 cars from the assembly line for that year and model of car (we’ll learn more about the importance of selecting a representative sample of cars shortly). Software is used to randomly assign a treatment or combination of the factors to each car of the 24 cars. For instance, low tire pressure, regulare octane fuel, and defensive driving would be a treatement. The cars would be called the experimental units or (EUs) as they are the unit the treatments are assigned to. The experiment is run and the gas mileage found for each car. As the car is being measured we’d refer to the car as the observational unit. This short description exhibits three important concepts in experimental design that we’ll come back to many times. Experimental Study - researchers manipulate the conditions in which the study is done. Pillars of experimental design: (Put an outer block around this) Randomization - treatments are randomly assigned to the experimental units Replication - multiple (independent) experimental units are assigned the same treatment Control - study conditions are held constant where possible to reduce variability in the response 5.1.4 Data from Observational Studies Some data comes from an observational study where the researcher collects data without imposing any changes. For example, an economist wants to investigate the effects of recently added tariffs on agricultural products to the amount and value of such products that are traded between the United States and Asia. This study would have two response variables, amount and value of each product traded between the two parties. In order to take into account season variation and time of year, the economist decides to compare the two response variables from the current year - 6 months worth of data - to the average values of the two response variables during the same 6 month periods for the past 5 years. We would refer to the time frame of the data as an explanatory variable. This time frame could be labeled to take on one of two values: no-tariff (past) or tariff (current). The researcher obtains the data from the census bureau and conducts their analysis. Notice that the researcher, while certainly being actively involved in the careful consideration of the data to be collected, does not actively intervene or impose a change. This is the key component of an observational study. Observational Study - researchers collects data without imposing any changes on the study environment. 5.1.5 Observational vs Experimental You may have noticed that both types of studies have some things in common. For instance, both studies have response (??? so I was thinking about maybe bolding most stats words as we go to point them out to students… thoughts???) variables that characterizes the performance of the study in some sense. Importantly, these response variables have variation. That is, observing the variable is non-deterministic even under identical situations. There are also explanatory variables that the researcher is interested in with regard to their relationship with the response variable. Beyond that, both studies hope to make conclusions about a larger group using data. This is the idea of statistical inference (??? Do we want to talk about the differences between prediction and inference here? - later???). More formally the group of values, items, or individuals defines the a population of interest and the data collected represents the sample. For the gas mileage example, the population would be all cars of the year and make in question and the sample would be the data on the 24 cars. For the tariff example, the population would be a conceptual population of all future agricultural products traded between the United States and Asia and the sample would be the information from the six years of trade data. Population - (Possibly conceptual) group of units of interest Sample - Subset of the population on which we observe data Statistical Inference - Process of using sample data to make statements or claims about a population (???Usually with the goal of determing which variables are important for a response???) Both of these studies had to determine how to obtain their observations. For the experiment, 24 cars were used. For the observational study, six years of data were collected. How this data is collected can be extremely important in terms of the types of conclusions that can be made. Data needs to be unbiased and representative of the population in which the researcher hopes to make inference otherwise the conclusions made are likely invalid. We’ll discuss the idea of what makes a good and bad sampling scheme later. The major difference between the two studies was the active (experimental) and passive (observational) roles played by the researcher. This difference is also of vital importance to the types of conclusions that can be made from the study. A well-designed experiment can often infer causation to the treatments where an observational study cannot. The conclusions a researcher can make based on how the data were collected and the type of study are outlined in the table below. (??? Probably just remake this table ourselves with our own words. This isn’t exactly ‘their’ original thought or something we need to attribute. ???) Figure 5.1: Scope of Inference, cite: Khan Academy Doing an observational study doesn’t mean that your study is bad! An observational study is sometimes done out of necessity when an experiment wouldn’t be ethical or feasible. For the tariff example, there really isn’t a way to conduct an experiment. If we wanted to design an experiment to see if smoking causes lung cancer, that would be unethical because we can’t force people to smoke. The key point is that the implications we can draw will differ greatly between experimental and observational studies and will depend heavily on the quality (in relation to the population) of the data you have. 5.1.6 The Role of Statistics Statistics is the science of learning from data. It encompasses the collection of data, the design of an experiment, the summarization of data, and the modeling or analysis used in order to make a decision or further scientific knowledge. (???I feel like this definition doesn’t quite get the sampling part right or maybe the holistic process or something - update as needed! JP???) (This will be changed to a different style of callout - maybe “note”?) Statistics in every day use usually refers to simply summaries about data (means/averages, proportions, or counts). Statistics as a field encompasses a much larger range of ideas including how to collect data, model data, and make decisions or come to conclusions when faced with uncertainty. Statistical methods are needed because data is variable. If we again collected data about the gas mileage of vehicles under the exact same study conditions we’ll get slightly different results. If we observed another six month period of trade data we’ll see different amounts and values. Accounting for this variability in data is a key component of a statistical analysis. Generally, one should try to take a holistic view of a study. Before any data is collected it is vital to understand the goals and background of the study. These will inform the data you ideally want to collect as well as the data that you are able to collect - which may need to act as a proxy. A plan should be determined for the actual collection and storing of the data. The entire study design will then inform the statistical analysis and conclusions that can be drawn. Taking this bigger picture view of the problem, we can usually follow these steps (we’ll try to follow these throughout the book!): Define the objective of the experiment and understand the background (Objective &amp; Background) Select appropriate response variables (Response) Identify sources of variation (Sources of Variation) Choose experimental design (if applicable) (Experimental Design) Perform the test/collect the data (??? not sure how to shorten that to make it make sense ???) Statistically analyze the data (Analysis) Draw conclusions (Conclusions) We’ll focus on this entire process and mostly investigate designed experiments. We attempt to tackle each topic in this text with a problem-based approach. That is, we identify a real-world problem and discuss the relevant statistical ideas in context. Summaries at the end of each chapter recap the main statistical ideas. 5.2 Marketing Example 5.2.1 Experiment Background Marketing example. Goal to describe the customers, how they tend to purchase/shop, and maybe find some shared qualities in order to adverstise curated packages to folks. Define basic things like population, parameters, statistics, and sample. Discuss conceptual vs actual populations and when we might care about one or the other. Our “sample” is really a bit of data from the conceptual population. Or we could consider it as the population and we just want to describe it. 5.2.2 Selecting Response Variables We don’t get to choose the variables here as the analytics company gives us what they deem important. We can however still think critically about what is important. Marketing example with data such as Clicks, Impressions, Total Revenue, Total Spent, Average Order Value, Sport, Time of visit/purchase, Campaigns running, etc. 5.2.3 Identifying Sources of Variation Consider variables linked to the user. Age, other accounts, etc. 5.2.4 Choose an Experimental Design Discuss our “sampling” scheme vs a random sample. This seems like a case where we aren’t doing a “good” scheme but not much else could be done… Maybe talk about how in the future you could do alternate email ads or something and do an AB type study. 5.2.5 Peform the Test Get the data from google analytics or whatever, have a plan for updating each month? 5.2.6 Look at the Data Careful discussion of not selecting a modeling technique based on this unless it is a pilot study or an exploratory study else we have increased our nominal type I error rate… (sometimes EDA sometimes data validation only/cleaning - more formal experiments) Spend a lot of time here talking about graphs of different types. Sample means, sample variances, etc. Discuss population curves vs sample histograms and the relationship. 5.2.7 Statistically Analyze the Data New variables as functions of old? Not a formal test here but comparisons of interest etc. 5.2.8 Draw conclusions What actionable things have we found? Likely some trends to investigate further. Perhaps run an experiment to formally see if some alteration can be effective. What can we conclude realistically from this data? To what population are we talking? 5.3 Statistical Testing Ideas 5.3.1 Experiment Background This example would lend itself to a reasonably easy randomization test or simulation based test. Maybe an AB type study where we swap labels and do that with a nice visual. Maybe third example with simulation test. 5.3.2 Selecting Response Variables 5.3.3 Identifying Sources of Variation 5.3.4 Choose an Experimental Design Good discussion of what makes a good sampling design. Maybe a statified example like the river and selecting houses example as a quick expose of the issues with not doing a truly random sampling technique. Basics of experimental design (randomization, replication, error control ideas). Recap benefits of doing an experiment vs an observational study. 5.3.5 Peform the Test 5.3.6 Explore the Data NHST paradigm with false discovery? 5.3.7 Statistically Analyze the Data 5.3.8 Draw conclusions 5.4 Software 5.4.1 R For each built-in distribution in R (Normal, Bernoulli, Binomial, Student’s t, etc.), R has four basic functions. We’ll go through them here and discuss briefly their uses. 5.5 Finding a specific value from a Probability Distribution Function or Probability Mass Function For the normal distribution, this is dnorm(). In the plot below, the red point is the point along the distribution function of a standard normal when x = -1. By input dnorm(-1), R evaluates the function and returns 0.2419707. ## [1] 0.2419707 I mostly use this for discrete random variables, like dbinom(), since those probability values are more often meaningful on their own (i.e., without integrating). 5.6 Finding the area under the curve pnorm(q) returns the area under the normal curve up to some value on the x-axis, q. This is good for finding p-values when you have a critical value, q. pnorm(-1) ## [1] 0.1586553 5.7 Finding quantiles of distributions qnorm(p) returns the value along the x-axis such that the area under the normal up to that point is equal to p. The example below illustrates how you’d get the critical value for the lower bound of a 95% confidence interval based off a Normal approximation. qnorm(0.025) ## [1] -1.959964 This is useful when you want to find, for example, a critical value to build a confidence interval or test a hypothesis. 5.8 Generating random numbers The last function in this set is rnorm(), and it doesn’t really go with the rest. This function generates random variables based on the distribution you give it. Quite useful for doing simulation studies and the like, but it rarely comes up when you’re doing inference. 5.9 Other distributions R has a lot of built in distributions, including the Binomial (pbinom()), Student’s t (pt()), the Chi-square (pchisq()), the F-distribution, (pf()), the Beta (pbeta()), and the Gamma (pgamma()). They all have functions similar to the ones listed above that behave in similar ways, though note that the way parameters are input to these functions can be weird. Make sure to read the notation! The implications for the conclusions that can be made from a set of data varies greatly with the quality of the data and study design. "],
["CI.html", "Chapter 6 Inference via Confidence Intervals for One Sample 6.1 The normal approximation 6.2 Other types of binomial confidence intervals", " Chapter 6 Inference via Confidence Intervals for One Sample There are many ways to build confidence intervals for sample proportions. Here are a few: 6.1 The normal approximation This is the basic interval they’ve taught in introductory statistics courses since time immamorial. Or at least the past few decades, I’d have to know the history of Stats Ed to give the real timeframe. Anyway, this confidence interval uses the fact from the Central Limit Theorem, that, as \\(n \\rightarrow \\infty\\), the sampling distribution for \\(\\hat\\pi = x/n\\) closely resembles a Normal distribution. Based on that, you get the equation: \\[\\hat\\pi \\pm z_{\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat\\pi (1 - \\hat\\pi)}{n}}\\] 6.1.1 Analog CI We can build this CI in R pretty easily by inputting the values for the sample size, \\(n\\), and the number of “successes” or “1”s from our binary response variable. One example from class discusses a poll of 2500 people with 400 responding “Satisfactory”. For a 90% confidence interval, we have: n &lt;- 2500 x &lt;- 400 pihat &lt;- x/n alpha &lt;- 0.1 # 90% CI --&gt; alpha = 1 - .9 = 0.1 lower_bound &lt;- pihat + qnorm(alpha/2) * sqrt((pihat * (1 - pihat)/n)) upper_bound &lt;- pihat + qnorm(1 - alpha/2) * sqrt((pihat * (1 - pihat)/n)) c(lower_bound, upper_bound) ## [1] 0.1479397 0.1720603 6.1.2 Easy mode But it’s much easier to just use the binom library, which contains the function binom.confint(): # install.packages(&quot;binom&quot;) library(binom) ## Warning: package &#39;binom&#39; was built under R version 3.5.3 binom.confint(x = 400, n = 2500, conf.level = 0.9, method = &quot;asymptotic&quot;) ## method x n mean lower upper ## 1 asymptotic 400 2500 0.16 0.1479397 0.1720603 Much easier! But now that we’re using binom.confint(), we discover that we have to specify method = &quot;asymptotic&quot;. But that implies that there are alternatives! And indeed, if we just remove that statement, we see that there are almost a DOZEN different methods that binom.confint() will compute for you! 6.2 Other types of binomial confidence intervals First off, most of these aren’t useful in most cases. They’re in there because (1) they’re not very hard to program, so the authors figured, “Why not?” and (2) in most cases, there is at least one circumstance where each one is the best option. (Or they’re included for historical reasons.) 6.2.1 Exact CIs, aka Clopper-Pearson For one simple example, recall the assumption that we always have to make for our Normal approximation method: \\(n * \\hat\\pi &gt; 5\\) and \\(n * (1 - \\hat\\pi) &gt; 5\\). This is required when we use the Normal approximation. It means we can’t build CIs for small-ish samples. But other methods don’t have this problem! method = &quot;exact&quot; uses what’s called the Clopper-Pearson method, which uses the Binomial distribution to calculate an “exact” confidence interval rather than rely on an approximation. While being “exact” sounds better than “approximate”, the truth of the matter is that the Clopper-Pearson interval is generally wider than it needs to be, meaning you get a less precise interval: library(dplyr) binom.confint(x = 400, n = 2500, conf.level = 0.9) %&gt;% mutate(`CI Width` = upper - lower) %&gt;% select(method, lower, upper, `CI Width`) %&gt;% arrange(`CI Width`) ## method lower upper CI Width ## 1 bayes 0.1480550 0.1721635 0.02410856 ## 2 cloglog 0.1481500 0.1722628 0.02411279 ## 3 profile 0.1481871 0.1723036 0.02411651 ## 4 wilson 0.1483082 0.1724269 0.02411870 ## 5 probit 0.1482369 0.1723573 0.02412042 ## 6 asymptotic 0.1479397 0.1720603 0.02412053 ## 7 logit 0.1483044 0.1724312 0.02412679 ## 8 agresti-coull 0.1483026 0.1724325 0.02412988 ## 9 lrt 0.1481877 0.1723265 0.02413880 ## 10 exact 0.1480388 0.1725544 0.02451559 ## 11 prop.test 0.1459601 0.1750977 0.02913765 Since we have a large sample, the differences aren’t very large, but there are times when you want every ounce of precision you can get! 6.2.2 Bayesian intervals Bayesian statistics is a school of thought that says we should try to incorporate our prior knowledge about a problem when making a decision instead of letting the data stand on its own.I don’t want to get into why some folks prefer Bayesian intervals, but if you want to, just specify method = &quot;bayes&quot; to get a Bayesian CI. 6.2.3 A good general-use CI My go-to for a simple binomial confidence interval is the Agresti-Coull method, method = &quot;agresti-coull&quot;. It’s one of the weirder ones (Seriously, go look at the equation for it!), but generally performs as well or better than the competition across most scenarios. It’s more precise than method = &quot;exact&quot;, doesn’t fail in small samples like method = &quot;asymptotic&quot;, and doesn’t rely on a Bayesian approach. "],
["twocategorical.html", "Chapter 7 Inference for Two Categorical Variables", " Chapter 7 Inference for Two Categorical Variables We have finished a nice book. "],
["anova.html", "Chapter 8 One-Way ANOVA", " Chapter 8 One-Way ANOVA Learning objectives for this lesson: - Write one-way ANOVA model - Define terms - state assumptions - interpret results - Interpret ANOVA table - Describe SSE, SST, MSE - F-statistic - degrees of freedom - understand how all of these interrelate - Understand how to compare mulitple group means how ANOVA is similar/different to t-tests - Understand partitioning of variation and coefficient of determination "],
["motivating-example.html", "Chapter 9 Motivating example", " Chapter 9 Motivating example The United States Air Force Academy has 24 sections of Calculus I, taught by three different types of instructors: In-uniform instructors, full-time civilian instructors, and visiting faculty. The Dean of Students wants to give students the best experience possible and make sure that all three types of instructors are doing a good job. There are plausible reasons why any one of the three could be doing well: In-uniform instructors are all members of the Air Force, and students may be extra attention in these classes because they know that these instructors rank above them in their chain of command. On the other hand, full-time instructors have been aroudn the Academy for many years and understand the Cadets and their workloads. Alternatively, visiting facutly tend to come from prestigeous institutions and may be familiar with more recently-developed pedagogical techniques. Regardless, the Dean wants to understand if there is any variation in end-of-semester grades of classes taught by these three types of instructors. At the end of the semester, she collects the average grades from each of the 24 sections. How can she go about investigating this question? Recall from Chapter 6 that we can use t-tests to compare two group means. In this case, we’d like to do a comparison across three groups, and instead of looking at a direct comparison of one group to another, what the Dean is interested in is whether there’s an overall difference across the three groups. One option might be to just do a bunch of different t-tests. We could first compare classes taught by in-uniform instructors to classes taught by full-time civilians, then compare the classes taught by the in-uniform instructors to the classes taugth by the visiting instructors, and then finally compare the classes taugth by the full-time civilains with the classes taught by the visiting facutly. We’d end up with three p-values, each addressing different questions than the one we initially set out to answer. We could do the same thing, except comparing courses taught by one type of instructor to the combined group of courses taught by the other two, and this gets a bit closer to the mark. But we’re still doing three tests that individually fail to answer the Dean’s question. What we’d like instead is a single hypothesis that we could test that direclty gets at the Dean’s concern about whether the three types of instructors were producing end-of-semester grades that were, on average, the same. [Need to make that motivation clearer above.] "],
["simple-model-for-the-data.html", "Chapter 10 Simple model for the data", " Chapter 10 Simple model for the data Narrative explanation that instructor type might matter, there shold be some variation from class to class. - write some things in greek, including model without any difference by instructor type - wirte model with differences by instructor type - note that we can use Gaussian errors b/c Academy grades do actually tend to be centered around a C, particularly for classes like Calc - discuss model assumptions in general sense "],
["exploratory-analysis.html", "Chapter 11 exploratory analysis", " Chapter 11 exploratory analysis course-to-course variability is expected maybe show a plot of it or something visualize groups using box-and-whisker plots "],
["sources-of-variation.html", "Chapter 12 sources of variation", " Chapter 12 sources of variation Things like student population, time of day, etc. But we’ll throw this all into an error term and focus on the main one, instructor type "],
["statistical-model-and-analysis.html", "Chapter 13 statistical model and analysis", " Chapter 13 statistical model and analysis ANVOA model explicit w/ assumptions variation around overall mean w/ no groups variation around group means introduce idea of reference level "],
["compare-analyses.html", "Chapter 14 compare analyses", " Chapter 14 compare analyses t-test methods from above ANOVA method compare and contrast results, interpretations, etc. "],
["multiway.html", "Chapter 15 Multi-way ANOVA", " Chapter 15 Multi-way ANOVA We have finished a nice book. "],
["block.html", "Chapter 16 Block Designs", " Chapter 16 Block Designs We have finished a nice book. "],
["regression.html", "Chapter 17 Regression Models", " Chapter 17 Regression Models We have finished a nice book. "],
["glm.html", "Chapter 18 The General Linear Model", " Chapter 18 The General Linear Model We have finished a nice book. "],
["mixedmodels.html", "Chapter 19 Mixed Models", " Chapter 19 Mixed Models We have finished a nice book. "],
["repeatedmeasures.html", "Chapter 20 Split Plot and Repeated Measures Designs", " Chapter 20 Split Plot and Repeated Measures Designs We have finished a nice book. "],
["logistic.html", "Chapter 21 Logistic Regression and Generalized Linear Models 21.1 Stuff here", " Chapter 21 Logistic Regression and Generalized Linear Models 21.1 Stuff here We have finished a nice book. "],
["glmm.html", "Chapter 22 Generalized Linear Mixed Models", " Chapter 22 Generalized Linear Mixed Models We have finished a nice book. "],
["learningobj.html", "Chapter 23 Appendix - Learning Objectives 23.1 Book-level 23.2 Topic-level 23.3 From ST512 23.4 For Point Estimates Chapter", " Chapter 23 Appendix - Learning Objectives 23.1 Book-level After reading this book you will be able to: identify relevent sources of variability for a potential study and, if applicable, utilize principles of design to plan a reasonable experiment to help answer questions of interest covariates noise variables random effects variance of indidvidual observations vice variance of summary statistics randomization systematic variation of factors/covariates factor identifiability understand issues surrounding multiple comparisons Bonferroni correction at least one other method (Tukey?) tradeoffs from replication within groups vice getting more groups compare and contrast methods for designing an experiment when the goal of a study is prediction versus when the goal is statistical inference explain the general concept of point estimation and how to account for sampling variability definition identify the right point estimate for your response variable of interest estimating uncertainty for point estimates normal approximation bootstrap CI others? Types of point estimates: means Simple effects interaction effects main effects standard deviations/variance components correlation coefficients quantiles/percentiles from distributions probabilities parameters of a distribution model parameters describe relevant properties of random variables and probabilities Distinguish between mutually exclusive and independent events. Calculate probability for a given scenario, either numerically or using a Venn diagram. Apply the General Addition Rule to solve probability problems. Apply the Rules for Probability Distributions to create a probability distribution for a given scenario. Use the complement of an event to solve probability problems. Apply the Multiplication Rule for Independent Processes to solve probability problems. random variables have a defined set of possible outcomes (“sample space”) Discrete vs. continuous RVs others??? probabilities/PDFs between 0 and 1 inclusive sum of probability of all possible events is 1 \\(P(A) + P(A^c) = 1\\), where \\(A\\) is an event and \\(A^c\\) is the complement of A explain the importance of statistical distributions when conducting statistical inference normal distribution and approximations plus properties robustness generality CLT costs and benefits of using nonparametric approaches describe the fundamental inferential techniques of hypothesis testing and confidence intervals as well as compare and contrast their uses and interpretations identify a null and alternative for a given problem interpret hypotheses characterize the test statistic under the null explain what a rejection region and be able to identify one define statistical power calculate statistical power for one- and two-sample tests of continuous and binary random variables define statistical confidence identify when using a CI and NSHT will result in the same conclusion explain when you can use a confidence interval to test for differences (e.g., comparing a single point estimate to a threshold) and when you can’t (e.g., when you have CIs for two different means) choose appropriate numerical summaries and graphical displays for a set of data and create these using software when to use tables vs. a picture types of graphical displays bar charts pie charts plotting data vice just predictions/conclusions when to include uncertainty bounds five-number summaries means vs. medians general plotting recommendations use of colors in you plots (discrete vs. divergent vs. continuous color scales, gray-scale, color-blind-friendly scales) use of annotations general graphical design philosophy (building a chart to illustrate a conclusion) trade-offs between detail and interpretability not screwing up your axes fit statistical models in software and interpret their output Which PROCs from SAS? REG, GLM, MIXED, GLIMMIX, others?? lm(), glm(), anova() …. broom? modelr? ciTools? p-values, point estimates, standard errors, f-statistics, chi-square-statistics, degrees of freedom, SS/MS, residual plots connect common statistical methods under the linear model framework Write statistical models using matrix representaiton identify models written in matrix representation with their representation in software identify when models written in different notation are the same or different describe when specific models will give you the same results ANOVA w/ 2 factors and a t-test or a SLR ANCOVA and MLR random effects vs. fixed effects split plots vs. more general mixed models logistic regression w/ categorical factors vice contingency table analysis discuss differences in assumptions associated with ANOVA vice SLR/MLR articulate the scope of inferential conclusions in light of the method of data collection, the experimental design used, the assumptions made, and the statistical analysis applied limitations due to sampling/sample frame missing data modeling assumptions sampling assumptions requirements for causal inference 23.2 Topic-level 23.2.1 Chapter 2 - Sampling, Design, and Exploratory Data Analysis 23.2.2 Chapter 3 - Point Estimation 23.2.3 Chapter 4 - Accounting for Uncertainty in Estimation 23.2.4 Chapter 5 - Inference via Hypothesis Testing for a Proportion or Mean 23.2.5 Chapter 6 - Inference via Confidence Intervals for a Proportion or Mean 23.2.6 Chapter 7 - Inference on Two Categorical Variables 23.2.7 Chapter 8 - Inference for Multiple Means 23.2.8 Chapter 9 - Multiway ANOVA 23.2.9 Chapter 10 - Block Designs 23.2.10 Chapter 11 - Regression 23.2.11 Chapter 12 - The General Linear Model 23.2.12 Chapter 13 - Mixed Models 23.2.13 Chapter 14 - Repeated Measures and Split Plot Designs 23.2.14 Chapter 15 - Logistic Regression and Generalized Linear Models 23.2.15 Chapter 16 - Generalized Linear Mixed Models 23.3 From ST512 WE NEED TO ORGANIZE THESE UNDER DIFFERENT CHAPTERS AT SOME POINT Learning Objectives Recognize a completely randomized design with one treatment factor and write the corresponding one-way analysis of variance model, with assumptions Estimate treatment means Estimate the variance among replicates within a treatment Construct the analysis of variance table for a one factor analysis of variance, including computing degrees of freedom, sums of squares, mean squares, and F-ratios Interpret results and draw conclusions from a one-factor analysis of variance Estimate differences between two treatment means in a one factor analysis of variance Test differences between two treatment means in a one factor analysis of variance Construct a contrast to estimate or test a linear combination of treatment means Estimate the standard error of a linear combination of treatment means Make inferences about linear combinations of treatment means, including contrasts. Obtain and understand SAS output for linear combinations of treatment means, including contrasts. Explain when and why corrections for multiple comparisons are needed Know when and how to use Tukey’s correction for all pairwise comparisons Compute Bonferroni confidence intervals Create and interpret orthogonal contrasts. Define main effects and interactions Write contrasts to estimate main effects and interactions Estimate these contrasts and their standard errors Compute sums of squares associated with these contrasts Test hypotheses about the main effects and interactions. Identify and define simple effects. Identify and define interaction effects. Identify and define main effects. Understand when to use simple, interaction, and main effects when drawing inferences in a two-way ANOVA. Write the analysis of variance model and SAS code for a completely randomized design with two factors Test hypotheses and interpret the analysis of variance for a factorial experiment. Explain the appropriate use of correlations and compute the correlation coefficient Read and interpret a scatterplot and guess the correlation coefficient by examination of a scatter plot Interpret the strength and direction of association indicated by the correlation coefficient and judge when a correlation coefficient provides an appropriate summary of a bivariate relationship Test the hypothesis that the correlation coefficient is zero using either a t-test or the Fisher z transformation, Compute confidence intervals using Fisher’s z transformation Write a statistical model for a straight line regression or a multiple regression and explain what all the terms of the model represent Explain the assumptions underlying regression models, evaluate whether the assumptions are met Estimate the intercept, slope and variance for a simple linear regression model Fit a multiple regression model in SAS and interpret the output, use the coefficient of determination to evaluate model fit Use a regression model to predict Y for new values of X Estimate the variance and standard error of parameters in regression models, test hypotheses about the parameters, and construct confidence intervals for the parameters. Explain the difference between a confidence interval and a prediction interval and know when to use each of them Construct a confidence interval for the expected value of Y at a given value of X Construct a prediction interval for a new value of Y at a given value of X Write a linear model in matrix notation Find the expectation and variance of a linear combination of random variables, a’Y Set up the expressions to calculate parameter estimates and predicted values using the matrix form of the model Estimate standard errors for parameter estimates and predicted values Use extra sums of squares to test hypotheses about subsets of parameters Construct indicator variables for including categorical regressor variables in a linear model Understand how to interpret parameters of a general linear model with indicator variables Estimate contrasts of treatment means and their standard errors using the general linear model notation and matrix form of the model Compare nested models with a lack of fit test to select a model Explain what a covariate is and how they are used Explain the assumptions of the analysis of covariance model and determine when these assumptions are met Fit an analysis of covariance model in SAS and conduct appropriate tests for treatment effects Estimate and interpret treatment means and their standard errors adjusted for covariates using SAS, Construct confidence intervals for adjusted treatment means Construct and estimate contrasts of treatment means adjusted for covariates and estimate the standard errors and confidence intervals of such contrasts. Analysis of variance and design of experiments Recognize each of the following types of experimental designs and determine when each type would be advantageous. 1. completely randomized design 2. randomized complete block design 3. split plot design Recognize whether factors should be considered fixed effects or random effects and explain the scope of inference for each case. Recognize whether factors are crossed or nested. For all of the designs listed and for experiments with crossed and/or nested fixed factors, random factors, or a combination of fixed and random effects, be able to 1. Write the corresponding analysis of variance model, with assumptions, and define all terms 2. Estimate treatment means and their standard errors 3. Construct the analysis of variance table, including computing degrees of freedom, sums of squares, mean squares, and F-ratios 4. Determine whether the assumptions of the model are satisfied 5. Interpret results and draw conclusions 6. Construct and estimate linear combinations of treatment means and their standard errors 7. Test hypotheses and construct confidence intervals about linear combinations of treatment means 8. Explain when and why corrections for multiple comparisons are needed, know when and how to use Tukey’s correction for all pairwise comparisons, compute Bonferroni confidence intervals 9. Create and interpret orthogonal contrasts. 10. Define and interpret main effects, simple effects and interactions 11. Use a table of expected mean squares to estimate variance components and determine appropriate F-statistics for testing effects in the analysis of variance 12. Interpret variance components and estimate and interpret the intraclass correlation coefficient. Regression and correlation Explain the appropriate use of correlations and compute the correlation coefficient, read and interpret a scatterplot and guess the correlation coefficient by examination of a scatter plot, test the hypothesis that the correlation coefficient is zero using either a t-test or the Fisher z transformation, compute confidence intervals using Fisher’s z transformation You should be able to do the following for fitting models to describe the relationships of one or several variables to a response variable. The regressor variables may be continuous or categorical or a mix of the two (e.g., analysis of covariance models) 1. Write a general linear model, including assumptions, in standard or matrix notation, and explain what all the terms and assumptions represent. Be able to handle models that contain interaction terms, polynomial terms, and dummy variables. 2. Evaluate whether the model assumptions are met 3. Fit a general linear model in SAS and interpret the output 4. Work with the general linear model in matrix form, including finding the expectation and variance of a linear combination of regression coefficients or treatment means 5. Test hypotheses and construct confidence intervals for linear combinations of the parameters 6. Construct and interpret a confidence interval for the expected value of Y at a given value of X 7. Construct and interpret a prediction interval for a new value of Y at a given value of X 8. Use extra sums of squares to test hypotheses about subsets of parameters. 9. Explain what a covariate is and how covariates are used 23.4 For Point Estimates Chapter Definitions for Mean, Median, Quantile, Percentile Explain uses for the above Identify the correct point estimate to use for a given test Define Systematic Random Sample and Convenience Sample Explain strengths and weaknesses of each Identify conditions when Systematic and Convenience Sampling may not provide representitive samples "],
["references.html", "References", " References "],
["notation.html", "Chapter 24 Appendix - Notation 24.1 Standard notation 24.2 Mixed models 24.3 Effects model representation 24.4 Estimators vs. Estimates", " Chapter 24 Appendix - Notation 24.1 Standard notation Vectors of variables are denoted with Roman letters, such as \\(x\\) and \\(Y\\). Capital letters denote random variables while lower case letters denote fixed variables. Note that these vectors may be of length 1 depending on context. Bolded values (\\(x\\)) denote matrices, and in the case of \\(Y\\), possibly single-column matrices. Unknown parameters are denoted with Greek letters, with boldface font indicating matrices. In most models, \\(Y\\) will denote the univariate response, \\(x\\) will describe a matrix of predictor variables, and \\(E\\) a vector of random errors. The Greek letter \\(\\beta\\) will be commonly used for regression parameters (either with subscripts for each values as in \\(\\beta_0 + \\beta_1 X_1\\) or as a vector (as in \\(X\\beta\\)). The letters \\(i, j, k,\\) and \\(l\\) will be most commonly used as subscripts or indices. \\(N\\) will typically denote a sample size (not a random vector), with subscripted versions (\\(n_i\\)) describing the number of observations in a group, and \\(p\\) describing the number of parameters in a model beyond the intercept. We may therefore describe a simple linear regresion model as: \\[Y = x\\beta + E\\] In this model, \\(Y\\) is a \\(N\\times 1\\) random vector, \\(x\\) is a \\(N\\times (p + 1)\\) matrix of fixed values, and \\(E\\) is a \\(N \\times 1\\) vector. \\(\\pi\\) is typically used to describe probability parameters, as in Bernoulli or binomial random variables. 24.2 Mixed models Still need to add something for this 24.3 Effects model representation In the effects formulation of ANOVA models, additional greek letters (\\(\\alpha\\), \\(\\gamma\\), etc.) will appear as parameter effects, as will \\(\\mu\\), which will typically represent the grand mean. Group-specfic means will be denoted via subscripts: \\(\\mu_{ij}\\). When using this representation, it is convenient to describe a single observation as \\(Y_{ijk}\\), which is the \\(k\\)th observation from the group with with the \\(i\\)th level of the first factor and the \\(j\\)th level of the second factor. In the main effects version of this model, we have: \\[Y_{ijk} = \\mu + \\alpha_i + \\gamma_j + E_{ijk}\\] We can therefore estimate \\(\\mu_{ij}\\) as \\(\\hat \\mu_{ij} = \\frac{1}{n}\\sum_{k = 1}^n Y_{ijk} = \\bar{Y}_{ij\\cdot}\\). This “dot” notation can be extended to any subscript and indicates summing over the index that has been replaced by the dot. Further note that the “hat” over a paremeter value denotes the estimator for that parameter value, and the “bar” indicates an average. These features are used generally throughout this book. 24.4 Estimators vs. Estimates If we want to get pedantic, we can differentiate between estimates and estimators in our notation. Estimators are functions of random variables used to estimate parameters. Estimates are realized values of estimators. To differentiate these, we use Roman letters with hats to represent estimators (\\(\\hat B = (x&#39;x )^{-1}x&#39;Y\\)) and Greek letters with hats to represent estimates (\\(\\hat \\beta = 1.52\\)). 24.4.1 Installing R and RStudio The R software itself can be downloaded and installed by visiting the Comprehensive R Archive Network (Cran) website. Here there are links to install R for Linux, Mac, and Windows based machines. For Windows users, follow the inital ‘Download R for Windows’ link and then click ‘install R for the first time.’ From here you should now see a Download R X.x.x for Windows link that will download a .exe file. Once downloaded run that file and follow the prompts. For Mac users, follow the inital ‘Download R for (Mac) OS X’ link and click on the link near the ‘Latest Release’ section similar to R-x.x.x.pkg. Once downloaded, you should be able to install by double clicking on the file. For Linux users, follow the inital ‘Download R for Linux’ link. Choose your OS and instructions are given on how to download R. Once you’ve installed R you’ll want to install RStudio. RStudio is a well developed environment that makes programming in R much easier! To download head to RStudio’s download page. From here choose RStudio Desktop (Open Source License) and a page with appropriate links to install are provided. 24.4.2 Using RStudio To program in R you’ll want to open RStudio. RStudio will submit R code for you so you never actually need to open R itself. There are four main ‘areas’ of the RStudio IDE: Console (&amp; Terminal) Scripting and Viewing Window Plots/Help (&amp; Files/Packages) Environment (&amp; Connections/Git) You may wish to rearrange the panes. This can be done via the menus at the top. Choose “Tools –&gt; Global Options”. Other useful global options to chnage are under the appearance tab (font size, theme) and under the code tab (editing –&gt; soft-wrap, display –&gt; show whitespace). 24.4.2.1 Console To evaluate code you can type directly into the console. #simple math operations # &lt;-- is a comment - code not evaluated 3 + 7 ## [1] 10 10 * exp(3) #exp is exponential function ## [1] 200.8554 log(pi^2) #log is natural log by default ## [1] 2.28946 mean(cars$speed) ## [1] 15.4 hist(cars$speed) In the R sections of the book we spend much of our time learning the R syntax needed to create the appropriate summaries or analysis. 24.4.2.2 Scripting and Viewing Window Usually you don’t want to type code directly into the console because there isn’t an easy way to get the code for later use. Instead code is usually written in an R ‘script’ which is then saved. From an R script you can send code to console via: “Run” button (runs current line) CTRL+Enter (PC) or Command+Enter (MAC) Highlight section and do above To create a new R script you can use the menus at the top and go to File –&gt; New File –&gt; R Script. Take a moment and do this! Type the following into your script: View(cars) (note capital V) plot(cars) Submit it to the console using a button or hot key! 24.4.2.3 Plots/Help Created plots are stored in the Plots tab. This is a nice feature that allows you to cycle through past plots and easily save plots via menus. In this pane there is also a Help tab that will enable you to learn about R functions. In the console type help(hist) for instance. Information about the hist function is presented. Being able to parse these types of help files is a really useful skill! For every R function there are a few sections: Description - What the function is intended for. Usage - How to call the function, inputs required, and which inputs have default arguments. Here we see hist(x, ...). This implies there is only one required input, x, and there is no default. Below you see a more detailed call to hist that includes other inputs. Each of these inputs has an equal sign with a value after it. This is the default value for that input (since there is a default value you don’t have to specify it when you call). For instance the breaks = &quot;Sturges&quot; input implies that the “Sturges” method is the default for determining how the bins of the histogram are created. Arguments - Describes the input requirements in more detail. Details - Information about how the function works. Values - Information about what is returned to the user. References See Also - Related functions. Examples - Highly useful section giving code you can copy and paste to see an example of how the function can be used. 24.4.2.4 Environment R stores data/info/functions/etc. in R objects. An object is a data structure having attributes and methods (more on this shortly). You can create an R object via &lt;- (recommended) or =. #save for later avg &lt;- (5 + 7 + 6) / 3 #call avg object avg ## [1] 6 #strings (text) can be saved as well words &lt;- c(&quot;Hello there!&quot;, &quot;How are you?&quot;) words ## [1] &quot;Hello there!&quot; &quot;How are you?&quot; Notice that when you send the line avg &lt;- (5+ 7 + 6) / 3 to the console (i.e. create the object avg) that nothing prints out. This is common behavior when storing the object. The output or information is saved for later use in the object. To see the output or information you then simply call the object (a default printing method is used to display it). You can look at all current objects with ls(). ls() ## [1] &quot;alpha&quot; &quot;aoc&quot; &quot;avg&quot; &quot;fit&quot; &quot;lower_bound&quot; ## [6] &quot;mat&quot; &quot;n&quot; &quot;pihat&quot; &quot;tb&quot; &quot;upper_bound&quot; ## [11] &quot;v&quot; &quot;vec&quot; &quot;words&quot; &quot;x&quot; &quot;y&quot; ## [16] &quot;z&quot; Use rm() to remove an object. rm(avg) ls() ## [1] &quot;alpha&quot; &quot;aoc&quot; &quot;fit&quot; &quot;lower_bound&quot; &quot;mat&quot; ## [6] &quot;n&quot; &quot;pihat&quot; &quot;tb&quot; &quot;upper_bound&quot; &quot;v&quot; ## [11] &quot;vec&quot; &quot;words&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; Built-in objects exist like letters and cars. letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; head(cars, n = 3) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 The function data() shows available built-in datasets. You should now be roughly familiar with the four main ‘areas’ of the RStudio IDE: Console (&amp; Terminal) Scripting and Viewing Window Plots/Help (&amp; Files/Packages) Environment (&amp; Connections/Git) 24.4.3 R Objects and Classes R has strong Object Oriented Programming (OOP) tools. Object: data structure with attributes (class) Method: procedures (functions) that act on object based on attributes R functions like print() or plot() act differently depending on an object’s class. class(cars) ## [1] &quot;data.frame&quot; plot(cars) class(exp) ## [1] &quot;function&quot; plot(exp) Many R functions exist to help understand an R Object. str() (structure) str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... class() class(cars) ## [1] &quot;data.frame&quot; typeof() typeof(cars) ## [1] &quot;list&quot; We’ll use these functions later to help us know how to extra information from an R object. Recall that we can create an R object via &lt;- (recommended) or =. This allocates computer memory to object. The object’s attributes depend on how you created it. vec &lt;- c(1, 4, 10) class(vec) ## [1] &quot;numeric&quot; fit &lt;- lm(dist ~ speed, data = cars) class(fit) ## [1] &quot;lm&quot; 24.4.4 Data Objects To understand how to use R for data analysis we need to understand commonly used data structures: 1. Atomic Vector (1D) 2. Matrix (2D) 3. Array (nd) (not covered) 4. Data Frame (2D) 5. List (1D) 24.4.4.1 Atomic Vector Let’s start with the most basic object and work our way up. An atomic vector is a 1D group of elements with an ordering. All of the elements must be same ‘type’. Types include numeric (integer or double), character, or logical. We create an atomic vector with the c() function (‘combine’). #vectors (1 dimensional) objects x &lt;- c(17, 22, 1, 3, -3) y &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;bird&quot;, &quot;frog&quot;) x ## [1] 17 22 1 3 -3 y ## [1] &quot;cat&quot; &quot;dog&quot; &quot;bird&quot; &quot;frog&quot; In addition, many ‘functions’ output a numeric vector. Functions are at the heart of R so it is vital to understand them. The concept of a function is that there the function takes an input or inputs and maps those inputs to some output(s). As an example, one function that outputs a numeric vector is the seq or sequence function. To know about a function you need to know about the inputs and ouputs. For seq we have the following: + Inputs = from, to, by (among others) + Output = a sequence of numbers v &lt;- seq(from = 1, to = 5, by = 1) v ## [1] 1 2 3 4 5 str(v) ## num [1:5] 1 2 3 4 5 str tells about the object v: num says it is numeric [1:5] implies one dimensional with elements 1, 2, 3, 4, 5 The seq function is used quite a bit. There is a shorthand way to create an integer sequence using :. 1:20 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 It is also important to know how R does math on its objects. R does elementwise addition/subtraction and multiplication/division to vectors, matrices, and data frames. (The matrix multiplicaiton operator is %*%.). 1:20/20 ## [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 ## [16] 0.80 0.85 0.90 0.95 1.00 1:20 + 1 ## [1] 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 As we mentioned earlier, understanding help files is really useful to being about to program in R. As functions are ubiquitous in R we often need to learn about their inputs (or arguments) and we can do so using help. To recap, our first commonly used R object for storing data is an atomic vectore. This is a 1D group of elements with an ordering where all of the elements are of the same type. Generally vectors are useful to know about but not usually useful for a storing a dataset exactly. They can often be considered as the ‘building blocks’ for other data types. 24.4.4.2 Matrix A Matrix is a 2D data structure in R whose elements are all of the same type. The first dimension refers to the rows and the second dimension refers to the columns. A 2D data object is very common. The rows often represent the observations and the columns represent the variables. Although not technically right, it is useful to think of the columns of a matrix as vectors of the same type and length. For instance, consider the three vectors created here: #populate vectors x &lt;- c(17, 3, 13, 11) y &lt;- rep(-3, times = 4) z &lt;- 1:4 These are all of the same type. This can be checked with an is. (read as ‘is dot’) function. #check &#39;type&#39; is.numeric(x) ## [1] TRUE is.numeric(y) ## [1] TRUE is.numeric(z) ## [1] TRUE Not only are these three objects the same type but they are also the same length. This can be checked using the length function. #check &#39;length&#39; length(x) ## [1] 4 length(y) ## [1] 4 length(z) ## [1] 4 Again, it is useful to visualize the columns of a potential matrix as these vectors. We can create the matrix using the matrix function. The matrix function requires us to give the data as one vector. We can combine the x, y, and z objects into one vector using the c funciton. This is the first argument to the matrix function. The only other argument required is to either specify the number of rows (nrow =) or the number of columns (ncol =) (R will attempt to figure out the one that is not given using the total length of the specified data vector). #combine in a matrix matrix(c(x, y, z), ncol = 3) ## [,1] [,2] [,3] ## [1,] 17 -3 1 ## [2,] 3 -3 2 ## [3,] 13 -3 3 ## [4,] 11 -3 4 A matrix can also store character data as well. An example of this is given below and the number of rows is specified rather than the number of columns. Note the use of is.character from the is. family of functions. x &lt;- c(&quot;Hi&quot;, &quot;There&quot;, &quot;!&quot;) y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) z &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;) is.character(x) ## [1] TRUE matrix(c(x, y, z), nrow = 3) ## [,1] [,2] [,3] ## [1,] &quot;Hi&quot; &quot;a&quot; &quot;One&quot; ## [2,] &quot;There&quot; &quot;b&quot; &quot;Two&quot; ## [3,] &quot;!&quot; &quot;c&quot; &quot;Three&quot; To recap, a Matrix is a 2D data structure where we can think of the columns as vectors of the same type and length. These are useful for some datasets but most datasets have some numeric and some character variables. Another 2D object called a data frame is perfect for this type of data! 24.4.4.3 Data Frame A Data Frame is a 2D data structure where elements within a column must be of the same type but the columns themselves can differ in type. When thinking of a data frame, consider them as a collection (list) of vectors of the same length. A data frame can be created with the data.frame function. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) y &lt;- c(1, 3, 4, -1, 5, 6) z &lt;- 10:15 data.frame(x, y, z) ## x y z ## 1 a 1 10 ## 2 b 3 11 ## 3 c 4 12 ## 4 d -1 13 ## 5 e 5 14 ## 6 f 6 15 You can also easily name the columns during creation. data.frame(char = x, data1 = y, data2 = z) ## char data1 data2 ## 1 a 1 10 ## 2 b 3 11 ## 3 c 4 12 ## 4 d -1 13 ## 5 e 5 14 ## 6 f 6 15 Notice that char, data1, and data2 become the variable names for the data frame. To recap, consider a data frame as a collection (list) of vectors of the same length. Tis type of data structure is perfect for most data sets! Most functions that read 2D data into R store it as a data frame. 24.4.4.4 List A List is a 1D group of objects with ordering. Really it is a vector that can have differing elements. Think of this in a similar way to the atomic vector previously discussed except the elements are really flexible. A list can be created with the list function. You specify the elements you want to include, separated by commas. list(1:3, rnorm(2), c(&quot;!&quot;, &quot;?&quot;)) ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] 0.9008016 -1.5937741 ## ## [[3]] ## [1] &quot;!&quot; &quot;?&quot; Similar to a data frame, you can add names to the list elements during creation. list(seq = 1:3, normVals = rnorm(2), punctuation = c(&quot;!&quot;, &quot;?&quot;)) ## $seq ## [1] 1 2 3 ## ## $normVals ## [1] 0.3460836 2.4692143 ## ## $punctuation ## [1] &quot;!&quot; &quot;?&quot; To recap, a list is a very flexible 1D object. It is really useful for more complex types of data. The table below gives a summary of the data objects we’ve covered. For most data analysis you’ll use data frames. Dimension Homogeneous Heterogeneous 1d Atomic Vector List 2d Matrix Data Frame Next we look at how to access or change parts of our these common data objects. 24.4.5 Accessing Common Data Objects When we are dealing with a data object (1D or 2D) we may want to extract a single element, certain columns, or certain rows. In this section we’ll look at how to subset or extract information from each of the common data objects covered in the previous section. 24.4.5.1 Atomic Vector (1D) For atomic vectors (and lists, see later) you can return elements using square brackets []. You may notice that when R prints a vector to the console you often see [1] next to the first element and perhaps a [#] where R has to break and move to the next line of the console. The [1] implies the element printed next is the first element of the vector (R starts its counting at 1 not 0 like some other languages). The [#] implies that the element printed to the right is the # element of the vector. This is a good reminder of how to extract values from an atomic vector. As an example, here we extract from a built-in R object called letters that is a vector of length 26 containing the letters of the alphabet. letters #built-in vector ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; letters[1] #R starts counting at 1! ## [1] &quot;a&quot; letters[26] ## [1] &quot;z&quot; To obtain more than one element you can ‘feed’ in a vector of indices to that you’d like to return. letters[1:4] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; letters[c(5, 10, 15, 20, 25)] ## [1] &quot;e&quot; &quot;j&quot; &quot;o&quot; &quot;t&quot; &quot;y&quot; x &lt;- c(1, 2, 5) letters[x] ## [1] &quot;a&quot; &quot;b&quot; &quot;e&quot; If you’d like to return all values except a certain subset, you can use negative indices. letters[-(1:4)] ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; ## [20] &quot;x&quot; &quot;y&quot; &quot;z&quot; x &lt;- c(1, 2, 5) letters[-x] ## [1] &quot;c&quot; &quot;d&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; ## [20] &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; 24.4.5.2 Matrices (2D) For rectangular data like a matrix you can return rectangular subsets using square brackets with a comma [ , ]. Notice default row and column names when R prints a matrix! mat &lt;- matrix(c(1:4, 20:17), ncol = 2) mat ## [,1] [,2] ## [1,] 1 20 ## [2,] 2 19 ## [3,] 3 18 ## [4,] 4 17 This is a nice reminder of how to index a matrix. The value prior to the columns represents which row(s) you want to return and the value after the comma which column(s). If an index is left blank then all of that corresponding dimension (row or column) is returned. mat[c(2, 4), ] ## [,1] [,2] ## [1,] 2 19 ## [2,] 4 17 mat[, 1] ## [1] 1 2 3 4 mat[2, ] ## [1] 2 19 mat[2, 1] ## [1] 2 Notice that R simplifies the result where possible. That is, returns an atomic vector if you have only 1 dimension and a matrix if two. This can be changed by adding an additional argument to the [ function. mat[ , 1, drop = FALSE] ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 Also, if you only give a single value in the [] then R uses the count of the value in the matrix. Counts go down columns first. mat[5] ## [1] 20 If your matrix has column names associated with it, you can also use those to return columns of interest. To add column names we can look run help(matrix) to learn how! Notice the dimnames argument. You can specify names for the rows and columns by using a list with two vectors. The first vector indicating row names and the second column names. If we don’t want to give rownames we can give a NULL (a special value in R that is used for undefined values - here giving no specification of row names). We can do this and give a character vector for the column names. mat&lt;-matrix(c(1:4, 20:17), ncol = 2, dimnames = list(NULL, c(&quot;First&quot;, &quot;Second&quot;)) ) mat ## First Second ## [1,] 1 20 ## [2,] 2 19 ## [3,] 3 18 ## [4,] 4 17 Now we can request columns be using a single name or a character vector of names. mat[, &quot;First&quot;] ## [1] 1 2 3 4 To return all but certain parts of a matrix you can still use negative indices but note that this won’t work with column names. mat[-c(1,3), -&quot;First&quot;] ## Error in -&quot;First&quot;: invalid argument to unary operator mat[-c(1,3), &quot;First&quot;] ## [1] 2 4 24.4.5.3 Data Frames (2D) Since a data frame is also a rectangular data object you can return rectangular subsets using square brackets with a comma [ , ]! As an example, we’ll subset the built-in iris data frame. To get an idea about this object we can run str(iris). str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... We can see this is a data frame with a few columns, four are numeric and one is a factor (a special type of character vector essentially - these will be covered when we discuss plotting). iris[1:4, 2:4] ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 iris[1, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa iris[, 1] ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 Notice the simplification done when a single column is selected. R will simplify to a vector unless drop = FALSE is included as done in the matrix section. (The simplification doesn’t occur when a single row is selected because data frames are actually lists - we’ll discuss this more in the list section!) You can use columns names to subset as well. iris[1:10 , c(&quot;Sepal.Length&quot;, &quot;Species&quot;)] ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa The most common way to access a single columns is to use the dollar sign operator. iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 A nice benefit of using RStudio is that column names will be filled in automatically as you type. In your console do the following: Type iris$ If no choices - hit tab Scroll up and down or continue typing to highlight the column of interest Hit tab again to choose 24.4.5.4 Lists (1D) As a list is a 1D data object we can use single square brackets [ ] for multiple list elements. x &lt;- list(&quot;HI&quot;, c(10:20), 1) x ## [[1]] ## [1] &quot;HI&quot; ## ## [[2]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 ## ## [[3]] ## [1] 1 x[2:3] ## [[1]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 ## ## [[2]] ## [1] 1 We can use double square brackets [[ ]] (or [ ]) to return a single list element. The major difference is in whether or not a list with the element chosen is returned or just the element itself. [[ will return just the element requested. x &lt;- list(&quot;HI&quot;, c(10:20), 1) x[1] ## [[1]] ## [1] &quot;HI&quot; x[[1]] ## [1] &quot;HI&quot; x[[2]] ## [1] 10 11 12 13 14 15 16 17 18 19 20 x[[2]][4:5] ## [1] 13 14 Recall we could name our list elements. If they are named we can use the $ similar to a data frame. x &lt;- list(&quot;HI&quot;, c(10:20), 1) str(x) ## List of 3 ## $ : chr &quot;HI&quot; ## $ : int [1:11] 10 11 12 13 14 15 16 17 18 19 ... ## $ : num 1 x &lt;- list(First = &quot;Hi&quot;, Second = c(10:20), Third = 1) x$Second ## [1] 10 11 12 13 14 15 16 17 18 19 20 Under the hood a data frame is just a list of equal length vectors! str(x) ## List of 3 ## $ First : chr &quot;Hi&quot; ## $ Second: int [1:11] 10 11 12 13 14 15 16 17 18 19 ... ## $ Third : num 1 str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... typeof(x) ## [1] &quot;list&quot; typeof(iris) ## [1] &quot;list&quot; This means we can index a data frame in a similar way to how we index a list if we want. iris[[2]] ## [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 3.7 3.4 3.0 3.0 4.0 4.4 3.9 3.5 ## [19] 3.8 3.8 3.4 3.7 3.6 3.3 3.4 3.0 3.4 3.5 3.4 3.2 3.1 3.4 4.1 4.2 3.1 3.2 ## [37] 3.5 3.6 3.0 3.4 3.5 2.3 3.2 3.5 3.8 3.0 3.8 3.2 3.7 3.3 3.2 3.2 3.1 2.3 ## [55] 2.8 2.8 3.3 2.4 2.9 2.7 2.0 3.0 2.2 2.9 2.9 3.1 3.0 2.7 2.2 2.5 3.2 2.8 ## [73] 2.5 2.8 2.9 3.0 2.8 3.0 2.9 2.6 2.4 2.4 2.7 2.7 3.0 3.4 3.1 2.3 3.0 2.5 ## [91] 2.6 3.0 2.6 2.3 2.7 3.0 2.9 2.9 2.5 2.8 3.3 2.7 3.0 2.9 3.0 3.0 2.5 2.9 ## [109] 2.5 3.6 3.2 2.7 3.0 2.5 2.8 3.2 3.0 3.8 2.6 2.2 3.2 2.8 2.8 2.7 3.3 3.2 ## [127] 2.8 3.0 2.8 3.0 2.8 3.8 2.8 2.8 2.6 3.0 3.4 3.1 3.0 3.1 3.1 3.1 2.7 3.2 ## [145] 3.3 3.0 2.5 3.0 3.4 3.0 Lastly, one nice thing about lists (and data frames) is that you can use partial matching with [[ and $. iris$Sp[1:10] ## [1] setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica iris[[&quot;Petal.Len&quot;, exact = FALSE]] ## [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4 ## [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2 ## [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0 ## [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0 ## [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0 ## [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3 ## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0 ## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 ## [145] 5.7 5.2 5.0 5.2 5.4 5.1 This is less important now that RStudio can auto-complete long column names. 24.4.6 Recap! RStudio IDE (Integrated Development Environment) R Objects and Classes Data Objects &amp; Basic Manipulation Dimension Homogeneous Heterogeneous 1d Atomic Vector List 2d Matrix Data Frame Basic access via Atomic vectors - x[ ] Matrices - x[ , ] Data Frames - x[ , ] or x$name Lists - x[ ], x[[ ]], or x$name 24.4.7 R Manipulating Data 24.4.8 R Reading Data "]
]
